id,abstract
1803.03045," our aim is to understand the role of the environment in the quenching of star formation of galaxies located in the infall cluster region of abell 85 (a 85). this is achieved by studying the post-starburst galaxy population as tracer of recent quenching. by measuring the equivalent width (ew) of the [oii] and hδ spectral lines, we classify the galaxies in three groups: passive (pas), emission line (el), and post-starburst (psb) galaxies. the psb galaxy population represents ∼ 4.5% of the full sample. dwarf galaxies (mr > −18.0) account for ∼ 70 − 80% of psbs, which indicates that most of the galaxies undergoing recent quenching are low-mass objects. independently of the environment, psb galaxies are disk-like objects with g − r colour between the blue els and the red pas ones. the psb and el galaxies in low-density environments show similar luminosities and local galaxy densities. the dynamics and local galaxy density of the psb population in high density environments are shared with pas galaxies. however, psb galaxies inside a 85 are at shorter clustercentric radius than pas and el ones. the value of the ew(hδ) is larger for those psbs closer to the cluster centre. we propose two diﬀerent physical mechanisms producing psb galaxies depending on the environment. in low density environments, gas-rich minor mergers or accretions could produce the psb galaxies. for high density environments like a 85, psbs would be produced by the removal of the gas reservoirs of el galaxies by ram-pressure stripping when they pass near to the cluster centre. key words: galaxies: clusters: individual: abell 85 – galaxies: kinematics and dy- namics, "
1803.03046," the intermediate mass fragments emission probability from projectile-like fragment break-up in semi-peripheral reactions has been measured in collisions of 124xe projectiles with two diﬀerent targets of 64ni and 64zn at the laboratory energy of 35 a mev. the two colliding systems diﬀer only for the target atomic number z and, consequently, for the isospin n/z ratio. an enhancement of intermediate mass fragments production for the neutron rich 64ni target, with respect to the 64zn, is found. in the case of one intermediate mass fragment emission, the contributions of the dynamical and statistical emissions have been evaluated, showing that the increase of the eﬀect above is due to an enhancement of the dynamical emission probability, especially for heavy imfs (z& 7). this proves an inﬂuence of the target isospin on inducing the dynamical fragment production from projectile-like fragment break-up. in addition, a comparison of the xe+ni,zn results with the previously studied 112,124sn+58,64n i systems is discussed in order to investigate the inﬂuence of the projectile isospin alone and to disentangle between isospin eﬀects against system- size eﬀects on the emission probability. these comparisons suggest that the prompt-dynamical emission is mainly ruled by the n/z content of, both, projectile and target; for the cases here investigated, the inﬂuence of the system size on the dynamical emission probability can be excluded. pacs numbers: 25.70mn, 25.70pq ∗ russotto@lns.infn.it † current address: stfc, rutherford appleton laboratory, isis facility, harwell, ox11 0qx, united kingdom ‡ deceased § current address: institut de physique nucl´eaire, cnrs-in2p3, univ. paris-sud, orsay, universit´e paris- saclay, orsay, cedex, france 2 i. "
1803.03049," zero-shot learning has gained popularity due to its po- tential to scale recognition models without requiring ad- ditional training data. this is usually achieved by asso- ciating categories with their semantic information like at- tributes. however, we believe that the potential offered by this paradigm is not yet fully exploited. in this work, we propose to utilize the structure of the space spanned by the attributes using a set of relations. we devise ob- jective functions to preserve these relations in the embed- ding space, thereby inducing semanticity to the embedding space. through extensive experimental evaluation on ﬁve benchmark datasets, we demonstrate that inducing seman- ticity to the embedding space is beneﬁcial for zero-shot learning. the proposed approach outperforms the state-of- the-art on the standard zero-shot setting as well as the more realistic generalized zero-shot setting. we also demonstrate how the proposed approach can be useful for making ap- proximate semantic inferences about an image belonging to a category for which attribute information is not available. "
1803.03051," a log gaussian cox process (lgcp) is a doubly stochastic construction consisting of a poisson point process with a random log-intensity given by a gaussian random ﬁeld. statistical methodology have mainly been developed for lgcps deﬁned in the d-dimensional euclidean space. this paper concerns the case of lgcps on the d-dimensional sphere, with d = 2 of primary interest. we discuss the existence problem of such lgcps, provide suﬃcient existence conditions, and establish further useful theoretical properties. the results are applied for the description of sky positions of galaxies, in comparison with previous analysis based on a thomas process, using simple estimation procedures and making a careful model checking. we account for inhomogeneity in our models, and as the model checking is based on a thinning procedure which produces homogeneous/isotropic lgcps, we discuss its sensitivity. keywords: h¨older continuity, pair correlation function, point processes on the sphere, reduced palm distribution, second order intensity reweighted homogeneity, thinning procedure for model checking "
1803.03052,"—in this article we present a geometric discrete-time pontryagin maximum principle (pmp) on matrix lie groups that incorporates frequency constraints on the controls in addition to pointwise constraints on the states and control actions directly at the stage of the problem formulation. this pmp gives ﬁrst order necessary conditions for optimality, and leads to two- point boundary value problems that may be solved by shooting techniques to arrive at optimal trajectories. we validate our theoretical results with a numerical experiment on the attitude control of a spacecraft on the lie group so(3). "
1803.03054," killing horizons which can be such for two or more linearly independent killing vectors are studied. we provide a rigorous deﬁnition and then show that the set of killing vectors sharing a killing horizon is a lie algebra ah of dimension at most the dimension of the spacetime. we prove that one cannot attach diﬀerent surface gravities to such multiple killing horizons, as they have an essentially unique non- zero surface gravity (or none). ah always contains an abelian (sub)-algebra —whose elements all have vanishing surface gravity— of dimension equal to or one less than dim ah. there arise only two inequivalent possibilities, depending on whether or not there exists the non-zero surface gravity. we show the connection with near horizon geometries, and also present a linear system of pdes, the master equation, for the proportionality function on the horizon between two killing vectors of a multiple killing horizon, with its integrability conditions. we provide explicit examples of all possible types of multiple killing horizons, as well as a full classiﬁcation of them in maximally symmetric spacetimes. 1 "
1803.03056," sato introduced the tau-function to describe solutions to a wide class of completely integrable diﬀerential equations. later segal-wilson repre- sented it in terms of the relevant integral operators on hardy space of the unit disc. this paper gives another representation of the tau-functions by the weyl functions for 1d schr¨odinger operators with real valued poten- tials, which will make it possible to extend the class of initial data for the kdv equation to more general one. 1 "
1803.03058,". [context and motivation] internet of things (iot) is be- coming common throughout everyday lives. however, the interaction is often diﬀerent from when using e.g. computers and other smart devices. furthermore, an iot device is often dependent on several other sys- tems, heavily impacting the user experience (ux). finally, the domain is changing rapidly and is driven by technological innovation. [question/problem] in this qualitative study, we explore how compa- nies elicit ux requirements in the context of iot. a key part of contem- porary iot development is also data-driven approaches. thus, these are also considered in the study. [principal idea / results] there is a knowledge gap around data- driven methodologies, there are examples of companies that collect large amount of data but do not always know how to utilize it. furthermore, many of the companies struggle to handle the larger system context, where their products and the ux they control are only one part of the complete iot ecosystem. [contribution] we provide qualitative empirical data from iot devel- oping companies. based on our ﬁndings, we identify challenges for the companies and areas for future work. 1 "
1803.03059," social norms serve as an important mechanism to regulate the be- haviors of agents and to facilitate coordination among them in multiagent systems. one important research question is how a norm can rapidly emerge through repeated local interaction within an agent society under diﬀerent envi- ronments when their coordination space becomes large. to address this prob- lem, we propose a hierarchically heuristic learning strategy (hhls) under the hierarchical social learning framework, in which subordinate agents report their information to their supervisors, while supervisors can generate instruc- tions (rules and suggestions) based on the information collected from their subordinates. subordinate agents heuristically update their strategies based on both their own experience and the instructions from their supervisors. ex- tensive experiment evaluations show that hhls can support the emergence of desirable social norms more eﬃciently and is applicable in a much wider range of multiagent interaction scenarios compared with previous work. we also in- vestigate the eﬀectiveness of hhls by separating out the diﬀerent components of the hhls and evaluating the relative importance of those components. the tianpei yang · jianye hao ( ) · sheng jin school of computer software, tianjin university, china e-mail: jianye.hao@tju.edu.cn tianpei yang e-mail: tpyang@tju.edu.cn zhaopeng meng tianjin university of traditional chinese medicine, china e-mail: mengzp@tju.edu.cn sandip sen university of tulsa, usa e-mail: sandip-sen@utulsa.edu sheng jin e-mail: jinsheng@tju.edu.cn 8 1 0 2   r a m 8       ] a m . s c [     1 v 9 5 0 3 0 . 3 0 8 1 : v i x r a 2 tianpei yang et al. inﬂuence of key related factors (e.g., hierarchical factors, non-hierarchical fac- tors, ﬁxed-strategy agents) are investigated as well. keywords norm emergence · multiagent social learning · multiagent hierarchical learning "
1803.03060,". for a hypergraph h, let q(h) denote the expected number of monochromatic edges when the color of each vertex in h is sampled uniformly at random from the set of size 2. let smin(h) denote the minimum size of an edge in h. erd˝os asked in 1963 whether there exists an unbounded function g(k) such that any hypergraph h with smin(h) > k and q(h) 6 g(k) is two colorable. beck in 1978 answered this question in the aﬃrmative for a function g(k) = θ(log∗ k). we improve this result by showing that, for an absolute constant δ > 0, a version of random greedy coloring procedure is likely to ﬁnd a proper two coloring for any hypergraph h with smin(h) > k and q(h) 6 δ · log k. 8 1 0 2   r a m 8       ] . o c h t a m [     1 v 0 6 0 3 0 . 3 0 8 1 : v i x r a "
1803.03062," we developed a chemical network for modeling the chemistry and non-ideal mhd eﬀects from the collapsing dense molecular clouds to protostellar disks. first, we re-formulated the cosmic-ray desorption rate by considering the variations of desorption rate over the grain size distribution. we ﬁnd that the diﬀerential desorption of volatile species is ampliﬁed by the grains larger than 0.1 µm, because larger grains are heated to a lower temperature by cosmic-rays and hence more sensitive to the variations in binding energies. as a result, atomic nitrogen n is ∼2 orders of magnitude more abundant than co; n2h+ also becomes a few times more abundant than hco+ due to the increased gas-phase n2. however, the changes in ionization fraction due to freeze-out and desorption only have minor eﬀects on the non-ideal mhd diﬀusivities. our chemical network conﬁrms that the very small grains (vsgs: below a few 100 ) weakens the eﬃciency of both ambipolar diﬀusion and hall eﬀect. in collapsing dense cores, a maximum ambipolar diﬀusion is achieved when truncating the mrn size distribution at 0.1 µm, and for a maximum hall eﬀect, the truncation occurs at 0.04 µm. we conclude that the grain size distribution is crucial to the diﬀerential depletion between co and n2 related molecules, as well as to the non-ideal mhd diﬀusivities in dense cores. key words: interstellar dust grain, astrochemistry, cosmic rays, magnetohydrodynamics "
1803.03063," this is the second installment for the large sky area multi-object fibre spectroscopic telescope (lamost) quasar survey, which includes quasars ob- served from september 2013 to june 2015. there are 9024 conﬁrmed quasars in dr2 and 10911 in dr3. after cross-match with the sdss quasar catalogs and ned, 12126 quasars are discovered independently. among them 2225 quasars were released by sdss dr12 qso catalogue in 2014 after we ﬁnalised the survey candidates. 1801 sources were identiﬁed by sdss dr14 as qsos. the remain- ing 8100 quasars are considered as newly founded, and among them 6887 quasars can be given reliable emission line measurements and the estimated black hole masses. quasars found in lamost are mostly located at low-to-moderate red- shifts, with a mean value of 1.5. the highest redshift observed in dr2 and dr3 is 5. we applied emission line measurements to hα, hβ, mgii and civ. we deduced the monochromatic continuum luminosities using photometry data, and estimated the virial black hole masses for the newly discovered quasars. results are compiled into a quasar catalog, which will be available online. subject headings: catalog - survey - quasars: general - quasars: emission lines – 3 – 1. "
1803.03066,". complex moment sequences are exactly those which admit pos- itive deﬁnite extensions on the integer lattice points of the upper diagonal half-plane. here we prove that the aforesaid extension is unique provided the complex moment sequence is determinate and its only representing measure has no atom at 0. the question of converting the relation is posed as an open problem. a partial solution to this problem is established when at least one of representing measures is supported in a plane algebraic curve whose intersec- tion with every straight line passing through 0 is at most one point set. further study concerns representing measures whose supports are zariski dense in c as well as complex moment sequences which are constant on a family of parallel “diophantine lines”. all this is supported by a bunch of illustrative examples. there are two ways of approaching the complex moment problem (see [3]; for a recent survey of the complex moment problem see also [20]). one following an idea due to marcel riesz (for continuation see [12, 13, 14]) and the other via positive deﬁnite extendibility (see [27, 9]). as is well-known, positive deﬁniteness is not suﬃcient for solving the complex moment problem (see [19, 3]). the present paper carries on with the study of [27] which characterizes solving the complex moment problem by extending a given sequence deﬁned on the integer lattice points of the ﬁrst quarter to a positive deﬁnite sequence on the lattice points of the upper diagonal half-plane. one may expect a relationship between the uniqueness of extending sequence on one hand and the determinacy of the resulting moment sequence. this question leads to quite a number of interesting thoughts which are exposed in this paper. our results, which are diverse in nature, are supported by elucidative examples and lead eventually to an open problem discussed on the ﬁnal pages of the paper. 2010 mathematics subject classiﬁcation. primary 44a60, 43a35; secondary 14p05. key words and phrases. positive deﬁnite sequences, complex moment problem, hamburger moment problem, herglotz moment problem, real algebraic set. this work was supported by the ncn (national science center), decision no. dec- 2013/11/b/st1/03613. 1 2 d. cicho ´n, j. stochel, f.h. szafraniec "
1803.03067," we present the mac network, a novel fully differentiable neural network archi- tecture, designed to facilitate explicit and expressive reasoning. mac moves away from monolithic black-box neural architectures towards a design that encourages both transparency and versatility. the model approaches problems by decompos- ing them into a series of attention-based reasoning steps, each performed by a novel recurrent memory, attention, and composition (mac) cell that maintains a separation between control and memory. by stringing the cells together and im- posing structural constraints that regulate their interaction, mac effectively learns to perform iterative reasoning processes that are directly inferred from the data in an end-to-end approach. we demonstrate the model’s strength, robustness and in- terpretability on the challenging clevr dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. more importantly, we show that the model is computationally-efﬁcient and data-efﬁcient, in particular requiring 5x less data than existing models to achieve strong results. 1 "
1803.03069," in this paper, we ﬁnd necessary and suﬃcient conditions for the bound- edness of fractional maximal operator mα on orlicz spaces. as an appli- cation of this results we consider the boundedness of fractional maximal commutator mb,α and nonlinear commutator of fractional maximal oper- ator [b, mα] on orlicz spaces, when b belongs to the lipschitz space, by which some new characterizations of the lipschitz spaces are given. 42b25, 46e30, 47b47, 26a16 ams mathematics subject classiﬁcation: key words: fractional maximal function; commutator; lipschitz space; orlicz space 1 "
1803.03071," detailed knowledge of the variability of classical cepheids, in particular their mod- ulations and mode composition, provides crucial insight into stellar structure and pulsation. however, tiny modulations of the dominant radial-mode pulsation were recently found to be very frequent, possibly ubiquitous in cepheids, which makes secondary modes diﬃcult to detect and analyse, since these modulations can easily mask the potentially weak secondary modes. the aim of this study is to re-investigate the secondary mode content in the sample of ogle-iii and -iv single-mode clas- sical cepheids using kernel regression with adaptive kernel width for pre-whitening, instead of using a constant-parameter model. this leads to a more precise removal of the modulated dominant pulsation, and enables a more complete survey of secondary modes with frequencies outside a narrow range around the primary. our analysis re- veals that signiﬁcant secondary modes occur more frequently among ﬁrst overtone cepheids than previously thought. the mode composition appears signiﬁcantly dif- ferent in the large and small magellanic clouds, suggesting a possible dependence on chemical composition. in addition to the formerly identiﬁed non-radial mode at p2 ≈ 0.6 . . . 0.65p1 (0.62-mode), and a cluster of modes with near-primary frequency, we ﬁnd two more candidate non-radial modes. one is a numerous group of secondary modes with p2 ≈ 1.25p1, which may represent the fundamental of the 0.62-mode, supposed to be the ﬁrst harmonic of an l ∈ {7, 8, 9} non-radial mode. the other new mode is at p2 ≈ 1.46p1, possibly analogous to a similar, rare mode recently discovered among ﬁrst overtone rr lyrae stars. key words: methods:statistical – stars:oscillations – stars:variables:cepheids – mag- ellanic cloud "
1803.03076," we introduce a method to obtain one-dimensional collective variables for studying rarely oc- curring transitions between two metastable states separated by a high free energy barrier. no previous information, not even approximated, on the path followed during the transition is needed. the only requirement is to know the ﬂuctuations of the system while in the two metastable states. with this information in hand we build the collective variable using a modi- ﬁed version of fishers linear discriminant analysis. the usefulness of this approach is tested on the metadynamics simulation of two representative systems. the ﬁrst is the freezing of silver iodide into the superionic α-phase, the second is the study of a classical diels alder reaction. the collective variable works very well in these two diverse cases. "
1803.03080,"— this paper extends the concept of scalar cepstrum coefﬁcients from single-input single-output linear time invariant dynamical systems to multiple-input multiple-output models, making use of the transfer function. these coefﬁcients are interpreted in terms of poles and transmission zeros of the underlying dynamical system. the smith-mcmillan form of we present a method to compute the mimo cepstrum based on input/output signal data for systems with square transfer function matrices (i.e. systems with as many inputs as outputs). this allows us to do a model-free analysis. two examples to illustrate these results are included: a simple mimo system with 3 inputs and 3 outputs, of which the poles and zeros are known exactly, that allows us to directly verify the equivalences derived in the paper, and a case study on realistic data. this case study analyses data coming from a (model of) a non-isothermal continuous stirred tank reactor, which experiences linear fouling. we analyse normal and faulty operating behaviour, both with and without a controller present. we show that the cepstrum detects faulty behaviour, even when hidden by controller compensation. the code for the numerical analysis is available online. "
1803.03081,". in chomp on graphs, two players alternatingly pick an edge or a vertex from a graph. the player that cannot move any more loses. the questions one wants to answer for a given graph are: which player has a winning strategy? can a explicit strategy be devised? we answer these questions (and determine the nim-value) for the class of generalized kneser graphs and for several families of johnson graphs. we also generalize some of these results to the clique complexes of these graphs. furthermore, we determine which player has a winning strategy for some classes of threshold graphs. "
1803.03082,". in this paper, we provide an eﬀective method to compute the topological entropies of g-subshifts of ﬁnite type (g-sfts) with g = fd and sd, the free group and free semigroup with d generators respectively. we develop the entropy formula by analyzing the corre- sponding systems of nonlinear recursive equations (snres). four types of snres of s2-sfts, namely the types e, d, c and o, are introduced and we could compute their entropies explicitly. this enables us to give the complete characterization of s2-sfts on two symbols. that is, the set of entropies of s2-sfts on two symbols is equal to e ∪ d ∪ c ∪ o. the methods developed in sd-sfts will also be applied to the study of the entropy theory of fd-sfts. the entropy formulae of sd-, fd-golden mean shifts and k-colored chessboards are also presented herein. "
1803.03083,. the equivariant euler characteristics of the buildings for the symplectic groups over ﬁnite ﬁelds are determined. contents 
1803.03084," upcoming fast radio burst (frb) surveys will search ∼10 3 beams on sky with very high duty cycle, generating large numbers of single-pulse candidates. the abundance of false positives presents an intractable problem if candidates are to be inspected by eye, making it a good application for artiﬁcial intelligence (ai). we apply deep learning to single pulse classiﬁcation and develop a hierarchical framework for ranking events by their probability of being true astrophysical transients. we construct a tree-like deep neural network (dnn) that takes multiple or individual data products as input (e.g. dynamic spectra and multi-beam detection information) and trains on them simultaneously. we have built training and test sets using false-positive triggers from real telescopes, along with simulated frbs, and single pulses from pulsars. training of the dnn was independently done for two radio telescopes: the chime pathﬁnder, and apertif on westerbork. high accuracy and recall can be achieved with a labelled training set of a few thousand events. even with high triggering rates, classiﬁcation can be done very quickly on graphical processing units (gpus). that speed is essential for selective voltage dumps or issuing real-time voevents. next, we investigate whether dedispersion back-ends could be completely replaced by a real-time dnn classiﬁer. it is shown that a single forward propagation through a moderate convolutional network could be faster than brute-force dedispersion; but the low signal-to-noise per pixel makes such a classiﬁer sub-optimal for this problem. real-time automated classiﬁcation may prove useful for bright, unexpected signals, both now and in the era of radio astronomy when data volumes and the searchable parameter spaces further outgrow our ability to manually inspect the data, such as for ska and ngvla. 8 1 0 2   r a m   8     ] m i . h p - o r t s a [     1 v 4 8 0 3 0 . 3 0 8 1 : v i x r a liam.dean.connor@gmail.com 2 connor & van leeuwen "
1803.03085," the generalized partially linear models on riemannian manifolds are introduced. these models, like ordinary generalized linear models, are a generalization of partially linear models on riemannian manifolds that allow for response variables with error distribution models other than a normal distribution. partially linear models are particularly useful when some of the covariates of the model are elements of a riemannian man- ifold, because the curvature of these spaces makes it diﬃcult to deﬁne parametric models. the model was developed to address an interesting application, the prediction of children’s garment ﬁt based on 3d scanning of their body. for this reason, we focus on logistic and ordinal models and on the important and diﬃcult case where the riemannian manifold is the three-dimensional case of kendall’s shape space. an experimental study with a well-known 3d database is carried out to check the goodness of the procedure. finally it is applied to a 3d database obtained from an anthropometric survey of the spanish child population. a comparative study with related techniques is carried out. keywordshape space;statistical shape analysis; generalized linear models; partially linear models; kernel regression; children’s wear. 1 "
1803.03086,". this paper considers the topological degree of g-shifts of ﬁnite type for the case where g is a nonabelian group. whenever the cayley graph of g has a ﬁnite representation and the relationships among the generators of g are determined by a matrix a, the coeﬃcients of the characteristic polynomial of a are revealed as the number of children of the graph. after introducing an algorithm for the computation of the degree, the degree spectrum, which is ﬁnite, relates to a collection of matrices in which the sum of each row of every matrix is bounded by the number of children of the graph. furthermore, the algorithm extends to g of ﬁnite followers. "
1803.03088," we empirically verify that the market capitalisations of coins and tokens in the cryptocur- rency universe follow power-law distributions with signiﬁcantly diﬀerent values, with the tail exponent falling between 0.5 and 0.7 for coins, and between 1.0 and 1.3 for tokens. we provide a rationale for this, based on a simple proportional growth with birth & death model previ- ously employed to describe the size distribution of ﬁrms, cities, webpages, etc. we empirically validate the model and its main predictions, in terms of proportional growth (gibrat’s law) of the coins and tokens. estimating the main parameters of the model, the theoretical predictions for the power-law exponents of coin and token distributions are in remarkable agreement with the empirical estimations, given the simplicity of the model. our results clearly characterize coins as being “entrenched incumbents” and tokens as an “explosive immature ecosystem”, largely due to massive and exuberant initial coin oﬀering activity in the token space. the theory predicts that the exponent for tokens should converge to 1 in the future, reﬂecting a more reasonable rate of new entrants associated with genuine technological innovations. zurich, switzerland, email: kwu@student.ethz.ch ∗corresponding author. address: chair of entrepreneurial risks at eth zurich, scheuchzerstrasse 7, 8092 †eth zürich, dept. of management, technology and economics, zürich, switzerland ‡also at the swiss finance institute, c/o university of geneva, switzerland 8 1 0 2   y a m 6 1       ] h p - c o s . s c i s y h p [     2 v 8 8 0 3 0 . 3 0 8 1 : v i x r a 1 "
1803.03089,": the dc resistivity of charge density waves weakly-pinned by disorder is controlled by diﬀusive, incoherent processes rather than slow momentum relaxation. the corresponding incoherent conductivity can be computed in the limit of zero disor- der. we compute this transport coeﬃcient in holographic spatially modulated break- ing translations spontaneously. as a by-product of our analysis, we clarify how the boundary heat current is obtained from a conserved bulk current, deﬁned as a suitable generalization of the iyer-wald noether current of the appropriate killing vector. contents "
1803.03093," swarm systems consist of large numbers of robots that collaborate autonomously. with an appropriate level of human control, swarm systems could be applied in a variety of contexts ranging from search-and-rescue situations to cyber defence. the two decision making cycles of swarms and humans operate on two different time-scales, where the former is normally orders of magnitude faster than the latter. closing the loop at the intersection of these two cycles will create fast and adaptive human- swarm teaming networks. this paper brings desperate pieces of the ground work in this research area together to review this multidisciplinary literature. we conclude with a framework to synthesize the ﬁndings and summarize the multi-modal indicators needed for closed-loop human-swarm adaptive systems. human-swarm teaming (hst), human-swarm interaction (hsi), adaptive systems, automation index terms "
1803.03094," background: virtual machine (vm) consolidation is an eﬀective technique to improve resource utilization and reduce energy footprint in cloud data centers. it can be implemented in a centralized or a distributed fashion. distributed vm consolidation approaches are currently gaining popularity because they are often more scalable than their centralized counterparts and they avoid a single point of failure. objective: to present a comprehensive, unbiased overview of the state-of-the-art on distributed vm con- solidation approaches. method: a systematic mapping study (sms) of the existing distributed vm consolidation approaches. results: 19 papers on distributed vm consolidation categorized in a variety of ways. the results show that the existing distributed vm consolidation approaches use four types of algorithms, optimize a number of diﬀerent objectives, and are often evaluated with experiments involving simulations. conclusion: there is currently an increasing amount of interest on developing and evaluating novel dis- tributed vm consolidation approaches. a number of research gaps exist where the focus of future research may be directed. keywords: cloud computing, data center, virtual machine, consolidation, placement, energy-eﬃciency "
1803.03095," we propose a novel crowd counting approach that lever- ages abundantly available unlabeled crowd imagery in a learning-to-rank framework. to induce a ranking of cropped images , we use the observation that any sub-image of a crowded scene image is guaranteed to contain the same number or fewer persons than the super-image. this al- lows us to address the problem of limited size of existing datasets for crowd counting. we collect two crowd scene datasets from google using keyword searches and query- by-example image retrieval, respectively. we demonstrate how to efﬁciently learn from these unlabeled datasets by in- corporating learning-to-rank in a multi-task network which simultaneously ranks images and estimates crowd density maps. experiments on two of the most challenging crowd counting datasets show that our approach obtains state-of- the-art results. "
1803.03098,". we give a short proof that the quartic polynomial f (z) = 1 is univalent, i.e. injective in the open unit disc, d = { z ∈ c | (cid:107)z(cid:107) < 1 }. 6 z4 + 2 3 z3 + 7 6 z2 + z "
1803.03099,. the existence and construction of periodic approximations with convergent spectra is crucial in solid state physics for the spectral study of corresponding schr¨odinger operators. in a forthcoming work [9] this task was boiled down to the existence and construction of periodic approximations of the underlying dynamical systems in the hausdorﬀ topology. as a result the one-dimensional systems admitting such approximations are completely classiﬁed in the present work. in addition explicit constructions are provided for dynamical systems deﬁned by primitive substitutions covering all studied examples such as the fibonacci sequence or the golay-rudin-shapiro sequence. one main tool is the description of the hausdorﬀ topology by the local pattern topology on the dictionaries as well as the gap-graphs describing the local structure. the connection of branching vertices in the gap-graphs and defects is discussed. contents 
1803.03102,. in this paper we derive quantitative conditions under which a compactly supported drift term depending on the direction of propagation blocks a traveling wave solution or lets it pass almost unchanged. we give explicit conditions on the drift term for blocking as well as almost unchanged propagation in one spacial dimension. 
1803.03103," 3 purpose: to characterize the noise distributions in 3d-mri accelerated acquisitions reconstructed with grappa using an exact noise propagation analysis that operates directly in k–space. theory and methods: we exploit the extensive symmetries and separability in the reconstruction steps to account for the correlation between all the acquired k–space samples. monte carlo simulations and multi- repetition phantom experiments were conducted to test both the accuracy and feasibility of the proposed method; an in–vivo experiment was performed to assess the applicability of our method to clinical scenarios. results: our theoretical derivation shows that the direct k–space analysis renders an exact noise char- acterization under the assumptions of stationarity and uncorrelation in the original k–space. simulations and phantom experiments provide empirical support to the theoretical proof. finally, the in–vivo experiment demonstrates the ability of the proposed method to assess the impact of the sub-sampling pattern on the overall noise behavior. conclusions: by operating directly in the k–space, the proposed method is able to provide an exact character- ization of noise for any cartesian pattern sub-sampled along the two phase–encoding directions. exploitation of the symmetries and separability into independent blocks through the image reconstruction procedure al- lows us to overcome the computational challenges related to the very large size of the covariance matrices involved. "
1803.03104," quantifying similarity between data objects is an important part of modern data science. deciding what similarity measure to use is very application dependent. in this paper, we combine insights from systems theory and machine learning, and investigate the weighted cepstral distance, which was previously deﬁned for signals coming from arma models. we provide an extension of this distance to invertible deterministic linear time invariant single input single output models, and assess its applicability. we show that it can always be interpreted in terms of the poles and zeros of the underlying model, and that, in the case of stable, minimum-phase, or unstable, maximum-phase models, a geometrical interpretation in terms of subspace angles can be given. we then devise a method to assess stability and phase-type of the generating models, using only input/output signal information. in this way, we prove a connection between the extended weighted cepstral distance and a weighted cepstral model norm. in this way, we provide a purely data-driven way to assess diﬀerent underlying dynamics of input/output signal pairs, without the need for any system identiﬁcation step. this can be useful in machine learning tasks such as time series clustering. an ipython tutorial is published complementary to this paper, containing implementations of the various methods and algorithms presented here, as well as some numerical illustrations of the equivalences proven here. key words: dynamic systems, metrics, subspace methods, time series analysis, deterministic systems "
1803.03106,"—growing trafﬁc demands and increasing security awareness is driving the need for secure services. current solu- tions require manual conﬁguration and deployment based on the customer’s requirements. in this work, we present an architecture for an automatic intent-based provisioning of a secure service in a multilayer — ip, ethernet and optical — network while choosing the appropriate encryption layer using an open-source sdn orchestrator. the approach is experimentally evaluated in a testbed with commercial equipment. results indicate that the processing impact of secure channel creation on a controller is negligible. as the time for setting up services over wdm is varying between technologies, it needs to be taken into account in the decision process. "
1803.03109," in this paper, a king-type modiﬁcation of (p, q)-lupa¸s bernstein operators are introduced. the rate of convergence of these operators are studied by means of modulus of continuity and lipschitz class functional. further, it has been shown that the error estimation of these operators on some subintervals of [0, 1] are better than the (p, q)-lupa¸s bernstein operators. keywords and phrases: (p, q)-integers; (p, q)-bernstein operators, (p, q)-lupa¸s bernstein operators, king type approximation, modulus of continuity. msc: primary 65d17; secondary 41a10, 41a25, 41a36.: 1 "
1803.03110,". we derive two generalizations of gasper’s transformation formula for basic hypergeometric series. using these generalized formulas, we give explicit expressions for the coeﬃcients of three-term relations for the basic hypergeometric series 2φ1, which are generalizations of the author’s previous results on three-term relations for 2φ1. "
1803.03111," launching orbital and suborbital rockets from a high altitude is beneﬁcial because of e.g. noz- zle optimisation and reduced drag. aeroplanes and gas balloons have been used for the pur- pose. here we present a concept where a balloon is ﬁlled with pure water vapour on ground so that it rises to the launch altitude. the system resembles a gas balloon because no onboard energy source is carried and no hard objects fall down. we simulate the ascent behaviour of the balloon. in the baseline simulation, we consider a 10 tonne rocket lifted to an altitude of 18 km. we model the trajectory of the balloon by taking into account steam adiabatic cooling, surface cooling, water condensation and balloon aerodynamic drag. the required steam mass proves to be only 1.4 times the mass of the rocket stage and the ascent time is around 10 minutes. for small payloads, surface cooling increases the relative amount of steam needed, unless insulation is applied to the balloon skin. the ground-ﬁlled steam balloon seems to be an attractive and sustainable method of lifting payloads such as rockets into high altitude. received dd mm yyyy; revised dd mm yyyy; accepted dd mm yyyy. 2 the aeronautical journal cross-sectional area of balloon (m2) drag coeﬃcient speciﬁc heat capacity of air in constant pressure (j kg−1 k−1) drag force (n) rejected fraction of condensed water acceleration due to gravity, g = 9.81 m s−2 thermal conductivity of air (w k−1 m−1) boltzmann constant (j k−1) length scale (m) height of cylinder approximating balloon (m) payload mass (kg) steam mass (kg) atmospheric number density (m−3) nomenclature a askin atmosphere facing area of steam-ﬁlled volume (m2) cd cp fd fr g k kb l lz mpay ms n nul nusselt number corresponding to length scale l p pressure (pa) pr prandtl number of air, pr = ν/α r radius of balloon (m) reynolds number or air corresponding to length scale l rel t temperature (k) uconv convective cooling rate of balloon skin (w m−2) radiative cooling rate of balloon skin (w m−2) urad vertical velocity (m s−1)  steam volume (m3) v greek symbols α γ  ν µ ρ σsb thermal diﬀusivity of air (m2 s−1) adiabatic index of water vapour, γ = 1.324 thermal infrared emissivity of balloon skin kinematic viscosity of air (m2 s−1) dynamic viscosity of air, µ = ρν (pa s) atmospheric mass density (kg m−3) stefan-boltzmann constant, σsb = 5.67 · 10−8 w m−2 k−4 "
1803.03113,. this paper focuses at the various stability results of reciprocal- nonic and reciprocal-decic functional equations in non-archimedean ﬁelds and illustrations of the proper examples for their non-stabilities. 
1803.03114," the enormous amount of data to be represented using large graphs exceeds in some cases the resources of a conventional computer. edges in particular can take up a considerable amount of memory as com- pared to the number of nodes. however, rigorous edge storage might not always be essential to be able to draw the needed conclusions. a similar problem takes records with many variables and attempts to extract the most discernible features. it is said that the “dimension” of this data is reduced. following an approach with the same objec- tive in mind, we can map a graph representation to a k-dimensional space and answer queries of neighboring nodes by measuring euclidean distances. the accuracy of our answers would decrease but would be compensated for by fuzzy logic which gives an idea about the likelihood of error. this method allows for reasonable representation in memory while maintaining a fair amount of useful information. promising pre- liminary results are obtained and reported by testing the proposed approach on a number of facebook graphs. 1 "
1803.03115," heun functions generalize well-known special functions such as spheroidal wave, lam´e, mathieu, and hypergeometric–type functions. they are applicable to diverse areas such as theory of black holes, lattice systems in statistical mechanics, solutions of the schr¨odinger equation of quantum mechanics, and addition of three quantum spins. we consider the radius of convergence of the heun function by rearranging the order of the terms in its power series. and we show why the poincar´e–perron (p–p) theorem is only available for the conditional convergence since it is applied to the heun’s equation. moreover, we construct the revised cauchy ratio test in which is suitable for the three term recurrence relation in a power series. keywords: heun functions; three term recurrence relation; approximative expansions 2000 msc: 33e30, 34a99, 34e05 "
1803.03117," to kinetically model implosion and explosion related phenomena, we present a theoretical framework for constructing discrete boltzmann model(dbm) with spherical symmetry in spherical coordinates. to this aim, a key technique is to use local cartesian coordinates to describe the particle velocity in the kinetic model. thus, the geometric eﬀects, like the divergence and convergence, are described as a “force term”. to better access the nonequilibrium behavior, even though the corresponding hydrodynamic model is one-dimensional, the dbm uses a discrete velocity model(dvm) with 3 dimensions. a new scheme is in- troduced so that the dbm can use the same dvm no matter considering the extra degree of freedom or not. as an example, a dvm with 26 velocities is formulated to construct the dbm in the navier-stokes level. via the dbm, one can study simultaneously the hydrodynamic and thermodynamic nonequi- librium behaviors in the implosion and explosion process which are not very close to the spherical center. the extension of current model to the multiple- relaxation-time version is straightforward. keywords: discrete boltzmann model, compressible ﬂows, spherical symmetry, geometric eﬀects, thermodynamic non-equilibrium ∗corresponding author email address: xu_aiguo@iapcm.ac.cn (aiguo xu) preprint submitted to journal of latex templates march 9, 2018 "
1803.03118," in this paper, poisson wavelets on n-dimensional spheres, derived from poisson kernel, are introduced and characterized. we compute their gegenbauer expansion with respect to the origin of the sphere, as well as with respect to the ﬁeld source. further, we give recursive formulae for their explicit representations and we show how the wavelets are localized in space. also their euclidean limit is calculated explicitly and its space localization is described. we show that poisson wavelets can be treated as wavelets derived from approximate identities and we give two inversion formulae. keywords: poisson wavelets, n-spheres ams classiﬁcation: 42c40 "
1803.03119," the paper shows that under some mild conditions n-dimensional spher- ical wavelets derived from approximate identities build semi-continuous frames. moreover, for suﬃciently dense grids poisson wavelets on n- dimensional spheres constitute a discrete frame. in the proof we only use the localization properties of the reproducing kernel and its gradient. keywords: spherical wavelets, poisson wavelets, frames, n-spheres ams classiﬁcation: 42c40 "
1803.03120," directional poisson wavelets, being directional derivatives of poisson kernel, are introduced on n–dimensional spheres. it is shown that, slightly modiﬁed and together with another wavelet family, they are an admissible wavelet pair according to the deﬁnition derived from the theory of ap- proximate identities. we investigate some of the properties of directional poisson wavelets, such as recursive formulae for their fourier coeﬃcients or explicit representations as functions of spherical variables (for some of the wavelets). we derive also an explicit formula for their euclidean limits. keywords:spherical directional wavelets, poisson multipole wavelets, n-spheres ams classiﬁcation: 42c40, 42b20 "
1803.03121," the main object of this paper is to present generalizations of gamma, beta and hyper- geometric functions. some recurrence relations, transformation formulas, operation formulas and integral representations are obtained for these new generalizations. keywords: gamma function, beta function, wright function, gauss hypergeometric func- tion, conﬂuent hypergeometric function, mellin transform 1 "
1803.03122," the main object of this paper is to present a new generalized beta function which de- ﬁned by three parametres mittag-leﬄer function. we also introduce new generalizations of hypergeometric and conﬂuent hypergeometric functions with the help of new generalized beta function. furthermore, we obtained various properties of these functions such as integral rep- resentations, mellin transforms, diﬀerentiation formulas, transformation formulas, recurrence relations and summation formula. keywords. gamma function, beta function, gauss hypergeometric function, conﬂuent hypergeometric function, mittag-leﬄer function. (1) (2) 1 "
1803.03125," age is an important characteristic of a planetary system, but also one that is difﬁcult to determine. assuming that the host star and the planets are formed at the same time, the challenge is to determine the stellar age. asteroseismology provides precise age determination, but in many cases the required detailed pulsa- tion observations are not available. here we concentrate on other techniques, which may have broader applicability but also serious limitations. further development of this area requires improvements in our understanding of the evolution of stars and their age-dependent characteristics, combined with observations that allow reliable calibration of the various techniques. "
1803.03126," in this work, the relativistic phenomena of lorentz-fitzgerald contrac- tion and time dilation are derived using a modiﬁed distance formula that is appropriate for discrete space. this new distance formula is diﬀerent than the pythagorean theorem but converges to it for distances large relative to the planck length. first, four candidate formulas developed by diﬀerent people over the last 70 years will be discussed. three of the formulas are shown to be identical for conditions that best describe discrete space. it is shown that this new distance formula is valid for all size-scales – from the planck length upwards – and solves two major problems historically asso- ciated with the discrete space-time (dst) model. one problem it solves is the widely believed anisotropic nature of most discrete space models. just as commonly believed is the second problem – the incompatibility of dst’s concept of an immutable atom of space and the length contraction of this atom required by special relativity. the new formula for distance in dst solves this problem. it is shown that length contraction of the atom of space does not occur for any relative velocity of two reference frames. it is also shown that time dilation of the atom of time does not occur. also discussed is the possibility of any object being able to travel at the speed of light for speciﬁc temporal durations given by an equation derived in this work. also discussed is a method to empirical verify the discreteness of space by studying any observed anomalies in the motion of astronomical bodies, such as diﬀerences in the bodies’ inertial masses and gravitational masses. the importance of the new distance formula for causal set theory and other theories of quantum gravity is also discussed. keywords: distance formula · discrete space · discrete time · pythagorean theorem · special relativity · general relativity · quantum gravity · causal set theory · dark energy · dark matter · logical positivism 1 1 "
1803.03128, we make a few general comments on the renormalization group ﬂows in certain yang-mills theories in the vicinity of phase transitions. we then present a model in d = 5 with non-periodic boundary conditions where a possible rg ﬂow starts from the trivial point and ends on a quantum phase transition. near the endpoint of the ﬂow interesting comments can be made about the higgs hierarchy problem. 8 1 0 2   r a m 8       ] h t - p e h [     1 v 8 2 1 3 0 . 3 0 8 1 : v i x r a 1 
1803.03130," for the quadratic family fc(z) = z2 + c with c in the exterior of the mandelbrot set, it is known that every point in the julia set moves holomorphically. let ˆc be a semi- in this paper we prove hyperbolic parameter in the boundary of the mandelbrot set. that for each z = z(c) in the julia set, the derivative dz(c)/dc is uniformly o(1/(cid:112)|c − ˆc|) when c belongs to a parameter ray that lands on ˆc. we also characterize the degeneration of the dynamics along the parameter ray. 8 1 0 2   r a m 8       ] s d h t a m . [     1 v 0 3 1 3 0 . 3 0 8 1 : v i x r a "
1803.03132, we present a fisher matrix forecast for the sensitivity on the mass of a thermal warm dark matter (wdm) particle from current (des-like) and future (lsst-like) photometric galaxy surveys using the galaxy angular power spectrum. we model the nonlinear clustering using a a modiﬁed halo model proposed to account for wdm eﬀects. we estimate that from this observable alone a lower bound of mwdm > 647 ev (mwdm > 126 ev) for the lsst (des) case could be obtained. key words: galaxy surveys – warm dark matter 
1803.03135," h(a) ≡ 4f3(a, a, a, a; 2a, a + 1, a + 1; 1) (1.1) in closed form [1]? after considerable analysis, the answer appears to be “no”, but during the attempt to answer this question, a number of interesting (and unexpected) related results were obtained, either as specialized transformations, or as closed-form expressions for several related functions. the purpose of this paper is to record and review both the methods attempted and the related identities obtained (speciﬁcally new 4f3(1), 5f6(1) and (generalized euler) sums containing digamma functions) - the former for their educational merit, since they appear to be not-very-well-known, the latter because they do not appear to exist in the literature. 1 "
1803.03137," in this paper, we discuss the (co)homology theory of biquandles, derived biquandle cocycle invariants for oriented surface-links using broken surface dia- grams and how to compute the biquandle cocycle invariants from marked graph diagrams. we also develop the shadow (co)homology theory of biquandles and construct the shadow biquandle cocycle invariants for oriented surface-links. 1 "
1803.03141,": we consider an i.i.d. supercritical bond percolation on zd, every edge is open with a probability p > pc(d), where pc(d) denotes the critical pa- rameter for this percolation. we know that there exists almost surely a unique inﬁnite open cluster cp [18]. we are interested in the regularity properties of two distinct objects deﬁned on this inﬁnite cluster: the isoperimetric (or cheeger) constant, and the chemical distance for supercritical bernoulli percolation. the chemical distance between two points x, y ∈ cp corresponds to the length of the shortest path in cp joining the two points. the chemical distance between 0 and nx grows asymptotically like nµp(x). we aim to study the regularity properties of the map p → µp on the supercritical regime. this may be seen as a special case of ﬁrst passage percolation where the distribution of the passage time is gp = pδ1 + (1 − p)δ∞, p > pc(d). it is already known that the map p → µp is continuous (see [15]). we prove here that p → µp satisﬁes stronger regularity properties, this map is almost lipschitz continuous up to a logarithmic factor in [p0, 1] for any p0 > pc(d). we prove an analog result for the cheeger constant in dimension 2 for all intervals [p0, p1] ⊂ (1/2, 1). for d ≥ 3, we prove that the modiﬁed cheeger constant deﬁned in [16] is lipschitz continuous on all intervals [p0, p1] ⊂ (pc(d), 1). ams 2010 subject classiﬁcations: primary 60k35, secondary 82b43. keywords: regularity, percolation, time constant, isoperimetric constant. 1 "
1803.03142,". we prove that a linear mapping on the algebra sln of all trace zero com- plex matrices is a local automorphism if and only if it is an automorphism or an anti-automorphism. we also show that a linear mapping on a simple leibniz algebra of the form sln ˙+i is a local automorphism if and only if it is an automorphism. we give examples of ﬁnite-dimensional nilpotent lie algebras l with dim l ≥ 3 which admit local automorphisms which are not automorphisms. keywords: simple lie algebra sln, simple leibniz algebra, automorphism, local au- tomorphism. ams subject classiﬁcation: 17a36, 17b20, 17b40. "
1803.03143,": in this paper, a novel formula expressing explicitly the fractional-order derivatives, in the sense of riesz-feller operator, of jacobi polynomials is presented. jacobi spectral collocation method together with trapezoidal rule are used to reduce the fractional l´evy- feller advection-dispersion equation (lfade) to a system of algebraic equations which greatly simpliﬁes solving like this fractional diﬀerential equation. numerical simulations with some comparisons are introduced to conﬁrm the eﬀectiveness and reliability of the proposed technique for the l´evy-feller fractional partial diﬀerential equations. keywords: l´evy-feller advection-dispersion equation; riesz-feller fractional derivative; spectral method; jacobi polynomials; trapezoidal rule. 1 "
1803.03144,". by a result of vallette [val14], we put a sensible model structure on the category of conilpotent lie coalgebras. this gives us a powerful tool to study the subcategory of lie algebras obtained by linear dualization, also known as the category of pronilpotent lie algebras. this way, we recover weaker versions of the celebrated goldman–millson theorem and dolgushev–rogers theorem by purely homotopical methods. we explore the relations of this procedure with the existent literature, namely the works of lazarev–markl and buijs–f´elix–murillo–tanr´e. contents "
1803.03145,"—this paper presents a novel method for synthe- sizing new physical layer modulation and coding schemes for communications systems using a learning-based approach which does not require an analytic model of the impairments in the channel. it extends prior work published on the channel autoencoder to consider the case where the channel response is not known or can not be easily modeled in a closed form analytic expression. by adopting an adversarial approach for channel response approximation and information encoding, we can jointly learn a good solution to both tasks over a wide range of channel environments. we describe the operation of the proposed adversarial system, share results for its training and validation over-the-air, and discuss implications and future work in the area. index terms—machine learning; deep learning; neural net- works; autoencoders; generative adversarial networks; modula- tion; neural networks; software radio "
1803.03147," the recently introduced {k}-packing function problem is considered in this paper. special relation between a case when k = 1, k ≥ 2 and linear pro- gramming relaxation is introduced with suﬃcient conditions for optimality. for arbitrary simple connected graph g there is construction procedure for ﬁnding values of k for which l{k}(g) can be determined in the polynomial time. additionally, relationship between {1}-packing function and indepen- dent set number is established. optimal values for some special classes of graphs and general upper and lower bounds are introduced. keywords: {k}-packing function problem, independent set, dominating set, integer linear programming. 2008 msc: 05c69, 05c12 "
1803.03148," in this paper, we present a technique for generating artiﬁcial datasets that retain statistical properties of the real data while providing differential privacy guaran- tees with respect to this data. we include a gaussian noise layer in the discrim- inator of a generative adversarial network to make the output and the gradients differentially private with respect to the training data, and then use the generator component to synthesise privacy-preserving artiﬁcial dataset. our experiments show that under a reasonably small privacy budget we are able to generate data of high quality and successfully train machine learning models on this artiﬁcial data. 1 "
1803.03150," we study the semilinear elliptic equation −∆u + g(u)σ = µ with dirichlet bound- ary condition in a smooth bounded domain where σ is a nonnegative radon measure, µ a radon measure and g is an absorbing nonlinearity. we show that the problem is well posed if we assume that σ belongs to some morrey class. under this condition we give a general existence result for any bounded measure provided g satisﬁes a subcritical integral assump- tion. we study also the supercritical case when g(r) = |r|q−1 r, with q > 1 and µ satisﬁes an absolute continuity condition expressed in terms of some capacities involving σ. 2010 mathematics subject classiﬁcation. 35 j 61; 31 b 15; 28 c 05 . key words: radon measures; morrey class; capacities; potential estimates; θ-regular measures . contents "
1803.03151,". we introduce the notion of a whitney dual of a graded poset. two posets are whitney duals to each other if (the absolute value of) their whitney numbers of the ﬁrst and second kind are interchanged between the two posets. we deﬁne new types of edge and chain-edge labelings which we call whitney labelings. we prove that every graded poset with a whitney labeling has a whitney dual. moreover, we show how to explicitly construct a whitney dual using a technique involving quotient posets. as applications of our main theorem, we show that geometric lattices, the lattice of non- crossing partitions, the poset of weighted partitions studied by gonz´alez d’le´on-wachs, and most of the r∗s-labelable posets studied by simion-stanley all have whitney duals. our technique gives a combinatorial description of a whitney dual of the noncrossing par- tition lattice in terms of a family of noncrossing dyck paths. our method also provides an explanation of the whitney duality between the poset of weighted partitions and a poset of rooted forests studied by reiner and sagan. an integral part of this explanation is a new chain-edge labeling for the poset of weighted partitions which we show is a whitney labeling. finally, we show that a graded poset with a whitney labeling admits a local action of the 0-hecke algebra of type a on its set of maximal chains. the characteristic of the associated representation is ehrenborg’s ﬂag quasisymmetric function. the existence of this action implies, using a result of mcnamara, that when the maximal intervals of the constructed whitney duals are bowtie-free, they are also snellable. in the case where these maximal intervals are lattices, they are supersolvable. keywords: graded posets, whitney numbers, whitney duality, edge labelings, chain- edge labelings, quotient posets, noncrossing partitions, weighted partitions, rooted forests, 0-hecke algebra actions, ﬂag quasisymmetric function. contents "
1803.03154," this article is devoted to study the eﬀects of the s-periodical frac- tional diﬀerencing ﬁlter (1 − ls)dt . to put this eﬀect in evidence, we have derived the periodic auto-covariance functions of two distinct uni- variate seasonally fractionally diﬀerenced periodic models. a multivariate representation of periodically correlated process is exploited to provide the exact and approximated expression auto-covariance of each models. the distinction between the models is clearly obvious through the ex- pression of periodic auto-covariance function. besides producing diﬀerent auto-covariance functions, the two models diﬀer in their implications. in the ﬁrst model, the seasons of the multivariate series are separately fractionally integrated. in the second model, however, the seasons for the univariate series are fractionally co-integrated. on the simulated sample, for each models, with the same parameters, the empirical periodic auto- covariance are calculated and graphically represented for illustrating the results and support the comparison between the two models. 1 "
1803.03155," complex classiﬁers may exhibit “embarassing” failures in cases where a human can easily provide a justiﬁed classiﬁcation. avoiding such failures is obviously of key importance. in this work, we focus on one such setting, where a label is perfectly predictable if the input contains certain features, and otherwise it is predictable by a linear classiﬁer. we deﬁne a hypothesis class that captures this notion and determine its sample complexity. we also give evidence that efﬁcient algorithms cannot achieve this sample complexity. we then derive a simple and efﬁcient algorithm and give evidence that its sample complexity is optimal, among efﬁcient algorithms. experiments on synthetic and sentiment analysis data demonstrate the efﬁcacy of the method, both in terms of accuracy and interpretability. 1 "
1803.03156," the article contains a summary of fundamentals of the perturbational non- canon- ical molecular orbital (pncmo) theory formerly developed by the author. in some respects, the pncmo theory is a generalization of the well-known simple pmo the- ory: first, the usual diagonalization problem (and/or the eigenvalue equation) for a certain model hamiltonian matrix (h) is now replaced by two interrelated non- canonical one-electron problems, namely by the block-diagonalization problem for the matrix h following from the brillouin theorem and determining non-canonical (localized) mos (ncmos) and by the commutation equation for the respective one- electron density matrix (charge-bond order (cbo)) matrix. second, perturbative solutions of the above-speciﬁed alternative problems are sought in terms of entire submatrices (blocks) of the matrix h instead of usual matrix elements (e.g. of coulomb and resonance parameters). third, a generalized version of the perturba- tion theory (pt) is used in place of the standard rayleigh-schr¨odinger pt (rspt), wherein non-commutative quantities stand for the usual (commutative) ones (cf. the so-called non-commutative rspt (ncrspt)). as a result, algebraic expres- sions are derived for the principal quantum-chemical characteristics (including the cbo matrix, the ncmo representation matrix and the total energy) that embrace deﬁnite classes of hamiltonian matrices and thereby of molecules. to illustrate the point, saturated and conjugated hydrocarbons are taken as examples. arguments are given that the pncmo theory possibly forms the basis of a novel way of quali- tative chemical thinking. "
1803.03158," we study ﬁnite-state transducers and their power for transforming inﬁnite words. inﬁnite se- quences of symbols are of paramount importance in a wide range of ﬁelds, from formal languages to pure mathematics and physics. while ﬁnite automata for recognising and transforming lan- guages are well-understood, very little is known about the power of automata to transform inﬁnite words. the word transformation realised by ﬁnite-state transducers gives rise to a complexity compar- ison of words and thereby induces equivalence classes, called (transducer) degrees, and a partial order on these degrees. the ensuing hierarchy of degrees is analogous to the recursion-theoretic degrees of unsolvability, also known as turing degrees, where the transformational devices are turing machines. however, as a complexity measure, turing machines are too strong: they trivi- alise the classiﬁcation problem by identifying all computable words. finite-state transducers give rise to a much more ﬁne-grained, discriminating hierarchy. in contrast to turing degrees, hardly anything is known about transducer degrees, in spite of their naturality. we use methods from linear algebra and analysis to show that there are inﬁnitely many atoms in the transducer degrees, that is, minimal non-trivial degrees. we also show that there exists an uncomputable degree that has only uncomputable degrees and the trivial bottom degree below itself. keywords and phrases ﬁnite state transducers, inﬁnite words, inﬁnite sequences, streams, com- plexity, degrees 1 "
1803.03159," we consider the general n = 4, d= 3 galilean superalgebra with arbitrary central charges and study its dynamical realizations. using the nonlinear realization techniques, we intro- duce a class of actions for n = 4 three-dimensional non-relativistic superparticle, such that they are linear in the central charge maurer-cartan one-forms. as a prerequisite to the quantization, we analyze the phase space constraints structure of our model for various choices of the central charges. the ﬁrst class constraints generate gauge transformations, involving fermionic κ-gauge transformations. the quantization of the model gives rise to the collection of free n = 4, d= 3 galilean superﬁelds, which can be further employed, e.g., for description of three-dimensional non-relativistic n = 4 supersymmetric theories. pacs: 11.30.pb, 12.60.jv, 11.10.ef, 11.30.-j, 03.65.-w keywords: extended galilean supersymmetry, non-relativistic superﬁelds, non-relativistic superparticles, quantization methods 1 "
1803.03160," using a new construction of rational linear forms in odd zeta values and the saddle point method, we prove the existence of at least two irrational numbers amongst the 33 odd zeta values ζ(5), ζ(7), . . . , ζ(69). dedicated to christian krattenthaler, on his 60th birthday 1 "
1803.03162,". in this paper, we give an optimal inequality relating the relative yamabe invariant of a certain compactiﬁcation of a conformally compact poin- car´e-einstein manifold with the yamabe invariant of its boundary at inﬁnity. as an application, we obtain an elementary proof of the rigidity of the hyper- bolic space as the only conformally compact poincar´e-einstein manifold with the round sphere as its conformal inﬁnity. "
1803.03164,". we investigate the complexity consequences of adding pointer arithmetic to separation logic. speciﬁcally, we study extensions of the points-to fragment of symbolic-heap separation logic with various forms of presburger arithmetic constraints. most signiﬁcantly, we ﬁnd that, even in the minimal case when we allow only conjunctions of simple “diﬀerence constraints” x′ ≤ x ± k (where k is an integer), polynomial-time decidability is already impossible: satis- ﬁability becomes np-complete, while quantiﬁer-free entailment becomes conp-complete and quantiﬁed entailment becomes π p 2 is the second class in the polynomial-time hierarchy) in fact we prove that the upper bound is the same, π p 2 , even for the full pointer arithmetic but with a ﬁxed pointer oﬀset, where we al- low any boolean combinations of the elementary formulas (x′ = x + k0), (x′ ≤ x + k0), and (x′ < x + k0), and, in addition to the points-to for- mulas, we allow spatial formulas of the arrays the length of which is ≤ k0 and lists which length is ≤ k0, etc, where k0 is a ﬁxed integer. however, if we allow a signiﬁcantly more expressive form of pointer arith- metic — namely arbitrary boolean combinations of elementary formulas over arbitrary pointer sums — then the complexity increase is relatively modest for satisﬁability and quantiﬁer-free entailment: they are still np- complete and conp-complete respectively, and the complexity appears to increase drastically for quantiﬁed entailments, which becomes π exp - complete. 2 -complete (π p 1 keywords: separation logic, pointer arithmetic, complexity. 1 "
1803.03166," in this paper, we introduce a new learning strategy based on a seminal idea of mojirsheibani (1999, 2000, 2002a, 2002b), who proposed a smart method for combining several classiﬁers, relying on a consensus notion. in many aggregation methods, the prediction for a new observation x is computed by building a linear or convex combination over a collection of basic estimators r1(x), . . . , rm(x) previously calibrated using a training data set. mojirsheibani proposes to compute the prediction associated to a new observation by combining selected outputs of the training examples. the output of a training example is selected if some kind of consensus is observed: the predictions computed for the training example with the diﬀerent machines have to be “similar” to the prediction for the new observation. this approach has been recently extended to the context of regression in biau et al. (2016). in the original scheme, the agreement condition is actually required to hold for all individual estimators, which appears inadequate if there is one bad initial estimator. in practice, a few disagree- ments are allowed ; for establishing the theoretical results, the proportion of estimators satisfying the condition is required to tend to 1. in this paper, we propose an alternative procedure, mixing the previous consensus ideas on the predictions with the euclidean distance computed between entries. this may be seen as an alternative approach allowing to reduce the eﬀect of a possibly bad estimator in the initial list, using a constraint on the inputs. we prove the consistency of our strategy in classiﬁcation and in regression. we also provide some numerical experiments on simulated and real data to illustrate the beneﬁts of this new aggregation method. on the whole, our practical study shows that our method may perform much better than the original combination technique, and, in particular, exhibit far less variance. we also show on simulated examples that this procedure mixing inputs and outputs is still robust to high dimensional inputs. keywords : classiﬁcation, regression estimation, aggregation, nonlinearity, consistency. ams classiﬁcation : 62g05, 62g08, 62h30 "
1803.03168," we study the condensate phase dynamics in a low-temperature equilibrium gas of weakly interacting bosons, har- monically trapped and isolated from the environment. we ﬁnd that at long times, much longer than the collision time between bogoliubov quasiparticles, the variance of the phase accumulated by the condensate grows with a ballistic term quadratic in time and a diﬀusive term aﬃne in time. we give the corresponding analytical expressions in the limit of a large system, in the collisionless regime and in the ergodic approximation for the quasiparticle motion. when properly rescaled, they are described by universal functions of the temperature divided by the thomas-fermi chemical potential. the same conclusion holds for the mode damping rates. such universality class diﬀers from the previously studied one of the homogeneous gas. keywords: bose gases; bose-einstein condensate; temporal coherence; trapped gases; ultracold atoms "
1803.03170,". in this paper, we reformulate certain nabla fractional diﬀerence equations which had been investigated by other researchers. the previous results seem to be incomplete. by using contraction mapping theorem, we establish conditions under which solutions exist and are unique and have certain asymptotic properties. keywords: nabla fractional diﬀerence equation, contraction mapping theorem, existence and uniqueness, asymptotic property "
1803.03173,". cyber-physical systems (cps) are systems controlled by one or more computer-based components tightly integrated with a set of physical components, typically described as sensors and actuators, that can either be directly attached to the computer components, or at a remote location, and accessible through a network connection. the modeling and veriﬁcation of such systems is a hard task and error prone that require rigorous techniques. hybrid automata is a formalism that extends ﬁnite-state automata with continuous behavior, described by ordinary diﬀerential equations. this paper uses a rewriting logic-based technique to model and validate cps, thus exploring the use of a formal technique to develop such systems that combines expressive speciﬁca- tion with eﬃcient state-based analysis. moreover, we aim at the modular speciﬁcation of such systems such that each cps component is indepen- dently speciﬁed and the ﬁnal system emerges as the synchronous product of its constituent components. we model cpss using linear hybrid au- tomaton and implement them in real-time maude, a rewriting logic tool for real-time systems. with this method, we develop a speciﬁcation for the n-reservoir problem, a cps that controls a hose to ﬁll a number of reservoirs according to the physical properties of the hose and the reservoirs. 1 "
1803.03178," q: i have heard its not possible to extend visit visa more than 6 months? can u please answer me.. thankzzz... community question answering (cqa) forums are very pop- ular nowadays, as they represent effective means for commu- nities around particular topics to share information. unfortu- nately, this information is not always factual. thus, here we explore a new dimension in the context of cqa, which has been ignored so far: checking the veracity of answers to par- ticular questions in cqa forums. as this is a new problem, we create a specialized dataset for it. we further propose a novel multi-faceted model, which captures information from the an- swer content (what is said and how), from the author proﬁle (who says it), from the rest of the community forum (where it is said), and from external authoritative sources of informa- tion (external support). evaluation results show a map value of 86.54, which is 21 points absolute above the baseline. "
1803.03179," we present optical photometry of superoutbursts in 2016 of two wz sge-type dwarf novae (dne), asassn-16dt and asassn-16hg. their light curves showed a dip in brightness between the ﬁrst plateau stage with no ordinary superhumps (or early superhumps) and the second plateau stage with ordinary superhumps. we ﬁnd that the dip is produced by slow evolution of the 3:1 resonance tidal instability and that it would be likely observed in low mass-ratio objects. the estimated mass ratio (q ≡ m2/m1) from the period of developing (stage a) superhumps (0.06420(3) d) was 0.036(2) in asassn-16dt. additionally, its superoutburst has many properties similar to those in other low-q wz sge-type dne: long-lasting stage a superhumps, small superhump amplitudes, long delay of ordinary superhump appearance, and slow decline rate in the plateau stage with superhumps. the very small mass ratio and observational characteristics suggest that this system is one of the best candidates for a period bouncer – a binary accounting for the missing population of post-period minimum cataclysmic variables. although it is not clearly veriﬁed due to the lack of detection of stage a superhumps, asassn-16hg might be a possible candidate for a period bouncer on the basis of the morphology of its light curves and the small superhump amplitudes. many outburst properties of period-bouncer candidates would originate from the small tidal eﬀects by their secondary stars. key words: accretion, accretion disks - novae, cataclysmic variables - stars: dwarf novae - stars: individual (asassn-16dt, asassn-16hg) 1. "
1803.03182,". let p be a principal indecomposable module of a ﬁnite group g in charac- teristic 2 and let ϕ be the brauer character of the corresponding simple g-module. we show that p aﬀords a non-degenerate g-invariant quadratic form if and only if there are involutions s, t ∈ g such that st has odd order and ϕ(st)/2 is not an algebraic integer. we then show that the number of isomorphism classes of quadratic principal inde- composable g-modules is equal to the number of strongly real conjugacy classes of odd order elements of g. "
1803.03184,". we evaluate for the ﬁrst time the dark matter bispectrum of large-scale structure at two loops in the standard perturbation theory and at three loops in the renormalised perturbation theory (mptbreeze formalism), removing in each case the leading diver- gences in the integrals in order to make them infrared-safe. we show that the standard perturbation theory at two loops can be employed to model the matter bispectrum further into the quasi-nonlinear regime compared to one loop, up to kmax ∼ 0.1 h/mpc at z = 0, but without reaching a high level of accuracy. in the case of the mptbreeze method, we show that its bispectra decay at smaller and smaller scales with increasing loop order, but with smaller improvements. at three loops, this model predicts the bispectrum accurately up to scales kmax ∼ 0.17 h/mpc at z = 0 and kmax ∼ 0.24 h/mpc at z = 1. 8 1 0 2   y a m 4       ] . o c h p - o r t s a [     2 v 4 8 1 3 0 . 3 0 8 1 : v i x r a contents "
1803.03185," adverse drug reactions (adrs) induced from high-order drug-drug interactions (ddis) due to polypharmacy represent a significant public health problem. in this paper, we formally formulate the to-avoid and safe (with respect to adrs) drug recommendation problems when multiple drugs have been taken simultaneously. we develop a joint model with a recommendation component and an adr label prediction component to recommend for a prescription a set of to-avoid drugs that will induce adrs if taken together with the prescription. we also develop real drug-drug interaction datasets and corresponding evaluation protocols. our experimental results on real datasets demonstrate the strong performance of the joint model compared to other baseline methods. ccs concepts • computing methodologies → machine learning; supervised learning; • applied computing → health care information systems; health informatics; bioinformatics; • information systems → data mining; keywords adverse drug reaction, polypharmacy, drug recommendation "
1803.03189," genome-wide association studies (gwas) have identiﬁed single nucleotide polymorphisms (snps) associated with trait diversity and disease susceptibility, yet the functional properties of many genetic variants and their molecular interactions remains unclear. it has been hypothesized that snps in mi- crorna binding sites may disrupt gene regulation by micrornas (mirnas), short non-coding rnas that bind to mrna and downregulate the target gene. while a number of studies have been conducted to predict the location of snps in mirna binding sites, to date there has been no comprehensive analysis of how snp variants may impact mirna regulation of genes. here we investigate the functional properties of genetic variants and their effects on mirna regula- tion of gene expression in cancer. our analysis is motivated by the hypothesis that distinct alleles may cause differential binding (from mirnas to mrnas or from transcription factors to dna) and change the expression of genes. we previously identiﬁed pathways—systems of genes conferring speciﬁc cell functions—that are dysregulated by mirnas in cancer, by comparing mirna–pathway associations be- tween healthy and tumor tissue. we draw on these results as a starting point to assess whether snps in genes on dysregulated pathways are responsible for mirna dysregulation of individual genes in tumors. using an integrative analysis that incorporates mirna expression, mrna expression, and snp geno- type data, we identify snps that appear to inﬂuence the association between mirnas and genes, which we term “regulatory qtls (regqtls)”: loci whose alleles impact the regulation of genes by mirnas. we describe the method, apply it to analyze four cancer types (breast, liver, lung, prostate) using data from the cancer genome atlas (tcga), and provide a tool to explore the ﬁndings. "
1803.03191," the rise of online social networks (osns) has caused an insurmountable amount of interest from advertisers and researchers seeking to monopolize on its features. researchers aim to develop strategies for determining how information is propagated among users within an osn that is captured by diffusion or inﬂuence models. we consider the inﬂuence models for the im-ro problem, a novel formulation to the inﬂuence maximization (im) problem based on implementing stochastic dynamic programming (sdp). in contrast to existing approaches involving inﬂuence spread and the theory of submodular functions, the sdp method focuses on optimizing clicks and ultimately revenue to advertisers in osns. existing approaches to inﬂuence maximization have been actively researched over the past decade, with applica- tions to multiple ﬁelds, however, our approach is a more practical variant to the original im problem. in this paper, we provide an analysis on the inﬂuence models of the im- ro problem by conducting ex- periments on synthetic and real-world datasets. we propose a bayesian and machine learning approach for estimating the parameters of the inﬂuence models for the (inﬂuence maximization- revenue opti- mization) im-ro problem. we present a bayesian hierarchical model and implement the well-known naive bayes classiﬁer (nbc), decision trees classiﬁer (dtc) and random forest classiﬁer (rfc) on three real-world datasets. compared to previous approaches to estimating inﬂuence model parameters, our strategy has the great advantage of being directly implementable in standard software packages such as winbugs/openbugs/jags and apache spark. we demonstrate the efﬁciency and usability of our methods in terms of spreading information and generating revenue for advertisers in the context of osns. "
1803.03197," unrooted phylogenetic networks are graphs used to represent evolutionary relationships. accurately reconstructing such networks is of great relevance for evolutionary biology. it has recently been conjectured that all phylogenetic networks with at least ﬁve leaves can be uniquely reconstructed from their subnetworks obtained by deleting a single leaf and suppressing degree-2 vertices. here, we show that this conjecture is false, by presenting a counter example for each possible number of leaves that is at least 4. moreover, we show that the conjecture is still false when restricted to binary networks. keywords: reconstruction; phylogenetics ams subject classiﬁciation: 05c60 isomorphism problems 1 "
1803.03199,". let td particle at each site of td n and let them evolve as independent, nearest-neighbor, symmet- ric, continuous-time random walks. each time two particles meet, they coalesce into one. denote by cn the ﬁrst time the set of particles is reduced to a singleton. cox [6] proved the existence of a time-scale θn for which cn /θn converges to the sum of independent exponential random variables. denote by z n the total number of particles at time t. we prove that the sequence of markov chains (z n )t≥0 converges to the total number of partitions in kingman’s coalescent. tθn t "
1803.03200," in codice ratio is a research project to study tools and techniques for analyzing the contents of historical documents conserved in the vatican secret archives (vsa). in this paper, we present our e(cid:130)orts to develop a system to support the transcription of medieval manuscripts. (cid:140)e goal is to provide paleographers with a tool to reduce their e(cid:130)orts in transcribing large volumes, as those stored in the vsa, producing good transcriptions for signi(cid:128)cant portions of the manuscripts. we propose an original approach based on character segmentation. our solution is able to deal with the dirty segmentation that inevitably occurs in handwri(cid:138)en documents. we use a convolutional neural network to recognize characters and language models to compose word transcriptions. our approach requires minimal training e(cid:130)orts, making the transcription process more scalable as the production of training sets requires a few pages and can be easily crowdsourced. we have conducted experiments on manuscripts from the vatican registers, an unreleased corpus containing the correspondence of the popes. with training data produced by 120 high school students, our system has been able to produce good transcriptions that can be used by paleographers as a solid basis to speedup the transcription process at a large scale. "
1803.03201," the evolution of antimicrobial resistance often occurs in a variable environment, as antimicrobial is given periodically to a patient or added and removed from a medium. this environmental variability has a huge impact on the microorganisms’ ﬁtness landscape, and thus on the evolution of resistance. indeed, mutations conferring resistance often carry a ﬁtness cost in the absence of antimicrobial, which may be compensated by subsequent mutations. as antimicrobial is added or removed, the relevant ﬁtness landscape thus switches from a ﬁtness valley to an ascending landscape or vice-versa. here, we investigate the eﬀect of these time-varying patterns of selection within a stochastic model. we focus on a homogeneous microbial population of ﬁxed size subjected to a periodic alternation of phases of absence and presence of an antimicrobial that stops growth. combining analytical approaches and stochastic simulations, we quantify how the time necessary for ﬁt resistant bacteria to take over the microbial population depends on the period of the alternations. we demonstrate that fast alternations strongly accelerate the evolution of resistance, and that a plateau is reached once the period gets suﬃciently small. besides, the acceleration of resistance evolution is stronger for larger populations. for asymmetric alternations, featuring a diﬀerent duration of the phases with and without antimicrobial, we shed light on the existence of a broad minimum of the time taken by the population to fully evolve resistance. at this minimum, if the alternations are suﬃciently fast, the very ﬁrst resistant mutant that appears ultimately leads to full resistance evolution within the population. this dramatic acceleration of the evolution of antimicrobial resistance likely occurs in realistic situations, and can have an important impact both in clinical and experimental situations. "
1803.03204," neural codes are collections of binary vectors that represent the ﬁring patterns of neurons. the information given by a neural code c can be represented by its neural ideal jc . in turn, the polynomials in jc can be used to determine the relationships among the receptive ﬁelds of the neurons. in a paper by curto et al., three such relationships, known as the type 1-3 relations, were linked to the neural ideal by three if-and-only-if statements. later, garcia et al. discovered the type 4-6 relations. these new relations diﬀered from the ﬁrst three in that they were related to jc by one-way implications. in this paper, we ﬁrst show that the converses of these new implications are false at the level of both the neural ideal jc and the larger ideal i(c) of a code. we then present modiﬁed statements of these relations that, like the ﬁrst three, can be related by if-and-only-if statements to both jc and i(c). using the modiﬁed relations, we uncover a new relationship involving jc , i(c), and the type 1-6 relations. "
1803.03205,". we show that several dichotomy theorems concerning the second level of the borel hierarchy are special cases of the ℵ0-dimensional generalization of the open graph dichotomy, which itself follows from the usual proof(s) of the perfect set theorem. under the axiom of determinacy, we obtain the generalizations of these results from analytic metric spaces to separable metric spaces. we also consider connections between cardinal invariants and the chromatic numbers of the corresponding dihypergraphs. "
1803.03206," we study and exactly solve the two-photon and k-photon jaynes-cummings models by using a novelty algebraic method. this algebraic method is based on the pauli matri- ces realization and the tilting transformation of the su (2) group and let us diagonalize the hamiltonian of these models by properly choosing the coherent state parameters of the transformation. finally, we explicitly obtain the energy spectrum and eigenfunc- tions for each model. pacs: 02.20.sv, 03.65.fd, 42.50.-p keywords: matrix diagonalization, two-photon jaynes-cummings model, k-photon jaynes- cummings model, tilting transformation. 1 "
1803.03207,". computable estimates for the error of ﬁnite element discretisations of parabolic problems in the l∞(0, t ; l2(ω)) norm are developed, which exhibit constant eﬀectivities (the ratio of the estimated error to the true error) with respect to the simulation time. these estimates, which are of optimal order, represent a signiﬁcant advantage for long-time simulations, and are derived using energy techniques based on elliptic reconstructions. the eﬀectivities of previous optimal order error estimates in this norm derived using energy techniques are shown numerically to grow either in proportion to the simulation duration or its square root, a key disadvantage compared with earlier estimators derived using parabolic duality arguments. the new estimates form a continuous family, almost all of which are new, reproducing certain familiar energy-based estimates well suited for short-time simulations and not available through the parabolic duality framework. for clarity, we demonstrate the technique applied to a linear parabolic problem discretised using standard conforming ﬁnite element methods in space coupled with backward euler and crank-nicolson time discretisations, although it can be applied much more widely. 8 1 0 2   r a m 8       ] . a n h t a m [     1 v 7 0 2 3 0 . 3 0 8 1 : v i x r a "
1803.03208,". the aim of this paper is to extend probability theory from the clas- sical to the product t-norm fuzzy logic setting. more precisely, we axiomatize a generalized notion of ﬁnitely additive probability for product logic formulas, called state, and show that every state is the lebesgue integral with respect to a unique regular borel probability measure. furthermore, the relation between states and measures is shown to be one-one. in addition, we study geometrical properties of the convex set of states and show that extremal states, i.e., the extremal points of the state space, are the same as the truth-value assignments of the logic. finally, we axiomatize a two-tiered modal logic for probabilistic reasoning on product logic events and prove soundness and completeness with respect to probabilistic spaces, where the algebra is a free product algebra and the measure is a state in the above sense. keywords. probability theory; nonclassical events; free product algebras; states; riesz representation theorem; regular borel measures; two-tiered modal logics. "
1803.03210," we introduce virtual tribrackets, an algebraic structure for coloring regions in the planar complement of an oriented virtual knot or link diagram. we use these structures to deﬁne counting invariants of virtual knots and links and provide examples of the computation of the invariant; in particular we show that the invariant can distinguish certain virtual knots. keywords: biquasiles, virtual biquasiles, tribrackets, virtual tribrackets, virtual knots and links 2010 msc: 57m27, 57m25 "
1803.03214," grain boundaries (gbs) are central defects for describing polycrystalline materials, and playing major role in a wide-range of physical properties of polycrystals. control over gb kinetics provides eﬀective means to tailor polycrystal properties through material processing. while many approaches describe diﬀerent gb kinetic phenomena, this review provides a unifying concept for a wide range of gb kinetic behavior. our approach rests on a disconnection description of gb kinetics. disconnections are topological line defects constrained to crystalline interfaces with both step and dislocation character. these characteristics can be completely speciﬁed by gb bicrystallography and the macroscopic degrees of freedom of gbs. gb thermal ﬂuctuations, gb migration and the ability of gbs to absorb/emit other defects from/into the delimiting grains can be modeled via the nucleation, propagation and reaction of disconnections in the gb. we review the fundamentals of bicrystallography and its relationship to disconnections and ultimately to the kinetic behavior of gbs. we then relate disconnection dynamics and gb kinetics to microstructural evolution. while this review of the gb kinetics literature is not exhaustive, we review much of the foundational literature and draw comparisons from a wide swath of the extant experimental, simulation, and theoretical gb kinetics literature. keywords: grain boundary, disconnections, grain-boundary kinetics, grain boundary migration, shear coupling contents "
1803.03215,"—in recent years, several successful applications of the artiﬁcial neural networks (anns) have emerged in nuclear physics and high-energy physics, as well as in biology, chem- istry, meteorology, and other ﬁelds of science. a major goal of nuclear theory is to predict nuclear structure and nuclear reactions from the underlying theory of the strong interactions, quantum chromodynamics (qcd). with access to powerful high performance computing (hpc) systems, several ab initio approaches, such as the no-core shell model (ncsm), have been developed to calculate the properties of atomic nuclei. however, to accurately solve for the properties of atomic nuclei, one faces immense theoretical and computational challenges. the present study proposes a feed-forward ann method for predicting the properties of atomic nuclei like ground state energy and ground state point proton root-mean-square (rms) radius based on ncsm results in computationally accessible basis spaces. the designed anns are sufﬁcient to produce results for these two very different observables in 6li from the ab initio ncsm results in small basis spaces that satisfy the theoretical physics condition: independence of basis space parameters in the limit of extremely large matrices. we also provide comparisons of the results from anns with established methods of estimating the results in the inﬁnite matrix limit. keywords–nuclear structure of 6li; ab initio no-core shell model; ground state energy; point proton root-mean-square radius; artiﬁcial neural network. i. "
1803.03217," fourier transform interferometry (fti) is an interferomet- ric procedure for acquiring hyperspectral (hs) data. re- cently, it has been observed that the light source highlight- ing a (biologic) sample can be coded before the fti acquisi- tion in a procedure called coded illumination-fti (ci-fti). this turns hs data reconstruction into a compressive sens- ing (cs) problem regularized by the sparsity of the hs data. ci-fti combines the high spectral resolution of fti with the advantages of reduced-light-exposure imaging in biology. in this paper, we leverage multilevel sampling scheme re- cently developed in cs theory to adapt the coding strategy of ci-fti to the spectral sparsity structure of hs data in fluores- cence spectroscopy (fs). this structure is actually extracted from the spectral signatures of actual ﬂuorescent dyes used in fs. accordingly, the optimum illumination coding as well as the theoretical recovery guarantee are derived. we con- duct numerous numerical experiments on synthetic and exper- imental data that show the faithfulness of the proposed theory to experimental observations. index terms— hyperspectral, fourier transform interfer- ometry, fluorescence spectroscopy, compressive sensing. "
1803.03218,". let d < 0 be a fundamental discriminant and h(d) be the class number of the imaginary quadratic ﬁeld q(√d). moreover, assume that πd(x) is the number of the split primes with norm less than x in q(√d) and r(x, d) is the number of the classes of the binary quadratic forms of discriminant d which represents a prime number less than x. we prove that (cid:16) πd(x) π(x) (cid:17)2 ≪ r(x, d) h(d) (cid:16)1 + h(d) π(x)(cid:17), where π(x) is the number of the primes less than x and the implicit constant in ≪ is independent of d. as a result, by assuming the riemann hypothesis for the dirichlet l-function l(s, χd), at least αh(d) number of the ideal classes of q(√d) contain a prime ideal with a norm less than the optimal bound h(d) log(|d|) where α > 0 is an absolute positive constant independent of d. more generally, let k be a bounded degree number ﬁeld over q with the discriminant dk and the class number hk . we conjecture that a positive proportion of the ideal classes of k contain a prime ideal with a norm less than hk log(|dk|). contents 1. "
1803.03219," this paper is concerned with developing an explicit correspondence between the quantum yang-baxter equation in star-triangle relation form, and the classical 3d-consistent quad equa- tions in the adler-bobenko-suris (abs) classiﬁcation. each of the 3d-consistent asymmetric h- type abs quad equations, including diﬀerent δ, ε = 0, 1 deformations, are explicitly derived from the quasi-classical expansion of a respective counterpart star-triangle relation. the interpretation of the diﬀerent star-triangle relations is given in terms of known (hyperbolic/rational/classical) hypergeometric integrals. limiting relations that exist between various star-triangle relations are given for each case. contents "
1803.03221,". let n ≧ 5. there is a smoothly knotted n-dimensional sphere in (n + 2)-space such that the singular point set of its projection in (n + 1)-space consists of double points and that the components of the singular point set are two. (the sphere is knotted in the sense that it does not bound any embedded (n + 1)-ball in (n + 2)-space.) furthermore, the projection is not the projection of any unknotted sphere in (n + 2)-space. there are two inequivalent embeddings of an n-manifold in (n + 2)-space such that the projection of one of these in (n + 1)-space has no double points and the projection of the other has a connected embedded double point set. this research was partially supported by research fellowships of the promotion of science for young scientists. 1 "
1803.03223," for a riemannian covering p : m2 → m1, we compare the spectrum of an essen- tially self-adjoint diﬀerential operator d1 on a bundle e1 → m1 with the spectrum of its lift d2 on p∗e1 → m2. we prove that if the covering is inﬁnite sheeted and amenable, then the spectrum of d1 is contained in the essential spectrum of any self-adjoint extension of d2. we show that if the deck transformations group of the covering is inﬁnite and d2 is essentially self-adjoint (or symmetric and bounded from below), then d2 (or the friedrichs extension of d2) does not have eigenvalues of ﬁnite multiplicity and in particular, its spectrum is essential. moreover, we prove that if m1 is closed, then p is amenable if and only if it preserves the bottom of the spectrum of some/any schr¨odinger operator, extending a result due to brooks. 1 "
1803.03227," g in a celebrated series of papers, d. freed, m. hopkins and c. teleman proved that the fusion ring of the loop group over a compact, simply connected lie group g at level k can be described as the equivariant twisted k-theory ring τ k dim g pgq, where g acts on itself by conjugation and the twist τ depends on the level. in other words, it can be described via equivariant bundles of compact operators over g. in this paper, the authors take a diﬀerent approach to these fusion rings, which also arise from wess-zumino-witten models in conformal ﬁeld theory. their long- term goal is to describe the fusion ring via the equivariant k-theory of mnpcqb8 under an action of the quantum group gq for q a root of unity, the idea being that the twist in the freed-hopkins-teleman picture is captured by the non-commutativity of mnpcqb8 and the deformation parameter q. as a step in this direction, the authors compute the k-theory of certain approximately ﬁnite dimensional c ˚-algebras that can be viewed as ﬁxed point algebras under gq or, more rigorously, arise from corresponding towers of relative commutants of a subfactor (axiomatized either via the λ-lattices of s. popa, the paragroups of a. ocneanu, or the planar algebras of v. f. r. jones), for g “ sup2q and g “ sup3q, and relate them, together with their natural ring structure, to the aforementioned fusion rings. we also comment on the other rank 2 lie groups g2 and spp2q. "
1803.03228," we study single-qutrit gates composed of cliﬀord and t gates, using the qutrit version of the t gate proposed by howard and vala [1]. we propose a normal form for single- qutrit gates analogous to the matsumoto-amano normal form for qubits. we prove that the normal form is optimal with respect to the number of t gates used and that any string of qutrit cliﬀord+t operators can be put into this normal form in polynomial time. we also prove that this form is unique and provide an algorithm for exact synthesis of any single qutrit cliﬀord+t operator. 8 1 0 2   r a m 8       ] h p - t n a u q [     1 v 8 2 2 3 0 . 3 0 8 1 : v i x r a contents "
1803.03229,. we prove a p-adic analog of kunz’s theorem: a p-adically complete noe- therian ring is regular exactly when it admits a faithfully ﬂat map to a perfectoid ring. this result is deduced from a more precise statement on detecting ﬁniteness of projective dimension of ﬁnitely generated modules over noetherian rings via maps to perfectoid rings. 
1803.03231,". this paper classiﬁes the set of supersolutions of a general class of periodic- parabolic problems in the presence of a positive supersolution. from this result we characterize the positivity of the underlying resolvent operator through the positivity of the associated principal eigenvalue and the existence of a positive strict supersolu- tion. lastly, this (scalar) characterization is used to characterize the strong maximum principle for a class of periodic-parabolic systems of cooperative type under arbitrary boundary conditions of mixed type. 2010 msc: primary: 35k57, 35b10, 35b50. secondary: 35p05. keywords and phrases: periodic-parabolic problems, maximum principle, principal eigenvalue, positivity of the resolvent, positive strict supersolution. this paper has been supported by the imi of complutense university and the min- istry of economy and competitiveness of spain under grant mtm2015-65899-p "
1803.03232," reinforcement learning (rl) is a promising approach to solve dialogue policy optimisa- tion. traditional rl algorithms, however, fail to scale to large domains due to the curse of dimensionality. we propose a novel dia- logue management architecture, based on feu- dal rl, which decomposes the decision into two steps; a ﬁrst step where a master policy selects a subset of primitive actions, and a sec- ond step where a primitive action is chosen from the selected subset. the structural in- formation included in the domain ontology is used to  the dialogue state space, tak- ing the decisions at each step using different parts of the ed state. this, combined with an information sharing mechanism be- tween slots, increases the scalability to large domains. we show that an implementation of this approach, based on deep-q networks, signiﬁcantly outperforms previous state of the art in several dialogue domains and environ- ments, without the need of any additional re- ward signal. "
1803.03234," many loss functions in representation learning are invariant under a continuous symmetry trans- formation. as an example, consider word em- beddings (mikolov et al., 2013b), where the loss remains unchanged if we simultaneously rotate all word and context embedding vectors. we show that representation learning models with a contin- uous symmetry and a quadratic markovian time series prior possess so-called goldstone modes. these are low cost deviations from the optimum which slow down convergence of gradient descent. we use tools from gauge theory in physics to de- sign an optimization algorithm that solves the slow convergence problem. our algorithm leads to a fast decay of goldstone modes, to orders of magnitude faster convergence, and to more interpretable representations, as we show for dy- namic extensions of matrix factorization and word embedding models. we present an example ap- plication, translating modern words into historic language using a shared representation space. "
1803.03237,"— hamilton-jacobi (hj) reachability analysis has been developed over the past decades into a widely-applicable tool for determining goal satisfaction and safety veriﬁcation in nonlinear systems. while hj reachability can be formulated very generally, computational complexity can be a serious impediment for many systems of practical interest. much prior work has been devoted to computing approximate solutions to large reachability problems, yet many of these methods apply to only restricted problem classes, do not generate controllers, and/or are extremely conservative. in this paper, we present a novel approach to approximate hj reachability in which computing an optimal controller is viewed as a sequential classiﬁcation problem. even though we employ neural networks for this classiﬁcation task, our method still provides safety guarantees in many cases. we demonstrate the utility of our approach in the context of safe trajectory following with speciﬁc application to quadrotor navigation. ofﬂine computation and online evaluation conﬁrm that our method preserves safety. "
1803.03238," given two random ﬁnite sequences from [k]n such that a preﬁx of the ﬁrst sequence is a suﬃx of the second, we examine the length of their longest common subsequence. if ℓ is the length of the overlap, we prove that the expected length of an lcs is approximately max(ℓ, e[ln]), where ln is the length of an lcs between two independent random sequences. we also obtain tail bounds on this quantity. "
1803.03239," we study the problem of fair classiﬁcation within the versatile framework of dwork et al. [dhp+12], which assumes the existence of a metric that measures similarity between pairs of individuals. unlike previous works on metric-based fairness, we do not assume that the entire metric is known to the learning algorithm. instead, we study the setting where a learning algorithm can query this metric a bounded number of times to ascertain similarities between particular pairs of individuals. for example, the queries might be answered by a panel of specialists spanning social scientists, statisticians, demographers, and ethicists. we propose metric multifairness, a new deﬁnition of fairness that is parameterized by a similarity metric δ on pairs of individuals and a collection c of “comparison sets” over pairs of individuals. one way to view this collection is as the family of comparisons that can be ex- pressed within some computational bound. with this interpretation, metric multifairness loosely guarantees that similar subpopulations are treated similarly, as long as these subpopulations can be identiﬁed within this bound. in particular, metric multifairness implies that a rich class of subpopulations are protected from a multitude of discriminatory behaviors. we provide a general-purpose framework for learning a metric multifair hypothesis that achieves near-optimal loss from a small number of random samples from the metric δ. we study the sample complexity and time complexity of learning a metric multifair hypothesis (providing rather tight upper and lower bounds) by connecting it to the task of learning the class c. in particular, if the class c admits an eﬃcient agnostic learner, then we can learn such a metric multifair hypothesis eﬃciently. 1 "
1803.03240," generalizing tur´an’s classical extremal problem, alon and shikhelman investigated the prob- lem of maximizing the number of t copies in an h-free graph, for a pair of graphs t and h. whereas alon and shikhelman were primarily interested in determining the order of magnitude for large classes of graphs h, we focus on the case when t and h are paths, where we ﬁnd asymptotic and in some cases exact results. we also consider other structures like stars and the set of cycles of length at least k, where we derive asymptotically sharp estimates. our results generalize well-known extremal theorems of erd˝os and gallai. 1 "
1803.03241," we give the ﬁrst polynomial-time algorithm for performing linear or polynomial regression resilient to adversarial corruptions in both examples and labels. given a suﬃciently large (polynomial-size) training set drawn i.i.d. from distribution d and subsequently corrupted on some fraction of points, our algorithm outputs a linear function whose squared error is close to the squared error of the best-ﬁtting linear function with respect to d, assuming that the marginal distribution of d over the input space is certiﬁably hypercontractive. this natural property is satisﬁed by many well-studied distributions such as gaussian, strongly log-concave distributions and, uniform distribution on the hypercube among others. we also give a simple statistical lower bound showing that some distributional assumption is necessary to succeed in this setting. these results are the ﬁrst of their kind and were not known to be even information- theoretically possible prior to our work. our approach is based on the sum-of-squares (sos) method and is inspired by the recent applications of the method for parameter recovery problems in unsupervised learning. our algorithm can be seen as a natural convex relaxation of the following conceptually simple non- convex optimization problem: ﬁnd a linear function and a large subset of the input corrupted sample such that the least squares loss of the function over the subset is minimized over all possible large subsets. 8 1 0 2   r a m 9       ] g l . s c [     2 v 1 4 2 3 0 . 3 0 8 1 : v i x r a ∗ut austin klivans@cs.utexas.edu †princeton university and institute for advanced study. kothari@cs.princeton.edu. ‡university of california, los angeles raghum@cs.ucla.edu "
1803.03242," we study fairness in machine learning. a learning algorithm, given a training set drawn from an underlying population, learns a classiﬁer that will be used to make decisions about in- dividuals. the concern is that this classiﬁer’s decisions might be discriminatory, favoring certain subpopulations over others. the seminal work of dwork et al. [itcs 2012] introduced fairness through awareness, positing that a fair classiﬁer should treat similar individuals similarly. sim- ilarity between individuals is measured by a task-speciﬁc similarity metric. in the context of machine learning, however, this fairness notion faces serious diﬃculties, as it does not generalize and can be computationally intractable. we introduce a relaxed notion of approximate metric-fairness, which allows a small fairness for a random pair of individuals sampled from the population, with all but a small error: probability of error, if they are similar then they are treated similarly. in particular, this provides discrimination-protections to every subpopulation that is not too small. we show that approximate metric-fairness does generalize from a training set to the underlying population, and we leverage these generalization guarantees to construct polynomial-time learning algorithms that achieve competitive accuracy subject to fairness constraints. 8 1 0 2   r a m 8       ] g l . s c [     1 v 2 4 2 3 0 . 3 0 8 1 : v i x r a ∗rothblum@alum.mit.edu. research supported by the israel science foundation (grant no. 5219/17). †gal.yona@gmail.com. research supported by the israel science foundation (grant no. 5219/17). 1 "
1803.03243," object detection typically assumes that training and test data are drawn from an identical distribution, which, how- ever, does not always hold in practice. such a distribution mismatch will lead to a signiﬁcant performance drop. in this work, we aim to improve the cross-domain robustness of object detection. we tackle the domain shift on two levels: 1) the image-level shift, such as image style, illumination, etc., and 2) the instance-level shift, such as object appear- ance, size, etc. we build our approach based on the recent state-of-the-art faster r-cnn model, and design two do- main adaptation components, on image level and instance level, to reduce the domain discrepancy. the two domain adaptation components are based on h-divergence theory, and are implemented by learning a domain classiﬁer in ad- versarial training manner. the domain classiﬁers on dif- ferent levels are further reinforced with a consistency regu- larization to learn a domain-invariant region proposal net- work (rpn) in the faster r-cnn model. we evaluate our newly proposed approach using multiple datasets including cityscapes, kitti, sim10k, etc. the results demonstrate the effectiveness of our proposed approach for robust ob- ject detection in various domain shift scenarios. "
1803.03246,": the loop quantization of 3d gravity consists in deﬁning the hilbert space of states satisfying the gauß constraint and the ﬂatness constraint. the gauß constraint is enforced at the kinematical level by introducing spin networks which form a basis for the hilbert space of gauge invariant functionals. the ﬂatness constraint is implemented at the dynamical level via the ponzano- regge state-sum model. we propose in this work a dual loop quantization scheme where the role of the constraints is exchanged. the ﬂatness constraint is imposed ﬁrst via the introduction of a new basis labeled by group variables, while the gauß constraint is implemented dynamically using a projector which is related to the dijkgraaf-witten model. we discuss how this alternative quantization program is related to 3d teleparallel gravity. 8 1 0 2   r a m 8       ] c q - r g [     1 v 6 4 2 3 0 . 3 0 8 1 : v i x r a "
1803.03248," we present a randomized distributed algorithm that computes a ∆-coloring in any non- complete graph with maximum degree ∆ ≥ 4 in o(log ∆) + 2o(√log log n) rounds, as well as a randomized algorithm that computes a ∆-coloring in o((log log n)2) rounds when ∆ ∈ [3, o(1)]. both these algorithms improve on an o(log3 n/ log ∆)-round algorithm of panconesi and srinivasan [stoc’1993], which has remained the state of the art for the past 25 years. moreover, the latter algorithm gets (exponentially) closer to an ω(log log n) round lower bound of brandt et al. [stoc’16]. 8 1 0 2   r a m 8       ] s d . s c [     1 v 8 4 2 3 0 . 3 0 8 1 : v i x r a ∗supported by erc grant no. 336495 (acdc) and ulla tuominen foundation. †supported by erc grant no. 336495 (acdc) 1 "
1803.03249," we provide an eﬃcient algorithm for computing the nucleolus for an instance of a weighted cooperative matching game. this resolves a long-standing open question of [kern and paulusma, mathematics of operations research, 2003]. 8 1 0 2   r a m 9       ] t g . s c [     2 v 9 4 2 3 0 . 3 0 8 1 : v i x r a 1 1 "
1803.03250,". bridgeland and maciocia showed that a complex enriques surface x has no fourier–mukai partners apart from itself: that is, if db(x) ∼= db(y ) then x ∼= y . we extend this to twisted fourier– mukai partners: if α is the non-trivial element of br(x) = z/2 and db(x, α) ∼= db(y, β), then x ∼= y and β is non-trivial. our main tools are twisted topological k-theory and twisted mukai lattices. . g a h t a m [     1 v 0 5 2 3 0 . 3 0 8 1 : v i x r a "
1803.03251," we consider the dynamical super-resolution problem consisting in the recovery of positions and velocities of moving particles from low-frequency static measurements taken over multiple time steps. the standard ap- proach to this issue is a two-step process: ﬁrst, at each time step some static reconstruction method is applied to locate the positions of the par- ticles with super-resolution and, second, some tracking technique is ap- plied to obtain the velocities. in this paper we propose a fully dynamical method based on a phase-space lifting of the positions and the veloci- ties of the particles, which are simultaneously reconstructed with super- resolution. we provide a rigorous mathematical analysis of the recovery problem, both for the noiseless case and in presence of noise. several nu- merical simulations illustrate and validate our method, which shows some advantage over existing techniques. we then discuss the application of this approach to the dynamical super-resolution problem in ultrafast ultrasound imaging: blood vessels’ locations and blood ﬂow velocities are recovered with super-resolution. key words. super-resolution, dynamic spikes, ultrafast ultrasound imaging, ﬂuorescence microscopy, blood ﬂow imaging, total variation regularization. mathematics subject classiﬁcation. 65z05, 42a05, 42a15, 94a08, 94a20, 65j22. 1 "
1803.03252," the atlas and cms collaborations have recently released signiﬁcant new data on higgs and diboson production in lhc run 2. measurements of higgs properties have improved in many channels, while kinematic information for h → γγ and h → zz can now be more accurately incorporated in ﬁts using the stxs method, and w +w − diboson production at high pt gives new sensitivity to deviations from the standard model. we have performed an updated global ﬁt to precision electroweak data, w +w − measurements at lep, and higgs and diboson data from runs 1 and 2 of the lhc in the framework of the standard model eﬀective field theory (smeft), allowing all coeﬃcients to vary across the combined dataset, and present the results in both the warsaw and silh operator bases. we exhibit the improvement in the constraints on operator coeﬃcients provided by the lhc run 2 data, and discuss the correlations between them. we also explore the constraints our ﬁt results impose on several models of physics beyond the standard model, including models that contribute to the operator coeﬃcients at the tree level and stops in the mssm that contribute via loops. march 2018 8 1 0 2   r a m 8       ] h p - p e h [     1 v 2 5 2 3 0 . 3 0 8 1 : v i x r a 1 "
1803.03253,". we study the projective logarithmic potential gµ of a prob- ability measure µ on the complex projective space pn equiped with the fubini-study metric ω. we prove that the green operator g : µ 7−→ gµ has strong regularizing properties. it was shown by the second author in [as17] that the range of the operator g is contained in the (local) domain of deﬁnition of the complex monge-amp`ere operator on pn. this result extend earlier results by carlehed [carlehed99]. here we will show that the complex monge-amp`ere measure (ω + ddcgµ)n of the logarithmic potential of µ is absolutely continuous with respect to the lebesgue measure on pn if and only if the measure µ has no atoms. moreover when the measure µ has a ”positive dimension”, we give more precise results on regularity properties of the potential gµ in terms of the dimension of µ. contents 1. "
1803.03254,"— we present semi-supervised deep learning ap- proaches for traversability estimation from ﬁsheye images. our method, gonet, and the proposed extensions leverage generative adversarial networks (gans) to effectively predict whether the area seen in the input image(s) is safe for a robot to traverse. these methods are trained with many positive images of traversable places, but just a small set of negative images depicting blocked and unsafe areas. this makes the proposed methods practical. positive examples can be collected easily by simply operating a robot through traversable spaces, while obtaining negative examples is time consuming, costly, and potentially dangerous. through extensive experiments and several demonstrations, we show that the proposed traversabil- ity estimation approaches are robust and can generalize to unseen scenarios. further, we demonstrate that our methods are memory efﬁcient and fast, allowing for real-time operation on a mobile robot with single or stereo ﬁsheye cameras. as part of our contributions, we open-source two new datasets for traversability estimation. these datasets are composed of approximately 24h of videos from more than 25 indoor environments. our methods outperform baseline approaches for traversability estimation on these new datasets. "
1803.03256,". we construct an inﬁnite family of distinct exotic copies of an interesting manifold m 4, obtained by attaching 2-handle to b 4. they are obtained by twisting m along a loose-cork w ⊂ m . from this we construct a 4-dimensional non-standard s-cobordism. "
1803.03257, we prove the global existence of solution to the small data mass critical stochastic nonlinear schrödinger equation in d = 1. we further show the stability of the solution under perturbation of initial data. our construction starts with the existence of the solution to the truncated subcritical problem. we then obtain uniform bounds on these solutions that enable us to reach criticality and then remove the truncation. contents 1 
1803.03258," context. the low wind eﬀect (lwe) refers to a characteristic set of quasi-static wavefront aberrations seen consistently by the sphere instrument when dome-level wind speeds drop below 3 ms−1. the lwe produces bright low-order speckles in the stellar psf, which severely limit the contrast performance of sphere under otherwise optimal observing conditions. aims. in this paper we propose the fast & furious (f&f) phase diversity algorithm as a viable software-only solution for real-time lwe compensation, which would utilise image sequences from the sphere diﬀerential tip-tilt sensor (dtts) and apply corrections via reference slope oﬀsets on the ao system’s shack-hartmann wavefront sensor. methods. we evaluated the closed-loop performance of f&f on the mithic high-contrast test-bench, under conditions emulating lwe-aﬀected dtts images. these results were contrasted with predictive simulations for a variety of convergence tests, in order to assess the expected performance of an on-sky implementation of f&f in sphere. results. the algorithm was found to be capable of returning lwe-aﬀected images to strehl ratios of greater than 90% within ﬁve iterations, for all appropriate laboratory test cases. these results are highly representative of predictive simulations, and demonstrate stability of the algorithm against a wide range of factors including low image signal-to-noise ratio (s/n), small image ﬁeld of view, and amplitude errors. it was also found in simulation that closed-loop stability can be preserved down to image s/n as low as ﬁve while still improving overall wavefront quality, allowing for reliable operation even on faint targets. conclusions. the fast & furious algorithm is an extremely promising solution for real-time compensation of the lwe, which can operate simultaneously with science observations and may be implemented in sphere without requiring additional hardware. the robustness and relatively large eﬀective dynamic range of f&f also make it suitable for general wavefront optimisation applications, including the co-phasing of segmented elt-class telescopes. key words. atmospheric eﬀects - telescopes - focal-plane wavefront sensing - high-contrast imaging - sphere instrumentation: adaptive optics - instrumentation: high angular resolution - techniques: high angular resolution - "
1803.03259," previous proposals to permit non-exponential free-path statistics in radiative transfer have not included support for volume and boundary sources that are spatially uncorrelated from the scatter- ing events in the medium. birth-collision free paths are treated identically to collision-collision free paths and application of this to general, bounded scenes with inclusions leads to non-reciprocal transport. beginning with reciprocity as a desired property, we propose a new way to integrate non-exponential transport theory into general scenes. we distinguish between the free-path-length statistics between correlated medium particles and the free-path- length statistics beginning at locations not correlated to medium particles, such as boundary surfaces, inclusions and uncorrelated sources. reciprocity requires that the uncorrelated free-path distri- butions are simply the normalized transmittance of the correlated free-path distributions. the combination leads to an equilibrium imbedding of a previously derived generalized transport equation into bounded domains. we compare predictions of this approach to monte carlo simulation of multiple scattering from negatively- correlated suspensions of monodispersive hard spheres in bounded two-dimensional domains and demonstrate improved performance relative to previous work. we also derive new, exact, reciprocal, single-scattering solutions for plane-parallel half-spaces over a va- riety of non-exponential media types. keywords: linear transport theory, kinetic theory, generalized linear boltzmann equation, reciprocity, stochastic media "
1803.03260," the increasing demand for gas electron multiplier (gem) foils has been driven by their application in many current and proposed high-energy physics experiments. micropack, a bengaluru-based com- pany, has established and commercialized gem foils for the ﬁrst time in india. micropack used the double-mask etching technique to suc- cessfully produce 10 cm × 10 cm gem foil. in this paper, we report on the development as well as the geometrical and electrical properties of these foils, including the size uniformity of the holes and leakage cur- rent measurements. our characterization studies show that the foils are of good quality and satisfy all the necessary quality control criteria. key words– gem, single-mask, double-mask ∗corresponding author: rasharma@cern.ch 1 "
1803.03261," luminous red novae transients, presumably from stellar coalescence, exhibit long-term precursor emission over hundreds of binary orbits leading to impulsive outbursts, with durations similar to a single orbital period. in an eﬀort to understand these signatures, we present and analyze a hydrodynamic model of unstable mass transfer from a giant-star donor onto a more-compact accretor in a binary system. our simulation begins with mass transfer at the roche limit separation and traces a phase of runaway decay leading up to the plunge of the accretor within the envelope of the donor. we characterize the ﬂuxes of mass and angular momentum through the system and show that the orbital evolution can be reconstructed from measurements of these quantities. the morphology of outﬂow from the binary changes signiﬁcantly as the binary orbit tightens. at wide separations, a thin stream of relatively high-entropy gas trails from the outer lagrange points. as the orbit tightens, the orbital motion desynchronizes from the donor’s rotation, and low-entropy ejecta trace a broad fan of largely- ballistic trajectories. an order-of-magnitude increase in mass ejection rate accompanies the plunge of the accretor with the envelope of the donor. we argue that this transition marks the precursor-to- outburst transition observed in stellar coalescence transients. keywords: binaries: close, methods: numerical, hydrodynamics "
1803.03263,": we explore the cosmological signatures associated with the twin baryons, electrons, photons and neutrinos in the mirror twin higgs framework. we consider a scenario in which the twin baryons constitute a subcomponent of dark matter, and the contribution of the twin photon and neutrinos to dark radiation is suppressed due to late asymmetric reheating, but remains large enough to be detected in future cosmic microwave background (cmb) experiments. we show that this frame- work can lead to distinctive signals in large scale structure and in the cosmic microwave background. baryon acoustic oscillations in the mirror sector prior to recombination lead to a suppression of struc- ture on large scales, and leave a residual oscillatory pattern in the matter power spectrum. this pattern depends sensitively on the relative abundances and ionization energies of both twin hydrogen and helium, and is therefore characteristic of this class of models. although both mirror photons and neutrinos constitute dark radiation in the early universe, their effects on the cmb are distinct. this is because prior to recombination the twin neutrinos free stream, while the twin photons are prevented from free streaming by scattering off twin electrons. in the mirror twin higgs framework the rela- tive contributions of these two species to the energy density in dark radiation is predicted, leading to testable effects in the cmb. these highly distinctive cosmological signatures may allow this class of models to be discovered, and distinguished from more general dark sectors. contents 1 "
1803.03265," in the past few years wide-ﬁeld optical and uv transient surveys as well as x-ray telescopes have allowed us to identify a few dozen candidate tidal disruption events (tdes). while in theory the physical processes in tdes are expected to be ubiquitous, a few distinct classes of tdes have been observed. some tdes radiate mainly in nuv/optical while others produce prominent x-rays. moreover, relativistic jets have been observed in only a handful of tdes. this diversity might be related to the details of the super-eddington accretion and emission physics relevant to tde disks. in this letter, we utilize novel three-dimensional general relativistic radiation magnetohydrodynamics simulations to study the super-eddington compact disk phase expected in tdes. consistent with previous studies, geometrically thick disks, wide-angle optically-thick fast outﬂows and relativistic jets are produced. the outﬂow density and velocity depend sensitively on the inclination angle, and hence so does the reprocessing of emission produced from the inner disk. we then use monte-carlo radiative transfer to calculate the reprocessed spectra and ﬁnd that that the observed ratio of optical to x-ray ﬂuxes increases with increasing inclination angle. this naturally leads to a uniﬁed model for different classes of tdes in which the spectral properties of the tde depend mainly on the viewing-angle of the observer with respect to the orientation of the disk. keywords: accretion, accretion disks — black hole physics — (galaxies:) quasars: supermassive black holes — magnetohydrodynamics (mhd) — radiative transfer 8 1 0 2   r a m 8   .     ] e h h p - o r t s a [     1 v 5 6 2 3 0 . 3 0 8 1 : v i x r a corresponding author: lixin dai lixin.dai@nbi.ku.dk 2 dai et al. "
1803.03268," we study the weak decay ω−b → (ξ+ c k−) π−, in view of the narrow ωc states recently measured by the lhcb collaboration and later conﬁrmed by the belle collaboration. the ωc(3050) and ωc(3090) are described as meson-baryon molecular states, using an extension of the local hidden gauge approach in coupled channels. we investigate the ξd, ξc ¯k and ξ′c ¯k invariant mass distributions making predictions that could be confronted with future experiments, providing useful information that could help determine the quantum numbers and nature of these states. 8 1 0 2   r a m 8       ] h p - p e h [     1 v 8 6 2 3 0 . 3 0 8 1 : v i x r a ∗ vinicius.rodrigues@iﬁc.uv.es † jdias@if.usp.br ‡ liangwh@gxnu.edu.cn § eulogio.oset@iﬁc.uv.es 1 i. "
1803.03269," we present the results from an analysis of deep herschel space observatory observations of six nearby dwarf galaxies known to host galactic-scale winds. the superior far-infrared sensitivity and angular resolution of herschel have allowed detection of cold circumgalactic dust features beyond the stellar components of the host galaxies traced by spitzer 4.5 µm images. comparisons of these cold dust features with ancillary data reveal an imperfect spatial correlation with the ionized gas and warm dust wind components. we ﬁnd that typically ∼10-20% of the total dust mass in these galaxies resides outside of their stellar disks, but this fraction reaches ∼60% in the case of ngc 1569. this galaxy also has the largest metallicity (o/h) deﬁcit in our sample for its stellar mass. overall, the small number of objects in our sample precludes drawing strong conclusions on the origin of the circumgalactic dust. we detect no statistically signiﬁcant trends with star formation properties of the host galaxies, as might be expected if the dust were lifted above the disk by energy inputs from on-going star formation activity. although a case for dust entrained in a galactic wind is seen in ngc 1569, in all cases, we cannot rule out the possibility that some of the circumgalactic dust might be associated instead with gas accreted or removed from the disk by recent galaxy interaction events, or that it is part of the outer gas-rich portion of the disk that lies below the sensitivity limit of the spitzer 4.5 µm data. keywords: galaxies: dwarf — galaxies: halos — galaxies: interactions — galaxies: star formation — galaxies: structure 8 1 0 2   r p a 4       ] . a g h p - o r t s a [     3 v 9 6 2 3 0 . 3 0 8 1 : v i x r a ∗ herschel is an esa space observatory with science instruments provided by european-led principal investigator consortia and with important participation from nasa. 2 "
1803.03270," given rapid photodissociation and photodegradation, the recently dis- covered organics in the martian subsurface and atmosphere were probably delivered in geologically recent times. possible parent bodies are c-type asteroids, comets, and interplanetary dust particles (idps). the dust infall rate was estimated, using diﬀerent methods, to be between 0.71 and 2.96× 106 kg/yr (nesvorn´y et al., 2011; borin et al., 2017; crismani et al., 2017); assuming a carbon content of 10% (flynn, 1996), this implies an idp carbon ﬂux of 0.07 − 0.3 × 106 kg/yr. we calculate for the ﬁrst time the carbon ﬂux from impacts of asteroids and comets. to this end, we perform dynamical simulations of impact rates on mars. we use the n-body integrator rmvs/swifter to propagate the sun and the eight planets from their current positions. we separately add comets and asteroids to the simulations as massless test particles, based on their current orbital elements, yielding mars impact rates of 4.34 × 10−3 comets/myr and 3.3 asteroids/myr. we estimate the delivered amount of carbon using published carbon con- tent values. in asteroids, only c types contain appreciable amounts of carbon. ∗corresponding author email address: k.frantseva@sron.nl (kateryna frantseva) preprint submitted to icarus march 12, 2018 given the absence of direct taxonomic information on the mars impactors, we base ourselves on the measured distribution of taxonomic types in com- bination with dynamic models of the origin of mars-crossing asteroids. we estimate the global carbon ﬂux on mars from cometary impacts to be ∼ 0.013× 106 kg/yr within an order of magnitude, while asteroids deliver ∼ 0.05 × 106 kg/yr. these values correspond to ∼ 4 − 19% and ∼ 17 − 71%, respectively, of the idp-borne carbon ﬂux estimated by nesvorn´y et al., borin et al. and crismani et al.. unlike the spatially homogeneous idp infall, impact ejecta are distributed locally, concentrated around the impact site. we ﬁnd organics from asteroids and comets to dominate over idp-borne organics at distances up to 150 km from the crater center. our results may be important for the interpretation of in situ detections of organics on mars. keywords: asteroids, dynamics, comets, dynamics, mars, surface, cratering, astrobiology "
1803.03272," the experiment to detect the global epoch of reionization signature (edges) re- cently reported a strong 21-cm absorption signal relative to the cosmic microwave background at z ∼ 18. while its anomalous amplitude may indicate new physics, in this work we focus on the timing of the signal, as it alone provides an impor- tant constraint on galaxy formation models. whereas rest-frame ultraviolet luminos- ity functions (uvlfs) over a broad range of redshifts are well ﬁt by simple models in which galaxy star formation histories track the assembly of dark matter halos, we ﬁnd that these same models, with reasonable assumptions about x-ray production in star-forming galaxies, cannot generate a narrow absorption trough at z ∼ 18. if veriﬁed, the edges signal therefore requires the fundamental inputs of galaxy for- mation models to evolve rapidly at z (cid:38) 10. unless extremely faint sources residing in halos below the atomic cooling threshold are responsible for the edges signal, star formation in ∼ 108-1010 m(cid:12) halos must be more eﬃcient than expected, implying that the faint-end of the uvlf at muv (cid:46) −12 must steepen at the highest redshifts. this steepening provides a concrete test for future galaxy surveys with the james webb space telescope and ongoing eﬀorts in lensed ﬁelds, and is required regardless of whether the amplitude of the edges signal is due to new cooling channels or a strong radio background in the early universe. however, the radio background solution requires that galaxies at z > 15 emit 1-2 ghz photons with an eﬃciency ∼ 103 times greater than local star-forming galaxies, posing a challenge for models of low-frequency photon production in the early universe. key words: galaxies: high-redshift – intergalactic medium – galaxies: luminosity function, mass function – dark ages, reionization, ﬁrst stars – diﬀuse radiation. "
1803.03273," astrophysical systems, such as clumps that form star clusters share a density pro- ﬁle that is close to ρ ∼ r −2. we prove analytically this density proﬁle is the result of the scale-free nature of the gravitational collapse. therefore, it should emerge in many diﬀerent situations as long as gravity is dominating the evolution for a period that is comparable or longer than the free-fall time, and this does not necessarily im- ply an isothermal model, as many have previously believed. to describe the collapse process, we construct a model called the turbulence-regulated gravitational collapse model, where turbulence is sustained by accretion and dissipates in roughly a cross- ing time. we demonstrate that a ρ ∼ r −2 proﬁle emerges due to the scale-free nature the system. in this particular case, the rate of gravitational collapse is regulated by the rate at which turbulence dissipates the kinetic energy such that the infall speed can be 20-50% of the free-fall speed(which also depends on the interpretation of the crossing time based on simulations of driven turbulence). these predictions are con- sistent with existing observations, which suggests that these clumps are in the stage of turbulence-regulated gravitational collapse. our analysis provides a uniﬁed description of gravitational collapse in diﬀerent environments. key words: hydrodynamics – turbulence – gravitation – methods:analytical – galax- ies: star formation "
1803.03274," in this paper we carry out anisotropic “separate universe” simulations by including a large- scale tidal ﬁeld in the n-body code gadget4 using an anisotropic expansion factor aij. we use the code in a pure particle-mesh (pm) mode to simulate the evolution of 16 realizations of an initial density ﬁeld with and without a large-scale tidal ﬁeld, which are then used to measure the response function describing how the tidal ﬁeld inﬂuences structure formation in the linear and non-linear regimes. together with the previously measured response to a large scale overdensity, this completely describes the nonlinear matter bispectrum in the squeezed limit. we ﬁnd that, contrary to the density response, the tidal response never signiﬁcantly exceeds the large-scale perturbation-theory prediction even on nonlinear scales for the redshift range we discuss. we develop a simple halo model that takes into account the effect of the tidal ﬁeld and compare it with our direct measurement from the anisotropic n-body simulations. key words: methods: numerical - cosmology: large-scale structure of universe. 1 "
1803.03275," we present the results of a wide spectroscopic survey aimed at detecting extragalactic globular clusters (gcs) in the core of the fornax cluster. about 4500 low resolution spectra (from 4800 to 10000 ˚a) were observed in 25 vlt/vimos masks covering the central 1 deg2 around the dominant galaxy ngc 1399 corresponding to ∼175 kpc galactocentric radius. we describe the methodology used for data reduction and data analysis. we found a total of 387 unique physical objects (372 gcs and 15 ultra compact dwarfs) in the ﬁeld covered by our observa- tions. most of these objects are associated with ngc 1399, with only 10% likely belonging to other giant galaxies. the new vimos dataset is complementary to the many gc cata- logues already present in the literature and it brings the total number of tracer particles around ngc 1399 to more than 1130 objects. with this comprehensive radial velocity sample we have found that the velocity dispersion of the gc population (equally for red and blue gc popula- tions) shows a relatively sharp increase from low velocity dispersion (∼ 250 − 350 km s−1) to high velocity dispersion (∼ 300 − 400 km s−1) at projected radius of ≈ 10 arcmin (∼ 60 kpc) from the galaxy centre. this suggests that at a projected radius of ≈ 60 kpc both blue and red gc populations begin to be governed by the dominating fornax cluster potential, rather than by the central ngc 1399 galaxy potential. this kinematic evidence corroborates similar results found using surface brightness analysis and planetary nebulae kinematics. key words: galaxies:star clusters – galaxies:evolution– galaxies: kinematics and dynamics "
1803.03276," we present the largest and most spatially extended planetary nebulae (pne) catalog ever obtained for the fornax cluster. we measured velocities of 1452 pne out to 200 kpc in the cluster core using a counter-dispersed slitless spectroscopic technique with data from fors2 on the vlt. with such extended spatial coverage, we can study separately the stellar halos of some of the cluster main galaxies and the intracluster light. in this second paper of the fornax cluster vlt spectroscopic survey (fvss), we identify and classify the emission-line sources, describe the method to select pne and calculate their coordinates and velocities from the dispersed slitless images. from the pn 2d velocity map we identify stellar streams that are possibly tracing the gravitational interaction of ngc 1399 with ngc 1404 and ngc 1387. we also present the velocity dispersion proﬁle out to ∼ 200 kpc radii, which shows signatures of a superposition of the bright central galaxy and the cluster potential, with the latter clearly dominating the regions outside r∼ 1000(cid:48)(cid:48)(∼ 100 kpc). key words: catalogues < astronomical data bases, galaxies, galaxies: formation < galaxies, galaxies: kinematics and dynamics < galaxies, (cosmology:) dark matter < cosmology "
1803.03277,". the hot gas in clusters of galaxies creates a distinctive spectral distortion in the cosmic microwave background (cmb) via the sunyaev-zeldovich (sz) eﬀect. to ﬁrst order, the shape of the spectral distortion is ﬁxed, but relativistic corrections (rsz) introduce a dependence on the gas temperature. in this paper, we extract ﬂuxes from a sample of 47 clusters in the planck maps and make a ∼5σ detection of the rsz eﬀect by measuring the scaling relation between the sz amplitude (a proxy for cluster mass) and the cluster temperature. our measurement requires no prior knowledge of the clusters’ gas temperatures and hence is an example of how the rsz can be used to probe fundamental astrophysics. we ﬁnd excellent agreement between our measurement and temperatures obtained with x-ray measurements. contents "
1803.03278," we have modeled direct collapse of a primordial gas within dark matter halos in the presence of radiative transfer. high-resolution zoom-in numerical simulations of gravitational collapse have been performed in a cosmological framework, down to the formation of the photosphere and growth of the central object. radiative transfer has been implemented in the ﬂux-limited diﬀusion (fld) approximation, and adia- batic models have been run for comparison. we ﬁnd that (1) the fld ﬂow forms an irregular central structure with dynamically insigniﬁcant rotation, and does not exhibit fragmentation. this is contrary to adiabatic ﬂow which forms an asymmetric, geometrically-thick disk that drives a pair of strong spiral shocks, subject to kelvin- helmholtz shear instability, forming fragments, which tend to merge with the central disk; (2) the growing central core in the fld ﬂow quickly reaches a core mass of ∼ 10 m(cid:12) and a highly variable luminosity of order of 1038 − 1039 erg s−1, comparable to the eddington luminosity. it experiences massive recurrent outﬂows driven by radi- ation force and thermal pressure gradients, which form dense expanding shells, mixing with the accretion ﬂow and transferring the angular momentum outwards; and (3) the interplay between radiation and thermal pressure gradients and gravity, subject to the massive accretion rate, results in photosphere of radius ∼ 10 au, much larger than that of a protostar. overall, the inclusion of radiative transfer reveals complex early stages of formation and growth of the central structure in direct collapse scenario of massive black hole formation. key words: methods: numerical — galaxies: formation — galaxies: high-redshift — quasars: supermassive black holes — cosmology: theory — cosmology: dark ages, reionization, ﬁrst stars 8 1 0 2   r a m 8       ] . a g h p - o r t s a [     1 v 8 7 2 3 0 . 3 0 8 1 : v i x r a "
1803.03279," we describe a new implementation of the one-ﬂuid method in the sph code phan- tom to simulate the dynamics of dust grains in gas protoplanetary discs. we revise and extend previously developed algorithms by computing the evolution of a new ﬂuid quantity that produces a more accurate and numerically controlled evolution of the dust dynamics. moreover, by limiting the stopping time of uncoupled grains that vio- late the assumptions of the terminal velocity approximation, we avoid fatal numerical errors in mass conservation. we test and validate our new algorithm by running 3d sph simulations of a large range of disc models with tightly- and marginally-coupled grains. key words: hydrodynamics, dust dynamics – methods: numerical – one ﬂuid – accretion, accretion discs. "
1803.03280," microwave kinetic inductance detectors (mkids) provide a compelling path forward to the large-format polarimeter, imaging, and spectrometer arrays needed for next- generation experiments in millimeter-wave cosmology and astronomy. we describe the de- velopment of feedhorn-coupled mkid detectors for the toltec millimeter-wave imaging polarimeter being constructed for the 50-meter large millimeter telescope (lmt). obser- vations with toltec are planned to begin in early 2019. toltec will comprise ∼7,000 po- larization sensitive mkids and will represent the ﬁrst mkid arrays fabricated and deployed on monolithic 150 mm diameter silicon wafers – a critical step towards future large-scale experiments with over 105 detectors. toltec will operate in observational bands at 1.1, 1.4, and 2.0 mm and will use dichroic ﬁlters to deﬁne a physically independent focal plane for each passband, thus allowing the polarimeters to use simple, direct-absorption inductive structures that are impedance matched to incident radiation. this work is part of a larger program at nist-boulder to develop mkid-based detector technologies for use over a wide range of photon energies spanning millimeter-waves to x-rays. we present the detailed pixel layout and describe the methods, tools, and ﬂexible design parameters that allow this solu- tion to be optimized for use anywhere in the millimeter and sub-millimeter bands. we also present measurements of prototype devices operating in the 1.1 mm band and compare the observed optical performance to that predicted from models and simulations. keywords kid, mkid, millimeter, sub-mm, thz, polarimetry, toltec, lmt "
1803.03282,. the main result of this work is the characterization of the covering relations of the bruhat order of the maximal parabolic quotients of type b. our approach is mainly combinatorial and is based in the pattern of the corre- sponding permutations also called signed k-grassmannians permutations. we obtain that a covering relation can be classiﬁed in four diﬀerent pairs of per- mutations. this answers a question raised by ikeda and matsumura providing a nice combinatorial model for maximal parabolic quotients of type b. 
1803.03284,". in the present paper we study the inﬂuence of second viscosity on non-modally induced heat- ing mechanism. for this purpose we study the set of equations governing the hydrodynamic system. in particular, we consider the navier stokes equation, the continuity equation and the equation of state, linearise them and analyse in the context of non-modal instabilities. unlike previous studies in the navier stokes equation we include the contribution of compressibility, thus the second viscosity. by analysing several typical cases we show that under certain conditions the second viscosity might signiﬁcantly change eﬃciency of the mechanism of heating. pacs. pacs-key discribing text of that key – pacs-key discribing text of that key "
1803.03285,"—this paper proposes a real-time movement control algorithm for massive unmanned aerial vehicles (uavs) that provide emergency cellular connections in an urban disaster site. while avoiding the inter-uav collision under temporal wind dynamics, the proposed algorithm minimizes each uav’s energy consumption per unit downlink rate. by means of a mean-ﬁeld game theoretic ﬂocking approach, the velocity control of each uav only requires its own location and channel states. numerical results validate the performance of the algorithm in terms of the number of collisions and energy consumption per data rate, under a realistic 3gpp uav channel model. index terms—uav communication, energy efﬁciency, collision avoidance, mobility control, temporal dynamics, mean-ﬁeld game theory "
1803.03287," in group synchronization, one attempts to ﬁnd a collection of unknown group elements from noisy measurements of their pairwise diﬀerences. several important problems in vision and data analysis reduce to group synchronization over various compact groups. spectral group synchronization is a commonly used, robust algorithm for solving group synchronization problems, which relies on diagonalization of a block matrix whose blocks are matrix representations of the measured pairwise diﬀerences. assuming uniformly distributed measurement errors, we present a rigorous analysis of the accuracy and noise sensitivity of spectral group synchronization algorithms over any compact group, up to the rounding error. we identify a baik-ben arous-péché type phase transition in the noise level, beyond which spectral group synchronization necessarily fails. below the phase transition, spectral group synchronization succeeds in recovering the unknown group elements, but its performance deteriorates with the noise level. we provide asymptotically exact formulas for the accuracy of spectral group synchronization below the phase transition, up to the rounding error. we also provide a consistent risk estimate, allowing practitioners to estimate the method’s accuracy from available measurements. acknowledgments. we thank yoel shkolnisky and amit singer for helpful discussions. er was partially supported by israeli science foundation grant no. 1523/16 and the huji leibniz center. er and mg were partially supported by united states – israel binational science foundation grant no. 2016201. reproducibility advisory. all the experimental results cited in this paper are fully re- producible. code to generate all the ﬁgures included in this paper and their underlying data can be found in the code supplement [rom18]. ∗e-mail: elad.romanov@mail.huji.ac.il †e-mail: gavish@cs.huji.ac.il 1 contents "
1803.03289," network quantization is an effective solution to compress deep neural networks for practical usage. existing network quantization methods cannot sufﬁciently exploit the depth in- formation to generate low-bit compressed network. in this pa- per, we propose two novel network quantization approaches, single-level network quantization (slq) for high-bit quan- tization and multi-level network quantization (mlq) for ex- tremely low-bit quantization (ternary). we are the ﬁrst to con- sider the network quantization both from width and depth level. in the width level, parameters are divided into two parts: one for quantization and the other for re-training to eliminate the quantization loss. slq leverages the distribu- tion of the parameters to improve the width level. in the depth level, we introduce incremental layer compensation to quantize layers iteratively which decreases the quantization loss in each iteration. the proposed approaches are validated with extensive experiments based on the state-of-the-art neu- ral networks including alexnet, vgg-16, googlenet and resnet-18. both slq and mlq achieve impressive results. "
1803.03291,". we provide rapidly converging formulae for the riemann zeta q(s) = p∞n=1 nsqn/(1−qn), function at odd integers using the lambert series l s = −(4k ± 1). our main formula for ζ(4k − 1) converges at rate of about √15π per term, and the formula for ζ(4k + 1), at the rate of e−4π per e− 3√15 100 + term. for example, the ﬁrst order approximation yields ζ(3) ≈ π )i which has an error only of order 10−10. √15π h 9 4 + 4 √15 √15π sinh( e− 2 "
1803.03293," the three-dimensional hilbert transform takes scalar data on the boundary of a domain ω ⊆ r3 and produces the boundary value of the vector part of a quaternionic monogenic (hyperholomorphic) function of three real variables, for which the scalar part coincides with the original data. this is analogous to the question of the boundary correspondence of harmonic conjugates. generalizing a representation of the hilbert transform h in r3 given by t. qian and y. yang (valid in rn), we deﬁne the hilbert transform hf associated to the main vekua equation dw = (df /f )w in bounded lipschitz domains in r3. this leads to an investigation of the three-dimensional analogue of the dirichlet-to-neumann map for the conductivity equation. keywords: hilbert transform, vekua-hilbert transform, main vekua equa- tion, conductivity equation, dirichlet-to-neumann map, div-curl system, quaternionic analysis, hyperholomorphic function, monogenic function classiﬁcation: 44a15 30e20 30g20 35j25 35q60 1 "
1803.03294,". we show that if there exists a counter example for the rational case of the franks-misiurewicz conjecture, then it must exhibit unbounded deviations in the complementary direction of its rotation set. "
1803.03295," in previous work by avena and den hollander [2], a model of a one-dimensional random walk in a dynamic random environment was proposed where the random environment is resampled from a given law along a growing sequence of deterministic times. in the regime where the increments of the resampling times diverge, which is referred to as the cooling regime, a weak law of large numbers and certain ﬂuctuation properties were derived under the annealed measure. in the present paper we show that a strong law of large numbers and a quenched large deviation principle hold as well. in the cooling regime, the random walk can be represented as a sum of independent variables, distributed as the increments of a random walk in a static random environment over increasing periods of time. our proofs require suitable multi-layer decompositions of sums of random variables controlled by moments bounds and concentration estimates. along the way we derive two results of independent interest, namely, a concentration inequality for the cumulants of the displacement in the static random environment and an ergodic theorem that deals with limits of sums of triangular arrays representing the structure of the cooling regime. we close by discussing our present understanding of homogenisation eﬀects as a function of the speed of divergence of the increments of the resampling times. msc 2010: 60f05, 60f10, 60g50, 60k37. keywords: random walk, dynamic random environment, resampling times, law of large numbers, large deviation principle, concentration inequalities. acknowledgment: the research in this paper was supported through nwo gravitation grant networks-024.002.003. 1mathematical institute, leiden university, p.o. box 9512, 2300 ra leiden, the netherlands 1 "
1803.03297," it has been recently shown that tachyonic chameleon model of dark energy in which tachyon scalar ﬁeld non-minimally coupled to the matter admits stable scaling attractor solution that could give rise to the late-time accelerated expansion of the universe and hence alleviate the coincidence problem. in the present work, we use data from type ia supernova (sn ia) and baryon acoustic oscillations to place constraints on the model parameters. in our analysis we consider in general exponential and non- exponential forms for the non-minimal coupling function and tachyonic potential and show that the scenario is compatible with observations. 8 1 0 2   r a m 8       ] c q - r g [     1 v 7 9 2 3 0 . 3 0 8 1 : v i x r a 1a.banijamali@nit.ac.ir 2stefano.bellucci@lnf.infn.it 3b.fazlpour@umz.ac.ir 1 1 "
1803.03298,"—generalized division multiplexing (gfdm) is suitable for cognitive radio (cr) networks due to its low out-of-band (oob) emission and high spectral efﬁciency. in this paper, we thus consider the use of gfdm to allow an unlicensed secondary user (su) to access a spectrum hole. however, in an extremely congested spectrum scenario, both active incumbent primary users (pus) on the left and right channels of the spectrum hole will experience oob interference. while constraining this interference, we thus investigate the problem of power allocation to the su transmit subcarriers in order to maximize the overall data rate where the su receiver is employing matched ﬁlter (mf) and zero-forcing (zf) structures. the power allocation problem is thus solved as a classic convex optimization problem. finally, total transmission rate of gfdm is compared with that of orthogonal frequency division multiplexing (ofdm). for instance, when right and left interference temperature should be below 10 dbm, the capacity gain of gfdm over ofdm is 400%. index terms—cr network, gfdm, signal-to-interference- plus-noise ratio, adjacent channel interference, rate optimization problem "
1803.03299," a faint star located 2 arcsec from kic 8462852 was discovered in keck 10 m adaptive optics imaging in the jhk near-infrared (nir) in 2014 by boyajian et al. (2016). the closeness of the star to kic 8462852 suggested the two could constitute a binary, which might have implications for the cause of the brightness dips seen by kepler (boyajian et al. 2016) and in ground-based optical studies (boyajian et al. 2018). here, nir imaging in 2017 using the mimir instrument resolved the pair and enabled measuring their separation. the faint star had moved 67 ± 7 milliarcsec (mas) relative to kic 8462852 since 2014. the relative proper motion of the faint star is 23.9 ± 2.6 mas yr−1, for a tangential velocity of 45 ± 5 km s−1 if it is at the same 390 pc distance as kic 8462852. circular velocity at the 750 au current projected separation is 1.5 km s−1, hence the star pair cannot be bound. keywords: stars: individual (kic 8462852) 8 1 0 2   r a m 8       ] . r s h p - o r t s a [     1 v 9 9 2 3 0 . 3 0 8 1 : v i x r a corresponding author: dan p. clemens clemens@bu.edu ∗ boston university research internship in science and engineering (rise) 2017 summer student 2 clemens et al. "
1803.03303," kepler-78b is a planet of comparable size to the earth (1.2 r⊕), but with an orbital period a thousand times shorter (8.5 hours). currently it is the smallest planet for which the mass, radius, and dayside brightness have all been measured. kepler-78b is an exemplar of the ultra-short-period (usp) planets, a category deﬁned by the simple criterion porb < 1 day. we describe our fourier-based search of the kepler data that led to the discovery of kepler- 78b, and review what has since been learned about the population of usp planets. they are about as common as hot jupiters, almost always smaller than 2 r⊕, and often members of compact multi-planet systems. they might be the exposed rocky cores of “gas dwarfs,” the planets between 2–4 r⊕ in size that are commonly found in somewhat wider orbits. keywords: planets, time-series photometry "
1803.03304," many recent studies of the motor system are divided into two distinct approaches: those that investigate how motor responses are encoded in cortical neurons’ ﬁring rate dynamics and those that study the learning rules by which mammals and songbirds develop reliable motor responses. computationally, the ﬁrst approach is encapsulated by reservoir computing models, which can learn intricate motor tasks and produce internal dynamics strikingly similar to those of motor cortical neurons, but rely on biologically unrealistic learning rules. the more realistic learning rules developed by the second approach are often derived for simpliﬁed, discrete tasks in contrast to the intricate dynamics that characterize real motor responses. we bridge these two approaches to develop a biologically realistic learning rule for reservoir computing. our algorithm learns simulated motor tasks on which previous reservoir computing algorithms fail, and reproduces experimental ﬁndings including those that relate motor learning to parkinson’s disease and its treatment. 8 1 0 2   r a m 8       ] . c n o i b - q [     1 v 4 0 3 3 0 . 3 0 8 1 : v i x r a 1 "
1803.03305," using a sample of 4 galaxy clusters at 1.35 < z < 1.65 and 10 galaxy clusters at 0.85 < z < 1.35, we measure the environmental quenching timescale, tq, corresponding to the time required after a galaxy is accreted by a cluster for it to fully cease star formation. cluster members are selected by a photometric-redshift criterion, and categorized as star-forming, quiescent, or intermediate according to their dust-corrected rest-frame colors and magnitudes. we employ a “delayed-then-rapid” quench- ing model that relates a simulated cluster mass accretion rate to the observed numbers of each type of galaxy in the cluster to constrain tq. for galaxies of mass m∗ (cid:38) 1010.5 m(cid:12), we ﬁnd a quenching timescale of tq = 1.24+0.23−0.20 gyr in the z ∼ 1.5 cluster sample, and tq = 1.50+0.19−0.18 gyr at z ∼ 1. using values drawn from the literature, we compare the redshift evolution of tq to timescales predicted for diﬀerent physical quenching mechanisms. we ﬁnd tq to depend on host halo mass such that quench- ing occurs over faster timescales in clusters relative to groups, suggesting that properties of the host halo are responsible for quenching high-mass galaxies. between z = 0 and z = 1.5, we ﬁnd that tq scrudgey@gmail.com gillian.wilson@ucr.edu 2 foltz et al. evolves faster than the molecular gas depletion timescale and slower than an sfr-outﬂow timescale, but is consistent with the evolution of the dynamical time. this suggests that environmental quench- ing in these galaxies is driven by the motion of satellites relative to the cluster environment, although due to uncertainties in the atomic gas budget at high redshift, we cannot rule out quenching due to simple gas depletion. keywords: galaxy clusters, galaxy formation, galaxies: evolution the evolution of environmental quenching timescales to z ∼ 1.6 3 "
1803.03306," 8 1 0 2   r a m 8       ] . r p h t a m [     1 v 6 0 3 3 0 . 3 0 8 1 : v i x r a the interchange of limits result. consider a system of n parallel single-server queues with unit-exponential service time distribution and a single dispatcher where tasks arrive as a poisson process of rate λ(n). when a task arrives, the dispatcher assigns it to one of the servers according to the join-the-shortest queue (jsq) policy. eschenfeldt and gamarnik (2015) established that in the halﬁn-whitt of the system under the jsq policy converges weakly on any ﬁnite time interval to a certain convergence result extends to the steady state as well, i.e., stationary occupancy measure of regime where (n − λ(n))/√n → β > 0 as n →∞, appropriately scaled occupancy measure diffusion process as n →∞. recently, it was further established by braverman (2018) that the the system converges weakly to the steady state of the diffusion process as n →∞, proving in this paper we perform a detailed analysis of the steady state of the above diffusion process. speciﬁcally, we establish precise tail-asymptotics of the stationary distribution and scaling of extrema of the process on large time-interval. our results imply that the asymptotic steady-state scaled number of servers with queue length two or larger exhibits an exponen- tial tail, whereas that for the number of idle servers turns out to be gaussian. from the methodological point of view, the diffusion process under consideration goes beyond the state-of-the-art techniques in the study of the steady-state of diffusion processes. lack of any closed form expression for the steady state and intricate interdependency of the process dy- namics on its local times make the analysis signiﬁcantly challenging. we develop a technique involving the theory of regenerative processes that provides a tractable form for the stationary measure, and in conjunction with several sharp hitting time estimates, acts as a key vehicle in establishing the results. the technique and the intermediate results might be of independent interest, and can possibly be used in understanding the bulk behavior of the process. keywords and phrases: join the shortest queue; diffusion limit; steady state analysis; local time; non-elliptic diffusion; halﬁn-whitt regime; regenerative processes. 2010 mathematics subject classiﬁcation: primary 60k25, 60j60; secondary 60k05, 60h20. ∗sayan@email.unc.edu †d.mukherjee@tue.nl 1 1 "
1803.03309," 2010 ams subject classiﬁcation: 20c15, 20c33 1 "
1803.03311,". we study liftings of abelian model structures to categories of chain com- plexes and construct a realization functor d(a ) → ho(m) for any coﬁbrantly gen- erated, hereditary abelian model structure m on a grothendieck category a . "
1803.03312," recent developments in nanoscale experimental techniques made it possible to utilize single molecule junctions as devices for electronics and energy transfer with quantum coherence playing an important role in their thermoelectric characteristics. theoretical studies on the eﬃciency of nanoscale devices usually employ rate (pauli) equations, which do not account for quantum coherence. therefore, the question whether quantum coherence could improve the eﬃciency of a molecular device cannot be fully addressed within such considerations. here, we employ a nonequilibrium green function approach to study the eﬀects of quantum coherence and dephasing on the thermoelectric performance of molecular heat engines. within a generic bichromophoric donor-bridge-acceptor junction model, we show that quantum coherence may increase eﬃciency compared to quasi-classical (rate equation) predictions and that pure dephasing and dissipation destroy this eﬀect. 8 1 0 2   r a m 8       ] l l a h - s e m . t a m - d n o c [     1 v 2 1 3 3 0 . 3 0 8 1 : v i x r a 1 i. "
1803.03315," as usual, pn (n ≥ 1) denotes the path on n vertices, and cn (n ≥ 3) denotes the cycle on n vertices. for a family h of graphs, we say that a graph g is h-free if no induced subgraph of g is isomorphic to any graph in h. we present a decomposition theorem for the class of (p7, c4, c5)-free graphs; in fact, we give a complete structural characterization of (p7, c4, c5)-free graphs that do not admit a clique- cutset. we use this decomposition theorem to show that the class of (p7, c4, c5)-free graphs is χ-bounded by a linear function (more precisely, every (p7, c4, c5)-free graph g satisﬁes χ(g) ≤ 3 2 ω(g)). we also use the decomposition theorem to construct an o(n3) algorithm for the minimum coloring problem, an o(n2m) algorithm for the maximum weight stable set problem, and an o(n3) algorithm for the maximum weight clique problem for this class, where n denotes the number of vertices and m the number of edges of the input graph. ∗department of mathematics, wilfrid laurier university, waterloo, on, canada, n2l 3c5. email: kcameron@wlu.ca. research supported by the natural sciences and engineering research council of canada (nserc) grant rgpin-2016-06517. †department of mathematics, wilfrid laurier university, waterloo, on, canada, n2l 3c5. email: dynamichuang@gmail.com. research supported by the natural sciences and engineering research council of canada (nserc) grant rgpin-2016-06517. ‡school of computing, university of leeds, leeds ls2 9jt, uk. email: i.penev@leeds.ac.uk. partially supported by epsrc grant ep/n0196660/1. §department of mathematics, university of central florida, orlando, fl 32816, usa. email: vaidysivaraman@gmail.com. partially supported by the european research coun- cil under the european union’s seventh framework programme (fp7/2007-2013) / erc grant agreement 339109. 1 1 "
1803.03316," a subgraph of an edge-coloured graph is called rainbow if all its edges have distinct colours. the study of rainbow subgraphs goes back more than two hundred years to the work of euler on latin squares. since then rainbow structures have been the focus of extensive research and have found applications in the areas of graph labelling and decomposition. an edge-colouring is locally k-bounded if each vertex is contained in at most k edges of the same colour. in this paper we prove that any such edge-colouring of the complete graph kn contains a rainbow copy of every tree with at most (1 − o(1))n/k vertices. as a locally k-bounded edge-colouring of kn may have only (n − 1)/k distinct colours, this is essentially tight. as a corollary of this result we obtain asymptotic versions of two long-standing conjectures in graph theory. firstly, we prove an asymptotic version of ringel’s conjecture from 1963, showing that any n-edge tree packs into the complete graph k2n+o(n) to cover all but o(n2) of its edges. secondly, we show that all trees have an almost-harmonious labelling. the existence of such a labelling was conjectured by graham and sloane in 1980. we also discuss some additional applications. 1 "
1803.03317," a large number of works in egocentric vision have con- centrated on action and object recognition. detection and segmentation of hands in ﬁrst-person videos, however, has less been explored. for many applications in this domain, it is necessary to accurately segment not only hands of the camera wearer but also the hands of others with whom he is interacting. here, we take an in-depth look at the hand segmentation problem. in the quest for robust hand segmen- tation methods, we evaluated the performance of the state of the art semantic segmentation methods, off the shelf and ﬁne-tuned, on existing datasets. we ﬁne-tune reﬁnenet, a leading semantic segmentation method, for hand segmen- tation and ﬁnd that it does much better than the best con- tenders. existing hand segmentation datasets are collected in the laboratory settings. to overcome this limitation, we contribute by collecting two new datasets: a) egoyoutube- hands including egocentric videos containing hands in the wild, and b) handoverface to analyze the performance of our models in presence of similar appearance occlusions. we further explore whether conditional random ﬁelds can help reﬁne generated hand segmentations. to demonstrate the beneﬁt of accurate hand maps, we train a cnn for hand-based activity recognition and achieve higher accu- racy when a cnn was trained using hand maps produced by the ﬁne-tuned reﬁnenet. finally, we annotate a subset of the egohands dataset for ﬁne-grained action recognition and show that an accuracy of 58.6% can be achieved by just looking at a single hand pose which is much better than the chance level (12.5%). "
1803.03319," in extreme classification problems, learning algorithms are required to map instances to labels from an extremely large label set. we build on a recent extreme classification framework with logarithmic time and space [15], and on a general approach for error correcting output coding (ecoc [1]), and introduce a flexible and efficient approach accompanied by bounds. our framework employs output codes induced by graphs, and offers a tradeoff between accuracy and model size. we show how to find the sweet spot of this tradeoff using only the training data. our experimental study demonstrates the validity of our assumptions and claims, and shows the superiority of our method compared with state-of-the-art algorithms. keywords multiclass learning, extreme classification, error correcting out- put coding acm reference format: itay evron, edward moroshko, and koby crammer. 2018. efficient loss- based decoding on graphs for extreme classification. in proceedings of (tech report). acm, new york, ny, usa, article 4, 9 pages. https://doi.org/ 10.1145/nnnnnnn.nnnnnnn "
1803.03321,". we suppose that a structure working as a quantum switch will be a signiﬁcant element of future networks realizing transmissions of quantum information. in this chapter we analyze a process of switch’s op- erating – especially in systems with a noise presence. the noise is caused by a phenomenon of quantum decoherence, i.e. distorting of quantum states because of an environmental inﬂuence, and also by some imper- fections of quantum gates’ implementation. in the face of mentioned problems, the possibility of tracing the switch’s behavior during its op- erating seems very important. to realize that we propose to utilize a coherence measure which, as we present in this chapter, is suﬃcient to describe operating of the quantum switch and to verify correctness of this process. it should be also stressed that the value of coherence mea- sure may be estimated by a quantum circuit, designed especially for this purpose. keywords: quantum information transfer, quantum switch, quantum coherence "
1803.03322," owing to its longevity and enormous information density, dna, the molecule encoding biological information, has emerged as a promising archival storage medium. however, due to technological con- straints, data can only be written onto many short dna molecules that are stored in an unordered way, and can only be read by sampling from this dna pool. moreover, imperfections in writing (synthesis), reading (sequencing), storage, and handling of the dna, in particular ampliﬁcation via pcr, lead to a loss of dna molecules and induce errors within the molecules. in order to design dna storage systems, a qualitative and quantitative understanding of the errors and the loss of molecules is crucial. in this paper, we characterize those error probabilities by analyzing data from our own experiments as well as from experiments of two diﬀerent groups. we ﬁnd that errors within molecules are mainly due to syn- thesis and sequencing, while imperfections in handling and storage lead to a signiﬁcant loss of sequences. the aim of our study is to help guide the design of future dna data storage systems by providing a quantitative and qualitative understanding of the dna data storage channel. "
1803.03323," we identify the desired characteristics and parameters of a β > 1 magnetized plasma, possibly with highly tangled, open ﬁeld lines, that could be a suitable target to be compressed to fusion conditions by a spherically imploding plasma liner [s. c. hsu et al., ieee trans. plasma sci. 40, 1287 (2012)] formed by merging hypersonic plasma jets. this concept is known as plasma-jet-driven magneto-inertial fusion (pjmif). we set requirements on the target and liner such that (a) compressional heating dominates over thermal trans- port in the target, and (b) magnetic ampliﬁcation due to compression dominates over dissipation over the en- tire implosion. we also evaluate the requirements to avoid drift-instability-induced anomalous transport and current-driven anomalous resistivity in the target. next, we describe possible approaches to create such a mag- netized, β > 1 plasma target. finally, assuming such a target can be created, we evaluate the feasibility of a proof-of-concept experiment using presently achievable plasma jets to demonstrate target compressional heat- ing at a plasma-liner kinetic energy of . 100 kj (a few hundred times below that needed in a pjmif reactor). keywords plasma liners · plasma jets · magneto- inertial fusion scott c. hsu los alamos national laboratory, los alamos, nm 87545 tel.: +1-505-667-3386 e-mail: scotthsu@lanl.gov samuel j. langendorf los alamos national laboratory, los alamos, nm 87545 tel.: +1-505-667-9292 e-mail: samuel.langendorf@lanl.gov "
1803.03324," graphs are fundamental data structures which con- cisely capture the relational structure in many im- portant real-world domains, such as knowledge graphs, physical and social interactions, language, and chemistry. here we introduce a powerful new approach for learning generative models over graphs, which can capture both their structure and attributes. our approach uses graph neural networks to express probabilistic dependencies among a graph’s nodes and edges, and can, in principle, learn distributions over any arbitrary graph. in a series of experiments our results show that once trained, our models can generate good quality samples of both synthetic graphs as well as real molecular graphs, both unconditionally and conditioned on data. compared to baselines that do not use graph-structured representations, our models often perform far better. we also explore key challenges of learning generative models of graphs, such as how to handle symmetries and ordering of elements during the graph generation process, and offer possible solutions. our work is the ﬁrst and most general approach for learn- ing generative models over arbitrary graphs, and opens new directions for moving away from re- strictions of vector- and sequence-like knowledge representations, toward more expressive and ﬂexi- ble relational data structures. "
1803.03325,". we prove that the cohomology sheaves of the relative dualizing complex of a ﬂat family of varieties with semi-log-canonical or du bois singularities are ﬂat and commute with base change. this is a local version of our earlier similar result where the family was assumed to be projective. we also derive several consequences for deformations of semi-log- canonical and of du bois singularities. one of the diﬃculties of higher dimensional birational geometry and moduli theory is that, while terminal and canonical singularities are cohen-macaulay, log canonical singularities are frequently not. "
1803.03328," this paper presents a method for hyperspectral image classiﬁcation using support vector data description (svdd) with gaussian kernel function. svdd has been a popular machine-learning technique for single-class classiﬁca- tion, but selecting the proper gaussian kernel bandwidth to achieve the best classiﬁcation performance is always a challenging problem. in this paper, we propose a new automatic, unsupervised gaussian kernel bandwidth selection approach. a multi-class svdd classiﬁcation scheme is designed based on the proposed approach and applied to hyperspectral image data. the performance of the multi-class svdd classiﬁcation scheme is evaluated on three frequently used hyperspectral data sets and preliminary results show our proposed method can achieve better performance than published results on these data sets. keywords: hyperspectral, support vector data description (svdd), one-class support vector machine (svm) "
1803.03332,"—this is the ﬁrst work augmenting hardware attacks mounted on obfuscated circuits by incorporating deep recurrent neural network (d − rnn). logic encryption obfuscation has been used for thwarting counterfeiting, overproduction, and reverse engineering but vulnerable to attacks. there have been efﬁcient schemes, e.g., satis f iability−checking (sat) based attack, which can potentially compromise hardware obfuscation circuits. nevertheless, not only there exist countermeasures against such attacks in the state-of-the-art (including the recent delay+logic locking (dll) scheme in dac(cid:48)17), but the sheer amount of time/resources to mount the attack could hinder its efﬁcacy. in this paper, we propose a deep rnn-oriented approach, called bocanet, to (i) compromise the obfuscated hardware at least an order-of magnitude more efﬁciently (> 20x faster with relatively high success rate) compared to existing attacks; (ii) attack such locked hardware even when the resources to the attacker are only limited to insigniﬁcant number of i/o pairs (< 0.5%) to reconstruct the secret key; and (iii) break a number of experimented benchmarks (iscas− 85 c423, c1355, c1908, and c7552) successfully. index terms— deep recurrent neural network (d-rnn), hardware obfuscation, logic encryption. over the past "
1803.03333," suppose that y = m(x1, . . . , xp), where (x1, . . . , xp) are inputs, y is an output, and m is an unknown link function. the sobol indices gauge the sensitivity of each x against y by estimating the regression curve’s variability between them. in this paper, we estimate these curves with a kernel-based method. under some regularity conditions, the mean squared error for the ﬁrst order sobol indices have a parametric behavior. the bandwidth converge to n−1/4 where n is the sample size and goes to inﬁnity. for ﬁnite samples, the cross-validation method produces a structural bias. to remedy this, we propose a bootstrap procedure which reconstruct the model residuals and re-estimate the nonparametric regression curve. with the new set of curves, the procedure corrects the bias in the sobol index. we test our method with two simulated numerical examples with complex functions, assessing its performance. "
1803.03334," quantum eﬀects on a pair of bateman oscillators embedded in an ambient non- commutative space (moyal plane) is analyzed using both path integral and canonical quantization schemes within the framework of hilbert-schmidt operator formulation. we adopt a method which is distinct from the one which employs ’t hooft’s scheme of quantization, carried out earlier in the literature where the ambient space was taken to be commutative. our quantization shows that we end up ﬁnally again with a bateman system except that the damping factor undergoes renormalization. the corresponding expression shows that the renormalized damping factor can be non-zero even if ”bare” one is zero to begin with. conversely, the noncommuatative parameter θ, taken to be a free one now, can be ﬁne-tuned to get a vanishing renormalized damping factor. this indicates a duality between dissipative commutative theory and non-dissipative noncommutative theory. 1 "
1803.03335,". morphological dynamics of bilayer membrane is intrinsically coupled to the transla- tional and orientational localization of membrane proteins. in this paper we are concerned with the orientational localization of membrane proteins in the absence of protein interaction and correlation. entropic energy depending on the angular distribution function and the curvature energy depending on the principal curvature vectors are introduced to assemble an energy functional for the coupled system. application of the onsager’s variational principle gives rise to a generalized smoluchowskii equation governing the temporal and angular variations of the protein orientation. we prove the existence of the stationary solution of the equation as ﬁxed points of a continuous nonlinear nonlocal map, and for biologically relevant conditions we obtain the uniqueness of the solution. to approx- imate the stationary solution in the fourier space we construct an eﬃcient numerical method that reduces the expansion and relates the coeﬃcients to the modiﬁed bessel functions of the ﬁrst kind. existence and uniqueness of the numerical solution are justiﬁed for biologically relevant conditions. key words. protein localization; curvature vector; energy functional; stationary solution; exis- tence and uniqueness; bessel functions ams subject classiﬁcations. 35a15, 35k15, 35q99, 60j60 "
1803.03337,". we show the existence of a lipschitz viscosity solution u in ω to a system of fully nonlinear equations involving pucci-type operators. we study the regularity of the interface ∂{u > 0}∩ω and we show that the viscosity inequalities of the system imply, in the weak sense, the free boundary condition u+ ν− , and hence u is a solution to a two-phase free boundary problem. we show that we can apply the classical method of sup-convolutions developed by the ﬁrst author in [5, 6], and generalized by wang [20, 21] and feldman [11] to fully nonlinear operators, to conclude that the regular points in ∂{u > 0} ∩ ω form an open set of class c 1,α. a novelty in our problem is that we have diﬀerent operators, f + and f −, on each side of the free boundary. in the particular case when these operators are the pucci’s extremal operators m+ and m−, our results provide an alternative approach to obtain the stationary limit of a segregation model of populations with nonlinear diﬀusion in [19]. "
1803.03339," we investigate the k-error linear complexity of pseudorandom binary se- quences of period pr derived from the euler quotients modulo pr−1, a power of an odd prime p for r ≥ 2. when r = 2, this is just the case of polynomial quotients (including fermat quotients) modulo p, which has been studied in an earlier work of chen, niu and wu. in this work, we establish a recursive relation on the k-error linear complexity of the sequences for the case of r ≥ 3. we also state the exact values of the k-error linear complexity for the case of r = 3. from the results, we can ﬁnd that the k-error linear complexity of the sequences (of period pr) does not decrease dramatically for k < pr−2(p − 1)2/2. keywords: cryptography, pseudorandom binary sequences, k-error linear com- plexity, euler quotients. 2010 msc: 94a55, 94a60, 65c10 1 "
1803.03340," we have investigated the tukh122 prestellar core in the orion a cloud using alma 3 mm dust continuum, n2h+ (j = 1 − 0), and ch3oh (jk = 2k − 1k) molecular line observations. previous studies showed that tukh122 is likely on the verge of star formation because the turbulence is almost dissipated and chemically evolved among other starless cores in the orion a cloud. by combining alma 12-m and aca data, we recover extended emission with a resolution of ∼ 5(cid:48)(cid:48) corresponding to 0.01 pc and identify 6 condensations with a mass range of 0.1 − 0.4 m(cid:12) and a radius of (cid:46) 0.01 pc. these condensations are gravitationally bound following a virial analysis and are embedded in the ﬁlament including the elongated core with a mass of ∼ 29 m(cid:12) and a radial density proﬁle of r−1.6 derived by herschel. the separation of these condensations is ∼ 0.035 pc, consistent with the thermal jeans length at a density of 4.4× 105 cm−3. this density is similar to the central part of the core. we also ﬁnd a tendency that the n2h+ molecule seems to deplete at the dust peak condensation. this condensation may be beginning to collapse because the linewidth becomes broader. therefore, the fragmentation still occurs in the prestellar core by thermal jeans instability and multiple stars are formed within the tukh122 prestellar core. the ch3oh emission shows a large shell-like distribution and surrounds these condensations, suggesting that the ch3oh molecule formed on dust grains is released into gas phase by non-thermal desorption such as photoevaporation caused by cosmic-ray induced uv radiation. keywords: ism: clouds —stars: individual (orion molecular cloud) —ism: formation —ism: molecules 1. "
1803.03345," in this paper, we present an effective and efﬁcient face deblurring algorithm by exploiting semantic cues via deep convolutional neural networks (cnns). as face images are highly structured and share several key semantic compo- nents (e.g., eyes and mouths), the semantic information of a face provides a strong prior for restoration. as such, we propose to incorporate global semantic priors as input and impose local structure losses to regularize the output within a multi-scale deep cnn. we train the network with percep- tual and adversarial losses to generate photo-realistic re- sults and develop an incremental training strategy to handle random blur kernels in the wild. quantitative and qualita- tive evaluations demonstrate that the proposed face deblur- ring algorithm restores sharp images with more facial de- tails and performs favorably against state-of-the-art meth- ods in terms of restoration quality, face recognition and ex- ecution speed. "
1803.03346," customer ratings are valuable sources to understand their satis- faction and are critical for designing better customer experiences and recommendations. the majority of customers, however, do not respond to rating surveys, which makes the result less repre- sentative. to understand overall satisfaction, this paper aims to investigate how likely customers without responses had satisfac- tory experiences compared to those respondents. to infer customer satisfaction of such unlabeled sessions, we propose models using recurrent neural networks (rnns) that learn continuous represen- tations of unstructured text conversation. by analyzing online chat logs of over 170,000 sessions from samsung’s customer service department, we make a novel finding that while labeled sessions contributed by a small fraction of customers received overwhelm- ingly positive reviews, the majority of unlabeled sessions would have received lower ratings by customers. the data analytics pre- sented in this paper not only have practical implications for helping detect dissatisfied customers on live chat services but also make theoretical contributions on discovering the level of biases in online rating platforms. "
1803.03347," current multi-person localisation and tracking systems have an over reliance on the use of appearance models for target re-identiﬁcation and almost no approaches em- ploy a complete deep learning solution for both objectives. we present a novel, complete deep learning framework for multi-person localisation and tracking. in this context we ﬁrst introduce a light weight sequential generative adver- sarial network architecture for person localisation, which overcomes issues related to occlusions and noisy detections, typically found in a multi person environment. in the pro- posed tracking framework we build upon recent advances in pedestrian trajectory prediction approaches and propose a novel data association scheme based on predicted trajecto- ries. this removes the need for computationally expensive person re-identiﬁcation systems based on appearance fea- tures and generates human like trajectories with minimal fragmentation. the proposed method is evaluated on mul- tiple public benchmarks including both static and dynamic cameras and is capable of generating outstanding perfor- mance, especially among other recently proposed deep neu- ral network based approaches. "
1803.03348,": the rapid development of high-throughput technologies has enabled the gen- eration of data from biological or disease processes that span multiple layers, like genomic, proteomic or metabolomic data, and further pertain to multiple sources, like disease sub- types or experimental conditions. in this work, we propose a general statistical framework based on gaussian graphical models for horizontal (i.e. across conditions or subtypes) and vertical (i.e. across diﬀerent layers containing data on molecular compartments) integra- tion of information in such datasets. we start with decomposing the multi-layer problem into a series of two-layer problems. for each two-layer problem, we model the outcomes at a node in the lower layer as dependent on those of other nodes in that layer, as well as all nodes in the upper layer. we use a combination of neighborhood selection and group- penalized regression to obtain sparse estimates of all model parameters. following this, we develop a debiasing technique and asymptotic distributions of inter-layer directed edge weights that utilize already computed neighborhood selection coeﬃcients for nodes in the upper layer. subsequently, we establish global and simultaneous testing procedures for these edge weights. performance of the proposed methodology is evaluated on synthetic data. keywords: data integration; gaussian graphical models; neighborhood selection; group lasso; high-dimensional asymptotics; multiple testing; false discovery rate 8 1 0 2   r a m 9       ] l m . t a t s [     1 v 8 4 3 3 0 . 3 0 8 1 : v i x r a ∗email: smajumdar@ufl.edu †corresponding author. email: gmichail@ufl.edu 1 1 "
1803.03349," let α : 1, (1,√x,√y)∧ be a weight sequence with stampﬂi’s sub- normal completion and let wα be its associated weighted shift. in this paper we discuss some properties of the region u : ={(x, y) : wα is semi- cubically hyponormal} and describe the shape of the boundary of u . in particular, we improve the results of [19, theorem 4.2] with properties of u . "
1803.03350,". the hermitian eigenvalue problem asks for the possible eigenvalues of a sum of hermitian matrices given the eigenvalues of the summands. this is a problem about the lie algebra of the maximal compact subgroup of g “ slpnq . there is a polyhedral cone (the “eigencone”) determining the possible answers to the problem. these eigencones can be deﬁned for arbitrary semisimple groups g, and also control the (suitably stabilized) problem of existence of non-zero invariants in tensor products of irreducible representations of g. we give a description of the extremal rays of the eigencones for arbitrary semisimple groups g by ﬁrst observing that extremal rays lie on regular facets, and then classifying extremal rays on an arbitrary regular face. explicit formulas are given for some extremal rays, which have an explicit geometric meaning as cycle classes of interesting loci, on an arbitrary regular face, and the remaining extremal rays on that face are understood by a geometric process we introduce, and explicate numerically, called induction from levi subgroups. several numerical examples are given. the main results, and methods, of this paper generalize [bel17] which handled the case of g “ slpnq. "
1803.03351," in this paper, we give strong lower bounds on the size of the sets of products of matrices in some certain groups. more precisely, we prove an analogue of a result due to chapman and iosevich for matrices in sl2(fp) with restricted entries on a small set. we also provide extensions of some recent results on expansion for cubes in heisenberg group due to hegyv´ari and hennecart. 8 1 0 2   r a   m 8 2     ] t n h t a m . [     2 v 1 5 3 3 0 . 3 0 8 1 : v i x r a 1 "
1803.03352,"—with the availability of low-cost and compact 2.5/3d visual sensing devices, computer vision community is experiencing a growing interest in visual scene understanding. this survey paper provides a comprehensive background to this research topic. we begin with a historical perspective, followed by popular 3d data representations and a comparative analysis of available datasets. before delving into the application speciﬁc details, this survey provides a succinct introduction to the core technologies that are the underlying methods extensively used in the literature. afterwards, we review the developed techniques according to a taxonomy based on the scene understanding tasks. this covers holistic indoor scene understanding as well as subtasks such as scene classiﬁcation, object detection, pose estimation, semantic segmentation, 3d reconstruction, saliency detection, physics-based reasoning and affordance prediction. later on, we summarize the performance metrics used for evaluation in different tasks and a quantitative comparison among the recent state-of-the-art techniques. we conclude this review with the current challenges and an outlook towards the open research problems requiring further investigation. index terms—3d scene understanding, semantic labeling, geometry estimation, deep networks, markov random ﬁelds. ! "
1803.03354," visual saliency patterns are the result of a variety of factors aside from the image being parsed, however existing approaches have ignored these. to address this limitation, we propose a novel saliency estimation model which leverages the semantic modelling power of conditional generative adversarial networks together with memory architectures which capture the subject’s behavioural patterns and task dependent factors. we make contributions aiming to bridge the gap between bottom-up feature learning capabilities in modern deep learning architectures and traditional top-down hand- crafted features based methods for task speciﬁc saliency modelling. the conditional nature of the proposed framework enables us to learn contextual semantics and relationships among diﬀerent tasks together, instead of learning them separately for each task. our studies not only shed light on a novel application area for gener- ative adversarial networks, but also emphasise the im- portance of task speciﬁc saliency modelling and demon- strate the plausibility of fully capturing this context via an augmented memory architecture. "
1803.03355,". in this paper, we mainly investigate the nonuniform sampling for random signals which are bandlimited in the linear canonical transform (lct) domain. we show that the nonuniform sampling for a random signal bandlimited in the lct domain is equal to the uniform sampling in the sense of second order statistic characters after a pre- ﬁlter in the lct domain. moreover, we propose an approximate recovery approach for nonuniform sampling of random signals bandlimited in the lct domain. furthermore, we study the mean square error of the nonuniform sampling. finally, we do some simulations to verify the correctness of our theoretical results. keywords. nonuniform sampling; linear canonical transform; random signals; ap- proximate reconstruction; sinc interpolation 1 "
1803.03356," many researchers and statisticians are conﬂicted over the practice of hypothesis test- ing and statistical signiﬁcance thresholds. there are several alternatives, and in this paper we propose one that focuses on estimation. in particular, we focus on the prob- ability that a future parameter estimate will exceed a speciﬁed amount. after brieﬂy reviewing background on p-values, signiﬁcance thresholds, and a few alternatives, we describe the exceedance probability for parameter estimates and provide examples of how the exceedance probability, along with corresponding conﬁdence intervals, can provide useful information for the purposes of drawing inference and making decisions. we focus on applications in one-sample tests and linear regression with potential ex- tensions to generalized linear models and cox regression. we also discuss connections to conditional and predictive power, and analyze the relationship between conﬁdence intervals for the exceedance probability and conﬁdence intervals for parameter esti- mates. this relationship leads to an alternative interpretation of conﬁdence intervals for parameter estimates that might be useful for pedagogical purposes. keywords: p-values, bayes factors, statistical intervals 8 1 0 2   r a m 9       ] e m . t a t s [     1 v 6 5 3 3 0 . 3 0 8 1 : v i x r a 1 1 "
1803.03357,". we prove majorization inequalities for diﬀerent means of posi- tive deﬁnite matrices. these include the cartan mean (the karcher mean), the log euclidean mean, the wasserstein mean and the power mean. "
1803.03358," given a ﬁxed graph h, the h-free editing problem asks whether we can edit at most k edges to make a graph contain no induced copy of h. we obtain a polynomial kernel for this problem when h is a diamond. the incompressibility dichotomy for h being a 3-connected graph [4] and the classical complexity dichotomy [1] suggest that except for h being a complete/empty graph, h-free editing problems admit polynomial kernels only for a few small graphs h. therefore, we believe that our result is an essential step toward a complete dichotomy on the compressibility of h-free editing. additionally, we give a cubic-vertex kernel for the diamond-free edge deletion problem, which is far simpler than the previous kernel of the same size for the problem. 2012 acm subject classiﬁcation g.2.2 graph algorithms, i.1.2 analysis of algorithms keywords and phrases kernelization, diamond-free graph, h-free editing, graph modiﬁcation problem funding supported in part by the hong kong research grants council (rgc) under grant polyu 252026/15e, the national natural science foundation of china (nsfc) under grant 61572414, and the european research council (erc) under grant 725978. 1 "
1803.03363," we present an effective blind image deblurring method based on a data-driven discriminative prior. our work is motivated by the fact that a good image prior should fa- vor clear images over blurred ones. in this work, we for- mulate the image prior as a binary classiﬁer which can be achieved by a deep convolutional neural network (cnn). the learned prior is able to distinguish whether an input im- age is clear or not. embedded into the maximum a posterior (map) framework, it helps blind deblurring in various sce- narios, including natural, face, text, and low-illumination images. however, it is difﬁcult to optimize the deblur- ring method with the learned image prior as it involves a non-linear cnn. therefore, we develop an efﬁcient numer- ical approach based on the half-quadratic splitting method and gradient decent algorithm to solve the proposed model. furthermore, the proposed model can be easily extended to non-uniform deblurring. both qualitative and quantitative experimental results show that our method performs favor- ably against state-of-the-art algorithms as well as domain- speciﬁc image deblurring approaches. "
1803.03364," barrier options are one of the most widely traded exotic options on stock ex- changes. in this paper, we develop a new stochastic simulation method for pricing barrier options and estimating the corresponding execution probabilities. we show that the proposed method always outperforms the standard monte carlo approach and becomes substantially more eﬃcient when the underlying asset has high volatil- ity, while it performs better than multilevel monte carlo for special cases of barrier options and underlying assets. these theoretical ﬁndings are conﬁrmed by numer- ous simulation results. jel classiﬁcation: g13, c15 keywords: simulation; barrier options pricing; path–dependent derivatives; monte carlo; discretely monitored fornia blvd., pasadena, ca 91125 usa. email: mendoncakeegan@gmail.com ∗the authors would like to thank siu-kui (ivan) au, james beck, damiano brigo, gianluca fusai, steven kou, ioannis kyriakou, zili zhu, and the participants at caltech, university of liverpool, and monash university seminars for helpful comments. any remaining errors are ours. †department of computing and mathematical sciences, california institute of technology, e. cali- ‡department of econometrics and business statistics, monash university, wellington rd, clayton, §department of econometrics and business statistics, monash university, wellington rd, clayton, ¶corresponding author. department of computing and mathematical sciences, california institute victoria 3800, australia. email: athanasios.pantelous@monash.edu. victoria 3800, australia. email: vasileios.kontosakos@monash.edu of technology, e. california blvd., pasadena, ca 91125 usa. email: kostia@caltech.edu 1 1 "
1803.03365," the prediction of extreme events, from avalanches and droughts to tsunamis and epidemics, depends on the formulation and analysis of relevant, complex dynamical systems. such dynamical systems are characterized by high intrinsic dimensionality with extreme events having the form of rare transitions that are several standard deviations away from the mean. such systems are not amenable to classical order-reduction methods through projection of the governing equations due to the large intrinsic dimensionality of the underlying attractor as well as the complexity of the transient events. alternatively, data-driven techniques aim to quantify the dynamics of specific, critical modes by utilizing data-streams and by expanding the dimensionality of the reduced-order model using delayed coordinates. in turn, these methods have major limitations in regions of the phase space with sparse data, which is the case for extreme events. in this work, we develop a novel hybrid framework that complements an imperfect reduced order model, with data-streams that are integrated though a recurrent neural network (rnn) architecture. the reduced order model has the form of projected equations into a low-dimensional subspace that still contains important dynamical information about the system and it is expanded by a long short-term memory (lstm) regularization. the lstm-rnn is trained by analyzing the mismatch between the imperfect model and the data-streams, projected to the reduced-order space. the data-driven model assists the imperfect model in regions where data is available, while for locations where data is sparse the imperfect model still provides a baseline for the prediction of the system state. we assess the developed framework on two challenging prototype systems exhibiting extreme events. we show that the blended approach has improved performance compared with methods that use either data streams or the imperfect model alone. notably the improvement is more significant in regions associated with extreme events, where data is sparse. "
1803.03368,". tianqin is a space-based laser interferometric gravitational wave detector aimed at detecting gravitational waves at low frequencies (0.1 mhz – 1 hz). it is formed by three identical drag- free spacecrafts in an equilateral triangular constellation orbiting around the earth. the distance between each pair of spacecrafts is approximately 1.7×105 km. the spacecrafts are interconnected by infrared laser beams forming up to three michelson-type interferometers. the detailed mission design and the study of science objectives for the tianqin project depend crucially on the orbit and the response of the detector. in this paper, we provide the analytic expressions for the coordinates of the orbit for each spacecraft in the heliocentric-ecliptic coordinate system to the leading orders. this enables a suﬃciently accurate study of science objectives and data analysis, and serves as a ﬁrst step to further orbit design and optimization. we calculate the response of a single michelson detector to plane gravitational waves in arbitrary waveform which is valid in the full range of the sensitive frequencies. it is then used to generate the more realistic sensitivity curve of tianqin. we apply this model on a reference white-dwarf binary as a proof of principle. keywords: gravitational waves, space-borne detector, tianqin "
1803.03369,". we investigate lp boundedness of the maximal bochner-riesz means for self-adjoint op- erators of elliptic type. assuming the ﬁnite speed of propagation for the associated wave operator, from the restriction type estimates we establish the sharp lp boundedness of the maximal bochner- riesz means for the elliptic operators. as applications, we obtain the sharp lp maximal bounds for the schr¨odinger operators on asymptotically conic manifolds, the harmonic oscillator and its pertur- bations or elliptic operators on compact manifolds. 8 1 0 2   r a m 9       ] p a h t a m . [     1 v 9 6 3 3 0 . 3 0 8 1 : v i x r a "
1803.03370," expert finding is an important task in both industry and academia. it is challenging to rank candidates with appropriate expertise for various queries. in addition, different types of objects interact with one another, which naturally forms heterogeneous information networks. we study the task of expert finding in heterogeneous bibliographical networks based on two aspects: textual content anal- ysis and authority ranking. regarding the textual content analysis, we propose a new method for query expansion via locally-trained embedding learning with concept hierarchy as guidance, which is particularly tailored for specific queries with narrow seman- tic meanings. compared with global embedding learning, locally- trained embedding learning projects the terms into a latent seman- tic space constrained on relevant topics, therefore it preserves more precise and subtle information for specific queries. considering the candidate ranking, the heterogeneous information network structure, while being largely ignored in the previous studies of ex- pert finding, provides additional information. specifically, different types of interactions among objects play different roles. we pro- pose a ranking algorithm to estimate the authority of objects in the network, treating each strongly-typed edge type individually. to demonstrate the effectiveness of the proposed framework, we apply the proposed method to a large-scale bibliographical dataset with over two million entries and one million researcher candidates. the experiment results show that the proposed framework outperforms existing methods for both general and specific queries. keywords expert finding, locally-trained embedding, concept hierarchy, query expansion, heterogeneous information networks, ranking "
1803.03371," we introduce a novel meshfree galerkin method for the solution of reissner-mindlin plate problems that is written in terms of the primitive variables only (i.e., rotations and transverse displacement) and is devoid of shear-locking. the proposed approach uses linear maximum-entropy approximations and is built variationally on a two-ﬁeld potential energy functional wherein the shear strain, written in terms of the primitive variables, is computed via a volume-averaged nodal projection operator that is constructed from the kirchhoﬀ constraint of the three-ﬁeld mixed weak form. the stability of the method is rendered by adding bubble-like enrichment to the rotation degrees of freedom. some benchmark problems are presented to demonstrate the accuracy and performance of the proposed method for a wide range of plate thicknesses. keywords: meshfree methods, maximum-entropy approximation, reissner-mindlin plate, shear-locking, vanp method ∗corresponding author. tel: +56 2 2978 4664, fax: +56 2 2689 6057, email address: aortizb@uchile.cl (a. ortiz-bernardin) march 12, 2018 "
1803.03374," a new consistent, spatially adaptive, smoothed particle hydrodynamics (sph) method for fluid-structure interactions (fsi) is presented. the method combines several attributes that have not been simultaneously satisﬁed by other sph methods. speciﬁcally, it is second-order convergent; it allows for resolutions spatially adapted with moving (translating and rotating) boundaries of arbitrary geometries; and, it accelerates the fsi solution as the adaptive approach leads to fewer degrees of freedom without sacriﬁcing accuracy. the key ingredients in the method are a consistent discretization of diﬀerential operators, a posteriori error estimator/distance-based criterion of adaptivity, and a particle-shifting technique. the method is applied in simulating six diﬀerent ﬂows or fsi problems. the new method’s convergence, accuracy, and eﬃciency attributes are assessed by comparing the results it produces with analytical, ﬁnite element, and consistent sph uniform high-resolution solutions as well as experimental data. keywords: spatially adaptive numerical methods, meshless methods, fluid-structure interactions, smoothed particle hydrodynamics, error estimator "
1803.03375," in this paper, we explained the observed γ2−10kev − lbol/ledd correlation in luminous active galactic nuclei within the framework of the condensation of the corona around a supermassive black hole (liu et al. 2015; qiao & liu 2017). speciﬁcally, we systemically test the eﬀects of black hole mass m, the viscosity parameter α, and the magnetic parameter β (with magnetic pressure pm = b2/8π = (1 − β)ptot, ptot = pgas + pm) on the structure of the accretion disc and the corona, as well as the corresponding emergent spectra. it is found that the hard x-ray photon index γ2−10kev nearly does not change with changing black hole mass m, or changing magnetic parameter β. meanwhile, it is found that the geometry of the accretion ﬂow, i.e., the relative conﬁguration of the disc and corona, as well as the emergent spectra can be strongly aﬀected by changing the value of α. by comparing with a sample composed of 29 luminous active galactic nuclei with well constrained x-ray spectra and eddington ratios, it is found that the observed γ2−10kev − lbol/ledd correlation can be well matched with a relatively bigger value of α, i.e., α ∼ 1, as previously also suggested by narayan (1996) for luminous accreting black holes. key words: accretion, accretion discs – black hole physics – galaxies: active "
1803.03376," structured prediction energy networks (spens; belanger & mccallum 2016) use neural network architectures to deﬁne energy functions that can capture arbitrary dependencies among parts of structured outputs. prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. we replace this use of gradient descent with a neural network trained to approximate structured argmax inference. this “inference network” outputs continuous values that we treat as the output structure. we develop large-margin training criteria for joint training of the structured energy function and inference network. on multi-label classiﬁcation we report speed-ups of 10-60x compared to (belanger et al., 2017) while also improving accuracy. for sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. we then demonstrate improved accuracy by augmenting the energy with a “label language model” that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random ﬁelds, suggestive for their general use for fast inference in structured settings. 1 "
1803.03377,". let pn denote the n-th prime number, and let dn = pn+1 − pn. under the hardy–littlewood prime-pair conjecture, we prove logα dn dn ∼( x log log log x log x (log log x)1+α log x 1+α x α = −1, α > −1, xn≤x and establish asymptotic properties for some series of dn without the hardy–littlewood prime-pair conjecture. let pn denote the n-th prime number, and let dn = pn+1 − pn. in [1], erd¨os and "
1803.03378," the task of fine-grained entity type clas- siﬁcation (fetc) consists of assigning types from a hierarchy to entity mentions in text. ex- isting methods rely on distant supervision and are thus susceptible to noisy labels that can be out-of-context or overly-speciﬁc for the train- ing sentence. previous methods that attempt to address these issues do so with heuristics or with the help of hand-crafted features. instead, we propose an end-to-end solution with a neu- ral network model that uses a variant of cross- entropy loss function to handle out-of-context labels, and hierarchical loss normalization to cope with overly-speciﬁc ones. also, previous work solve fetc a multi-label classiﬁcation followed by ad-hoc post-processing. in con- trast, our solution is more elegant: we use pub- lic word embeddings to train a single-label that jointly learns representations for entity men- tions and their context. we show experimen- tally that our approach is robust against noise and consistently outperforms the state-of-the- art on established benchmarks for the task. "
1803.03382," autoregressive sequence models based on deep neural networks, such as rnns, wavenet and the transformer attain state-of-the-art results on many tasks. however, they are difﬁcult to parallelize and are thus slow at processing long sequences. rnns lack parallelism both during training and decoding, while architectures like wavenet and transformer are much more parallelizable during training, yet still operate sequentially during de- coding. inspired by (van den oord et al., 2017), we present a method to extend sequence mod- els using discrete latent variables that makes de- coding much more parallelizable. we ﬁrst auto- encode the target sequence into a shorter sequence of discrete latent variables, which at inference time is generated autoregressively, and ﬁnally de- code the output sequence from this shorter latent sequence in parallel. to this end, we introduce a novel method for constructing a sequence of discrete latent variables and compare it with pre- viously introduced methods. finally, we eval- uate our model end-to-end on the task of neu- ral machine translation, where it is an order of magnitude faster at decoding than comparable autoregressive models. while lower in bleu than purely autoregressive models, our model achieves higher scores than previously proposed non-autoregressive translation models. 8 1 0 2   r p a 9 2       ] g l . s c [     5 v 2 8 3 3 0 . 3 0 8 1 : v i x r a "
1803.03383," low-precision computation is often used to lower the time and energy cost of machine learning, and recently hardware accelerators have been developed to support it. still, it has been used primarily for inference—not training. previous low-precision training algorithms suffered from a fundamental tradeoff: as the number of bits of precision is lowered, quantization noise is added to the model, which limits statistical accuracy. to address this issue, we describe a simple low-precision stochastic gradient descent variant called halp. halp converges at the same theoretical rate as full-precision algorithms despite the noise introduced by using low precision throughout execution. the key idea is to use svrg to reduce gradient variance, and to combine this with a novel technique called bit centering to reduce quantization error. we show that on the cpu, halp can run up to 4× faster than full-precision svrg and can match its convergence trajectory. we implemented halp in tensorquant, and show that it exceeds the validation performance of plain low-precision sgd on two deep learning tasks. "
1803.03385," solar prominences are subject to all kinds of perturbations during their lifetime, and frequently demonstrate oscillations. the study of prominence oscillations provides an alternative way to in- vestigate their internal magnetic and thermal structures as the oscillation characteristics depend on their interplay with the solar corona. prominence oscillations can be classiﬁed into longitudinal and transverse types. we perform three-dimensional ideal magnetohydrodynamic simulations of promi- nence oscillations along a magnetic ﬂux rope, with the aim to compare the oscillation periods with those predicted by various simpliﬁed models and to examine the restoring force. we ﬁnd that the longitudinal oscillation has a period of about 49 minutes, which is in accordance with the pendulum model where the ﬁeld-aligned component of gravity serves as the restoring force. in contrast, the horizontal transverse oscillation has a period of about 10 minutes and the vertical transverse oscilla- tion has a period of about 14 minutes, and both of them can be nicely ﬁtted with a two-dimensional slab model. we also ﬁnd that the magnetic tension force dominates most of the time in transverse oscillations, except for the ﬁrst minute when magnetic pressure overwhelms. corresponding author: c. xia chun.xia@ynu.edu.cn; chenpf@nju.edu.cn 2 keywords: magnetohydrodynamics (mhd) — sun: ﬁlaments, prominences — sun: oscillations — methods: numerical 3 "
1803.03388," the recent discoveries of many double neutron star systems and their detection as ligo-virgo merger events call for a detailed understanding of their origin. explosions of ultra-stripped stars in binary systems have been shown to play a key role in this context and have also gen- erated interest as a potential explanation for rapidly evolving hydrogen-free transients. here we present the ﬁrst attempt to model such explosions based on binary evolution calculations that follow the mass transfer to the companion to obtain a consistent core-envelope structure as needed for reliable predictions of the supernova transient. we simulate the explosion in 2d and 3d, and conﬁrm the modest explosion energies ∼1050 erg and small kick velocities reported earlier in 2d models based on bare carbon-oxygen cores. the spin-up of the neutron star by asymmetric accretion is small in 3d with no indication of spin-kick alignment. simu- lations up to shock breakout show the mixing of sizeable amounts of iron group material into the helium envelope. in view of recent ideas for a mixing-length treatment (mlt) of rayleigh- taylor instabilities in supernovae, we perform a detailed analysis of the mixing, which reveals evidence for buoyancy-drag balance, but otherwise does not support the mlt approximation. the mixing may have implications for the spectroscopic signatures of ultra-stripped super- novae that need to be investigated in the future. our stellar evolution calculation also predicts presupernova mass loss due to an oﬀ-centre silicon deﬂagration ﬂash, which suggests that supernovae from extremely stripped cores may show signs of interactions with circumstellar material. key words: supernovae: general – binaries: close – stars: massive – stars: evolution – stars: neutron 1 "
1803.03390,". we study topological phase transitions and topological quantum ﬁeld eﬀect transistor in monolayer molybdenum disulﬁde (mos2) using a two-band hamiltonian model. without considering the quadratic (q2) diagonal term in the hamiltonian, we show that the phase diagram includes quantum anomalous hall (qah), quantum spin hall (qsh), and spin quantum anomalous hall eﬀect (sqah) regions such that the topological kirchhoﬀ law is satisﬁed in the plane. by considering the q2 diagonal term and including one valley, it is shown that mos2 has a non-trivial topology, and the valley chern number is non-zero for each spin. we show that the wave function is (is not) localized at the edges when the q2 diagonal term is added (deleted) to (from) the spin-valley dirac mass equation. we calculate the quantum conductance of zigzag mos2 nanoribbons by using the nonequilibrium green function method and show how this device works as a ﬁeld eﬀect topological quantum transistor. keywords: phase transition; mos2; topological insulator; field eﬀect transistor phase transition and field eﬀect topological quantum transistor made of monolayer mos22 "
1803.03391,"—visual saliency detection model simulates the human visual system to perceive the scene, and has been widely used in many vision tasks. with the acquisition technology development, more comprehensive information, such as depth cue, inter-image correspondence, or temporal relationship, is available to extend image saliency detection to rgbd saliency detection, co-saliency detection, or video saliency detection. rgbd saliency detection model focuses on extracting the salient regions from rgbd im- ages by combining the depth information. co-saliency detection model introduces the inter-image correspondence constraint to discover the common salient object in an image group. the goal of video saliency detection model is to locate the motion-related salient object in video sequences, which considers the motion cue and spatiotemporal constraint jointly. in this paper, we review different types of saliency detection algorithms, summarize the important issues of the existing methods, and discuss the existent problems and future works. moreover, the evaluation datasets and quantitative measurements are brieﬂy introduced, and the experimental analysis and discission are conducted to provide a holistic overview of different saliency detection methods. index terms—salient object, rgbd saliency detection, depth inter-image correspondence, attribute, co-saliency detection, video saliency detection, spatiotemporal constraint. "
1803.03393," let h = (v, e) be an s-uniform hypergraph of order n and k ≥ 0 be an integer. a k-independent set s ⊆ h is a set of vertices such that the maximum degree in the hypergraph induced by s is at most k. denoted by αk(h) the maximum cardinality of the k-independent set of h. in this paper, we ﬁrst give a lower bound of αk(h) by the maximum degree of h. furthermore, we prove that αk(h) ≥ s(k+1)n 2d+s(k+1) where d is average degree of h, and k ≥ 0 is an integer. ams : 05c65; 05c69 keywords s-uniform hypergraphs, k-independent set 1 "
1803.03395," this is a sequel of our previous work [8] on characterization of maximum sum rate of slotted aloha networks. by extending the analysis to incorporate the capacity-achieving receiver structure, successive interference cancel- lation (sic), this paper aims to identify the rate loss due to random access. speciﬁcally, two representative sic receivers are considered, i.e, ordered sic where packets are decoded in a descending order of their received power, and unordered sic where packets are decoded in a random order. the maximum sum rate and the corresponding optimal parameter setting including the transmission probability and the information encoding rate in both cases are obtained as functions of the mean received signal-to-noise ratio (snr). the comparison to the capture model shows that the gains are signiﬁcant only with the ordered sic at moderate values of the mean received snr ρ. with a large ρ, the rate gap diminishes, and they all have the same high-snr slope of e−1, which is far below that of the ergodic sum capacity of fading channels. the effect of multipacket reception (mpr) on the sum rate performance is also studied by comparing the mpr receivers including sic and the capture model to the classical collision model. "
1803.03396," learning to generate natural scenes has always been a challenging task in computer vision. it is even more painstaking when the generation is conditioned on images with drastically different views. this is mainly because un- derstanding, corresponding, and transforming appearance and semantic information across the views is not trivial. in this paper, we attempt to solve the novel problem of cross-view image synthesis, aerial to street-view and vice versa, using conditional generative adversarial networks (cgan). two new architectures called crossview fork (x- fork) and crossview sequential (x-seq) are proposed to generate scenes with resolutions of 64×64 and 256×256 pixels. x-fork architecture has a single discriminator and a single generator. the generator hallucinates both the im- age and its semantic segmentation in the target view. x-seq architecture utilizes two cgans. the ﬁrst one generates the target image which is subsequently fed to the second cgan for generating its corresponding semantic segmenta- tion map. the feedback from the second cgan helps the ﬁrst cgan generate sharper images. both of our proposed architectures learn to generate natural images as well as their semantic segmentation maps. the proposed methods show that they are able to capture and maintain the true se- mantics of objects in source and target views better than the traditional image-to-image translation method which con- siders only the visual appearance of the scene. extensive qualitative and quantitative evaluations support the effec- tiveness of our frameworks, compared to two state of the art methods, for natural scene generation across drastically different views. "
1803.03400," the oxygen dopants are essential in tuning electronic properties of bi2sr2can−1cuno2n+4+δ su- perconductors. here we apply the technique of scanning tunneling microscopy and spectroscopy to study the inﬂuence of oxygen dopants in an optimally doped bi2sr2−xlaxcuo6+δ and an over- doped bi2−ypbysr2cuo6+δ. in both samples, we ﬁnd that interstitial oxygen atoms on the sro layers dominate over the other two forms of oxygen dopants, oxygen vacancies on the sro layers and interstitial oxygen atoms on the bio layers. the hole doping is estimated from the oxygen concentration, as compared to the result extracted from the measured fermi surface. the precise spatial location is employed to obtain a negative correlation between the oxygen dopants and the inhomogeneous pseudogap. 8 1 0 2   r p a 3       ] n o c - r p u s . t a m - d n o c [     2 v 0 0 4 3 0 . 3 0 8 1 : v i x r a ∗ yiyin@zju.edu.cn 1 i. "
1803.08061," we have explored the evolution of gas distributions from cosmological simulations carried out using the ramses adaptive mesh reﬁnement (amr) code, to explore the eﬀects of resolution on cosmological hydrodynamical simulations. it is vital to understand the eﬀect of both the resolution of initial conditions and the ﬁnal resolution of the simulation. lower initial resolution simulations tend to produce smaller numbers of low mass structures. this will strongly aﬀect the assembly history of objects, and has the same eﬀect of simulating diﬀerent cosmologies. the resolution of initial conditions is an important factor in simulations, even with a ﬁxed maximum spatial resolution. the power spectrum of gas in simulations using amr diverges strongly from the ﬁxed grid approach - with more power on small scales in the amr simulations - even at ﬁxed physical resolution and also produces oﬀsets in the star formation at speciﬁc epochs. this is because before certain times the upper grid levels are held back to maintain approximately ﬁxed physical resolution, and to mimic the natural evolution of dark matter only simulations. although the impact of hold back falls with increasing spatial and initial-condition resolutions, the oﬀsets in the star formation remain down to a spatial resolution of 1 kpc. these oﬀsets are of order of 10-20%, which is below the uncertainty in the implemented physics but are expected to aﬀect the detailed properties of galaxies. we have implemented a new grid-hold-back approach to minimize the impact of hold back on the star formation rate. key words: galaxies: evolution — methods: numerical "
1803.08062," cannibals are dark matter particles with a scattering process that allows three particles to annihilate to two. this exothermic process keeps the gas of the remaining particles warm long after they become non-relativistic. a cannibalizing dark sector which is decoupled from the standard model naturally arises from a pure-glue conﬁning hidden sector. it has an eﬀective ﬁeld theory description with a single massive interacting real scalar ﬁeld, the lightest glueball. since warm dark matter strongly suppresses growth of structure cannibals cannot be all of the dark matter. thus we propose a scenario where most dark matter is non-interacting and cold but about 1 percent is cannibalistic. we review the cannibals’ unusual scaling of the temperature and energy and number densities with redshift and generalize the equations for the growth of matter density perturbations to the case of cannibals. we solve the equations numerically to predict the scaling of the hubble parameter and the characteristic shape of the linear matter power spectrum as a function of model parameters. our results may have implications for the σ8 and h0 problems. 8 1 0 2   r a   m 1 2     ] h p - p e h [     1 v 2 6 0 8 0 . 3 0 8 1 : v i x r a ∗electronic address: buenabad@bu.edu †electronic address: iasraziehm@ust.hk ‡electronic address: schmaltz@bu.edu 1 i. "
1803.08063," the search for habitable exoplanets inspires the question - how do habitable planets form? planet habitability models traditionally focus on abiotic processes and neglect a biotic response to changing conditions on an inhabited planet. the gaia hypothesis postulates that life inﬂuences the earth’s feedback mechanisms to form a self-regulating system, and hence that life can maintain habitable conditions on its host planet. if life has a strong inﬂuence, it will have a role in determining a planet’s habitability over time. we present the exogaia model - a model of simple ‘planets’ host to evolving microbial biospheres. microbes interact with their host planet via consumption and excretion of atmospheric chemicals. model planets orbit a ‘star’ which provides incoming radiation, and atmospheric chemicals have either an albedo, or a heat-trapping property. planetary temperatures can therefore be altered by microbes via their metabolisms. we seed multiple model planets with life while their atmospheres are still forming and ﬁnd that the microbial biospheres are, under suitable conditions, generally able to prevent the host planets from reaching inhospitable temperatures, as would happen on a lifeless planet. we ﬁnd that the underlying geochemistry plays a strong role in determining long-term habitability prospects of a planet. we ﬁnd ﬁve distinct classes of model planets, including clear examples of ‘gaian bottlenecks’ - a phenomenon whereby life either rapidly goes extinct leaving an inhospitable planet, or survives indeﬁnitely maintaining planetary habitability. these results suggest that life might play a crucial role in determining the long-term habitability of planets. ∗ arwen.e.nicholson@gmail.com "
1803.08064,". we study production of self-interacting dark matter (dm) during an early matter- dominated phase. as a benchmark scenario, we consider a model where the dm consists of singlet scalar particles coupled to the visible standard model (sm) sector via the higgs portal. we consider scenarios where the initial dm abundance is set by either the usual thermal freeze-out or an alternative freeze-in mechanism, where dm was never in thermal equilibrium with the sm sector. in both cases, the number density of dm may change considerably compared to the standard radiation-dominated case, having signiﬁcant observational and experimental ramiﬁcations. keywords: dark matter, wimp, fimp, simp, freeze-out, freeze-in, dark matter self- interactions, matter-dominated universe 8 1 0 2   r a   m 1 2     ] h p - p e h [     1 v 4 6 0 8 0 . 3 0 8 1 : v i x r a contents "
1803.08065," in an unconventional presentation of the data on the transverse momentum spectra of baryons produced in heavy-ion collisions, regularities are found that make possible the discovery of a uni- versal formula valid for p, λ, ξ and ω. the formula describes the baryon distributions over wide ranges of pt (0.5 < ∼ pt ∼ 5 gev/c) for 0.06 < < ∼ √sn n < ∼ 3 tev, except for very peripheral collisions. some aspects of their empirical properties are derived in the recombination model, resulting in a revelation of some features of the light and strange quark distributions before hadronization. in- terpretation of the inverse slopes of their exponential behavior leads to an implication that cannot accommodate the conventional description of ﬂuid ﬂow. this is mainly a study of phenomenology without detailed model input. 8 1 0 2   r a   m 1 2     ] x e - l c u n [     1 v 5 6 0 8 0 . 3 0 8 1 : v i x r a 1 i. "
1803.08066," modern machine learning techniques, such as convolutional, recurrent and recursive neural networks, have shown promise for jet substructure at the large hadron collider. for example, they have demonstrated eﬀectiveness at boosted top or w boson identi- ﬁcation or for quark/gluon discrimination. we explore these methods for the purpose of classifying jets according to their electric charge. we ﬁnd that neural networks that incorporate distance within the jet as an input can provide signiﬁcant improvement in jet charge extraction over traditional methods. we ﬁnd that both convolutional and recurrent networks are eﬀective and both train faster than recursive networks. the advantages of using a ﬁxed-size input representation (as with the cnn) or a smaller input representation (as with the rnn) suggest that both convolutional and recurrent networks will be essential to the future of modern machine learning at colliders. 8 1 0 2   r a   m 1 2     ] h p - p e h [     1 v 6 6 0 8 0 . 3 0 8 1 : v i x r a 1 1 "
1803.08068,". we give an algorithm to compute the periods of smooth projective hypersurfaces of any dimension. this is an improvement over existing algorithms which could only compute the periods of plane curves. our algorithm reduces the evaluation of period integrals to an initial value problem for ordinary diﬀerential equations of picard–fuchs type. in this way, the periods can be computed to extreme-precision in order to study their arithmetic properties. the initial conditions are obtained by an exact determination of the cohomology pairing on fermat hypersurfaces with respect to a natural basis. "
1803.08069,"— this paper presents an automated method for creating spatial maps of soil condition with an outdoor mobile robot. effective soil mapping on farms can enhance yields, reduce inputs and help protect the environment. traditionally, data are collected manually at an arbitrary set of locations, then soil maps are constructed ofﬂine using kriging, a form of gaussian process regression. this process is laborious and costly, limiting the quality and resolution of the resulting information. instead, we propose to use an outdoor mobile robot for automatic collection of soil condition data, building soil maps online and also adapting the robot’s exploration strategy on-the-ﬂy based on the current quality of the map. we show how using kriging variance as a reward function for robotic exploration allows for both more efﬁcient data collection and better soil models. this work presents the theoretical foundations for our proposal and an experimental comparison of exploration strategies using soil compaction data from a ﬁeld generated with a mobile robot. "
1803.08070," we study the borel-reducibility of isomorphism relations in the generalized baire space κκ . in the main result we show for inaccessible κ, that if t is a classiﬁable theory and t′ is superstable with s-dop, then the isomorphism of models of t is borel reducible to the isomorphism of models of t′. in fact we show the consistency of the following: if t is a superstable theory with s-dop, then the isomorphism of models of t is σ1 1-complete. acknowledgement this work was made under the supervision of tapani hyttinen. i want to express my gratitude to him for introducing me to the topic, his valuable advices and support during this work. this research was supported by the doctoral programme in mathematics and statistics (domast). "
1803.08071,". many classical computer vision problems, such as essential matrix computation and pose estimation from 3d to 2d correspondences, can be solved by ﬁnding the eigenvector corresponding to the smallest, or zero, eigenvalue of a matrix representing a linear system. incorporat- ing this in deep learning frameworks would allow us to explicitly encode known notions of geometry, instead of having the network implicitly learn them from data. however, performing eigendecomposition within a net- work requires the ability to diﬀerentiate this operation. unfortunately, while theoretically doable, this introduces numerical instability in the optimization process in practice. in this paper, we introduce an eigendecomposition-free approach to train- ing a deep network whose loss depends on the eigenvector corresponding to a zero eigenvalue of a matrix predicted by the network. we demon- strate on several tasks, including keypoint matching and 3d pose esti- mation, that our approach is much more robust than explicit diﬀerentia- tion of the eigendecomposition, it has better convergence properties and yields state-of-the-art results on both tasks. keywords: end-to-end learning, eigendecomposition, singular value de- composition, geometric vision. "
1803.08072,". considering a compact riemann surface of genus greater than two, a higgs bundle is a pair composed of a holomorphic bundle over the riemann surface, joint with an auxiliar vector ﬁeld, so-called higgs ﬁeld. this theory started around thirty years ago, with hitchin’s work, when he reduced the self-duality equations from dimension four to dimension two, and so, studied those equations over riemann surfaces. hitchin baptized those ﬁelds as “higgs ﬁelds” beacuse in the context of physics and gauge theory, they describe similar particles to those described by the higgs bosson. later, simpson used the name “higgs bundle” for a holomorphic bundle together with a higgs ﬁeld. today, higgs bundles are the subject of research in several areas such as non-abelian hodge theory, langlands, mirror symmetry, integrable systems, quantum ﬁeld theory (qft), among others. the main purposes here are to introduce these objects, and to present a brief construction of the moduli space of higgs bundles. keywords: hodge bundles, moduli of higgs bundles, stratiﬁcations, variations of hodge structures, vector bundles. ams 2010 msc classes: primary 14h60; secondaries 14d07, 55q52. 1 "
1803.08073," automatic interpretation of the relation between the constituents of a noun com- pound, e.g. olive oil (source) and baby oil (purpose) is an important task for many nlp applications. recent approaches are typically based on either noun-compound representations or paraphrases. while the former has initially shown promising re- sults, recent work suggests that the suc- cess stems from memorizing single pro- totypical words for each relation. we ex- plore a neural paraphrasing approach that demonstrates superior performance when such memorization is not possible. "
1803.08075," dark matter, one of the important portion of the universe, could aﬀect the visible matter in neutron stars. an important physical feature of dark matter is due to the spin of dark matter particles. here, applying the piecewise polytropic equation of state for the neutron star matter and the equation of state of spin polarized self-interacting dark matter, we investigate the structure of neutron stars which are inﬂuenced by the spin polarized self-interacting dark matter. the behavior of the neutron star matter and dark matter portions for the stars with diﬀerent values of the interaction between dark matter particles and spin polarization of dark matter is considered. in addition, we present the value of the gravitational redshift of these stars in diﬀerent cases of spin polarized and self-interacting dark matter. pacs: 97.60.jd 95.35.+d key words: neutron stars; dark matter; spin polarization 1 "
1803.08076," as multi-agent networks grow in size and scale, they become increasingly difﬁcult to synchronize, though agents must work together even when generating and sharing different information at different times. targeting such cases, this paper presents an asynchronous optimization framework in which the time between successive communications and computations is unknown and unspeciﬁed for each agent. agents’ updates are carried out in blocks, with each agent updating only a small subset of all decision variables. each agent uses an independently chosen tikhonov regularization, and convergence is measured with respect to a weighted block-maximum norm in which agents’ blocks can be measured in different p-norms and weighted differently to normalize problems. asymptotic convergence is shown and convergence rates are derived explicitly in terms of a problem’s parameters, with only mild restrictions on what they can be. simulation results are included to verify the theoretical developments made. "
1803.08079,". eﬃcient representations of convex sets are of crucial importance for many algorithms that work with them. it is well-known that sometimes, a complicated convex set can be expressed as the projection of a much simpler set in higher dimensions called a lift of the original set. this is a brief survey of recent developments in the topic of lifts of convex sets. our focus will be on lifts that arise from aﬃne slices of real positive semideﬁnite cones known as psd or spectrahedral lifts. the main result is that projection representations of a convex set are controlled by factorizations, through closed convex cones, of an operator that comes from the convex set. this leads to several research direc- tions and results that lie at the intersection of convex geometry, combinatorics, real algebraic geometry, optimization, computer science and more. "
1803.08080,": we explore the phenomenology of a qcd-like dark sector which conﬁnes around the gev scale. the dark sector inherits a ﬂavour structure from a coupling between dark quarks and sm quarks via a heavy mediator, which leads to exciting new phenomena. while stable baryonic bound states are the dark matter candidates, the phenomenology is dominated by the lightest composite mesons, the dark pions, which can have decay lengths ranging from millimetres to hundreds of meters. for masses below 1.5 gev, their exclusive decays to sm mesons are calculated for the ﬁrst time by matching both dark and visible sectors to a chiral lagrangian. constraints from big bang nucleosynthesis, dark matter direct detection and ﬂavour single out a small region of allowed parameter space for dark pion masses below 5 gev. it is best probed by the ﬁxed target experiments na62 and ship, where dark pions can be produced copiously in rare decays like b → kπd. the dominant πd → k±π∓ and πd → 3π decay modes are a smoking gun for a cp-odd, ﬂavour violating new resonance. heavier dark pions are best searched for at the lhc, where they decay after hadronisation to produce jets which emerge into sm states within the detector. here the ﬂavour structure ensures diﬀerent ﬂavours emerge on diﬀerent length scales, leading to a striking new feature in the emerging jets signature. 8 1 0 2   r p a 6       ] h p - p e h [     2 v 0 8 0 8 0 . 3 0 8 1 : v i x r a contents "
1803.08081," based on a simple object, an i.i.d. sequence of positive integer- valued random variables, {an}n∈z, we introduce and study two ran- dom structures and their connections. first, a population dynamics, in which each individual is born at time n and dies at time n + an. this dynamics is that of a d/gi/∞ queue, with arrivals at integer times and service times given by {an}n∈z. second, the directed ran- dom graph t f on z generated by the random map f (n) = n + an. only assuming e[a0] < ∞ and p[a0 = 1] > 0, we show that, in steady state, the population dynamics is regenerative, with one individual alive at each regenerative epochs. we identify a unimodular structure in this dynamics. more precisely, t f is a unimodular directed tree, in which f (n) is the parent of n. this tree has a unique bi-inﬁnite path. moreover, t f splits the integers into two categories: ephemeral inte- gers, with a ﬁnite number of descendants of all degrees, and successful integers, with an inﬁnite number. each regenerative epoch is a suc- cessful individual such that all integers less than it are its descendants of some order. ephemeral, successful, and regenerative integers form stationary and mixing point processes on z. key words: population dynamics, queuing system, unimodular random graph, eternal family tree, branching process. msc 2010 subject classiﬁcation: primary: 92d25, 60k25, 05c80. ∗baccelli@math.utexas.edu †asodre@math.utexas.edu 1 figure 1: the population process. here, a−6 = 10, a−5 = 3, a−4 = 8, a−3 = 3, a−2 = 3, and a−1 = 6. assuming a−i < i for all i > 6, there are four edges crossing 0. individuals -6,-4,-2, and -1 are still alive at time 0. hence, n0 = 5, as it includes the individual born at 0. "
1803.08082,". we derive the 3d energy critical quintic nls from quantum many-body dy- namics with 3-body interaction in the t3 (periodic) setting. due to the known complexity of the energy critical setting, previous progress was limited in comparison to the 2-body interaction case yielding energy subcritical cubic nls. previously, the only result for the 3d energy critical case was [42], which proved the uniqueness part of the argument in the case of small solutions. in the main part of this paper, we develop methods to prove the convergence of the bbgky hierarchy to the inﬁnite gross-pitaevskii (gp) hierarchy, and separately, the uniqueness of large gp solutions. since the trace estimate used in the previous proofs of convergence is the false sharp trace estimate in our setting, we instead introduce a new frequency interaction analysis and apply the ﬁnite dimensional quantum de finetti theorem. for the large solution uniqueness argument, we discover the new hufl (hierarchical uni- form frequency localization) property for the gp hierarchy and use it to prove a new type of uniqueness theorem. the hufl property reduces to a new statement even for nls. with the help of [24, 43] which proved the global well-posedness for the quintic nls, this new uniqueness theorem establishes global uniqueness. 8 1 0 2   r a   m 1 2     ] p a h t a m . [     1 v 2 8 0 8 0 . 3 0 8 1 : v i x r a contents "
1803.08084," we examine some inﬂationary models based on modiﬁ- cations of gravity in the light of planck 2015 data, such as the generalised chaplygin inspired inﬂation, models based in n = 1 supergravity and braneworld scenar- ios. we also show that, conversely, potentials with a very ﬂat plateau yield a primordial spectrum similar to that of the starobinsky model with no need to modify general relativity. "
1803.08087,". let ℓ be a commutative ring with unit. to every pair of ℓ-algebras a and b one can associate a simplicial set hom(a, b∆) so that π0hom(a, b∆) equals the set of polynomial homotopy classes of morphisms from a to b. we prove that πnhom(a, b∆) is the set of homotopy classes of morphisms from a to bsn is the ind-algebra of polynomials on the n-dimensional cube with coeﬃcients in b vanishing at the boundary of the cube. this is a generalization to arbitrary dimensions of a theorem of corti˜nas- thom, which addresses the cases n ≤ 1. as an application we give a simpliﬁed proof of a theorem of garkusha that computes the homotopy groups of his matrix-unstable algebraic kk-theory space in terms of polynomial homotopy classes of morphisms. • , where bsn • "
1803.08088," we study holographic conformal anomalies and the corresponding a-theorem for einstein gravity extended with horndeski terms that involve up to and including linear curvature tensors. we focus on our discussion in d = 5 bulk dimensions. for the generic horndeski coupling, the a-charge is the same as that in einstein gravity, but the inclusion of the horn- deski term violates the a-theorem. however, there exists a critical point of the horndeski coupling, for which the theory admits nearly ads spacetimes with non-vanishing horndeski scalar. the full ads isometry is broken down by the logarithmic scalar hair to the poincar´e group plus the scale invariance. we ﬁnd that in this case the a-charge depends on the ads radius ℓ and the integration constant χs of the horndeski scalar. in addition, we ﬁnd that two new central charges emerge, that are absent in gravities with minimally-coupled matter. we call them b-charges. these b-charges also depend on ℓ and χs. we construct an a-function for ﬁxed ℓ but with the running horndeski scalar χ replacing the constant χs, and establish the holographic a-theorem using the null energy condition in the bulk. furthermore, we ﬁnd that there exist analogous monotonous b-functions as well. we also obtain the a-charge and the a-theorem in general odd bulk dimensions. 8 1 0 2   y a m 1 2       ] h t - p e h [     3 v 8 8 0 8 0 . 3 0 8 1 : v i x r a †liyuezhou@tju.edu.cn ∗mrhonglu@gmail.com contents "
1803.08089," in learning-to-learn the goal is to infer a learning algorithm that works well on a class of tasks sampled from an unknown meta distribution. in contrast to previous work on batch learning-to- learn, we consider a scenario where tasks are presented sequentially and the algorithm needs to adapt incrementally to improve its performance on future tasks. key to this setting is for the algorithm to rapidly incorporate new observations into the model as they arrive, without keeping them in memory. we focus on the case where the underlying algorithm is ridge regression parameterized by a positive semideﬁnite matrix. we propose to learn this matrix by applying a stochastic strategy to minimize the empirical error incurred by ridge regression on future tasks sampled from the meta distribution. we study the statistical properties of the proposed algorithm and prove non-asymptotic bounds on its excess transfer risk, that is, the generalization performance on new tasks from the same meta distribution. we compare our online learning-to-learn approach with a state of the art batch method, both theoretically and empirically. "
1803.08091,". the direct product n × n of two free monogenic semigroups contains uncountably many pairwise non-isomorphic subdirect products. furthermore, the fol- lowing hold for n × s, where s is a ﬁnite semigroup. it contains only countably many pairwise non-isomorphic subsemigroups if and only if s is a union of groups. and it contains only countably many pairwise non-isomorphic subdirect products if and only if every element of s has a relative left- or right identity element. "
1803.08092,"— hybrid dynamical systems have proven to be a powerful modeling ion, yet fundamental questions regarding their dynamical properties remain. in this paper, we develop a novel solution concept for a class of hybrid systems, which is a generalization of filippov’s solution concept. in the mathematical theory, these hybrid filippov solutions eliminate the notion of zeno executions. building on previous techniques for relaxing hybrid systems, we then introduce a family of smooth control systems that are used to approximate this solution concept. the trajectories of these relaxations vary differentiably with respect to initial conditions and inputs, may be numerically approximated using existing techniques, and are shown to converge to the hybrid filippov solution in the limit. finally, we outline how the results of this paper provide a foundation for future work to control hybrid systems using well-established techniques from control theory. "
1803.08093,". we develop a theory of grassmann triples via hasse-schmidt derivations, which formally generalizes results such as the cayley-hamilton theorem in linear algebra, thereby providing a uniﬁed approach to classical linear algebra and tropical algebra. y a m 4 1     ]   . a r h t a m [     2 v 3 9 0 8 0 . 3 0 8 1 : v i x r a "
1803.08094,". an action should remain identiﬁable when modifying its speed: consider the contrast between an expert chef and a novice chef each chopping an onion. here, we expect the novice chef to have a rela- tively measured and slow approach to chopping when compared to the expert. in general, the speed at which actions are performed, whether slower or faster than average, should not dictate how they are recog- nized. we explore the erratic behavior caused by this phenomena on state-of-the-art deep network-based methods for action recognition in terms of maximum performance and stability in recognition accuracy across a range of input video speeds. by observing the trends in these metrics and summarizing them based on expected temporal behaviour w.r.t. variations in input video speeds, we ﬁnd two distinct types of net- work architectures. in this paper, we propose a preprocessing method named t-recs, as a way to extend deep-network-based methods for ac- tion recognition to explicitly account for speed variability in the data. we do so by adaptively resampling the inputs to a given model. t-recs is agnostic to the speciﬁc deep-network model; we apply it to four state- of-the-art action recognition architectures, c3d, i3d, tsn, and con- vnet+lstm. on hmdb51 and ucf101, t-recs-based i3d models show a peak improvement of at least 2.9% in performance over the base- line while t-recs-based c3d models achieve a maximum improvement in stability by 59% over the baseline, on the hmdb51 dataset. keywords: preprocessing, temporal modeling, action recognition "
1803.08095,". let b be an inﬁnite subset of n. when we consider parti- tions of natural numbers into elements of b, a partition number without a restriction of the number of equal parts can be expressed by partition numbers with a restriction α of the number of equal parts. although there are many way of the expression, we prove that there exists a ex- pression form such that this expression form is true for all possible set b. this identities comes from the partition numbers of natural numbers into {1, α, α2, α3, · · · }. furthermore, we prove that there exist inverse forms of the expression forms. and we prove other similar identities. the proofs in this paper are constructive. let p(n) be the number of partitions of n into natural numbers, and d(n) be the number of partitions of n into distinct natural numbers. if one calculates p(5) and from d(1) to d(5) except d(4), p(5) = 7, d(1) = 1, d(2) = 1, d(3) = 2, d(5) = 3. however, p(5) can de expressed by the following product and sum of d(1), d(2), d(3), d(5). p(5) = 7 = 3 + 1 × 1 + 1 × 1 + 2 × 1 = d(5) + d(1)d(1) + d(2)d(1) + d(3)d(1). next, let pp(n) be the number of partitions of n into primes, and pp number of partitions of n into distinct primes. then 1 (n) be the pp(5) = 2, pp 1 (1) = 0, pp 1 (2) = 1, pp 1 (3) = 1, pp 1 (5) = 2 and pp(5) can de expressed by the following product and sum of pp pp 1 (3), pp 1 (1), pp 1 (2), 1 (5). pp(5) = 2 = 2 + 0 × 0 + 1 × 0 + 1 × 0 1 (1) + pp 1 (5) + pp 1 (1)pp = pp 1 (2)pp 1 (1) + pp 1 (3)pp 1 (1). finally, let p o(n) be the number of partitions of n into odd numbers, and po 1 (n) be the number of partitions of n into distinct odd numbers. then po(5) = 3, po 1 (1) = 1, po 1 (2) = 0, po 1 (3) = 1, po 1 (5) = 1 1the theorems in this paper stated and proved in 2012 winter. 2010 mathematics subject classiﬁcation. 11p81, 11p84, 05a17 key words and phrases. partition number identities, change of set of parts. 1 8 1 0 2   r a   m 9 1     ] . o c h t a m [     1 v 5 9 0 8 0 . 3 0 8 1 : v i x r a 2 and kim, bongju po(5) = 3 = 1 + 1 × 1 + 0 × 1 + 1 × 1 1 (1) + po 1 (5) + po 1 (1)po = po 1 (2)po 1 (1) + po 1 (3)po 1 (1). if one compares above three expressions, one can see that these three expres- sion forms are same though the set of parts was changed. the numbers in the previous identity are not random but come from the all possible binary expressions of 5 i.e. 5 = 5 × 20 = 1 × 20 + 1 × 22 = 1 × 20 + 2 × 21 = 3 × 20 + 1 × 21. deﬁnition. let n be the set of natural numbers and ψ be an one to one function such that ψ : n → n. we denote the set {ψ(n) ∈ n} by a. pa α (n) is the number of partitions of n into elements of a such that the number of equal parts is less than or equals to α ∈ n \ {0}. pa(n) is the number of partitions of n into elements of ψ(n) without a restriction of the number of equal parts. we deﬁne pa α (0) = pa(0) := 1 for all α ∈ n \ {0}. 1. identities between pa and pa α before considering the identities between pa and pa α , we prove the identities between pa and pa 1 . deﬁnition. consider all non-negative integer solutions of the indeterminate equation n = n0+2n1+4n2+· · · = pi≥0 2ini and denote by (an 13 · · · ), 21, an (an 23 · · · ), · · · . in other word, 22, an 12, an 11, an 11 + 2an 21 + 2an 12 + 4an 22 + 4an 13 + · · · 23 + · · · n = an = an · · · then the solution matrix of this equation is proposition 1.1. let an = (an let ψ be an one to one function such that ψ : n → n. then ij). an := (an ij) be the solution matrix of n = pi≥0 2ini and pa(n) = x i≥1 y j≥1 1 (an pa ij ) for all n ∈ n. partition number identities which are true for all set of parts 3 proof. it is well known fact that for |q| < 1, x n≥0 pa(n)qn = y n≥1 1 1 − qψ(n) and x n≥0 1 (n)qn = y pa n≥1 (1 + qψ(n)) (see [1] or [2]). on the other hand, y (1 − q2ψ(n)) = y (1 − qψ(n)) y (1 + qψ(n)) n≥1 y n≥1 (1 − q4ψ(n)) = y (1 − qψ(n)) y (1 + qψ(n)) y (1 + q2ψ(n)) n≥1 n≥1 n≥1 n≥1 n≥1 ... y n≥1 (1 − q2i+1ψ(n)) = y (1 − qψ(n)) n≥1 i y i=0 (1 + q2iψ(n)) y n≥1 for all i ∈ n. so, 1 = lim i→∞ y n≥1 (1 − q2i+1ψ(n)) (1 − qψ(n)) = y n≥1 ∞ y i=0 (1 + q2iψ(n)). y n≥1 therefore, and y n≥1 1 1 − qψ(n) = y y i≥0 n≥1 (1 + q2iψ(n)) x n≥0 pa(n)qn = y (x i≥0 n≥0 1 (n)q2in). pa if one expands the above inﬁnite product to an inﬁnite series, one can express pa(n) by pa 1 (1) and one can see that the coeﬃcient of qn is related with non-negative integer solutions of n = pi≥0 2ini. if one expands 1 (n − 1), · · · , pa 1 (n), pa 4 kim, bongju some terms, 1 (n)q2in) pa y (x n≥0 i≥0 = 1 + pa + [pa + [pa + [pa · · · 1 (1)]q2 1 (1)q + [pa 1 (1)pa 1 (2)pa 1 (1)pa 1 (3) + pa 1 (4) + pa 1 (5) + pa 1 (2) + pa 1 (1)]q3 1 (1) + pa 1 (2) + pa 1 (2) + pa 1 (1)pa 1 (1)]q4 1 (1) + pa 1 (3)pa 1 (1)]q5 now, we prove more general identities between pa and pa α . (cid:3) deﬁnition. let n ∈ n and {(an,α 23 · · · ), · · · } be the set of all non-negative integer solutions of the indeterminate equation n = n0 + (α + 1)n1 + (α + 1)2n2 + · · · = pi≥0(α + 1)ini. in other words, 13 · · · ), (an,α 11 , an,α 21 , an,α 22 , an,α 12 , an,α 11 + (α + 1)an,α 21 + (α + 1)an,α 12 + (α + 1)2an,α 22 + (α + 1)2an,α 13 + · · · 23 + · · · n = an,α = an,α · · · where an,α ij ∈ n. then the solution matrix of this equation is an,α := (an,α ij ). theorem 1.2. let ψ be an one to one function such that ψ : n → n. and let n ∈ n and an,α = (an,α ij ) be the solution matrix of n = pi≥0(α + 1)ini. then pa(n) = x i≥1 y j≥1 α (an,α pa ij ) for all α ∈ n \ {0}. proof. it is well known fact that for |q| < 1, x n≥0 α (n)qn = y pa n≥1 1 − q(α+1)ψ(n) 1 − qψ(n) (1 + qψ(n) + q2ψ(n) + · · · + qαψ(n)) = y n≥1 (see [2]). on the other hand, partition number identities which are true for all set of parts 5 y n≥1 y n≥1 (1 − q(α+1)ψ(n)) = y (1 − qψ(n)) y (1 + qψ(n) + q2ψ(n) + · · · + qαψ(n)) (1 − q(α+1)2ψ(n)) = y (1 − qψ(n)) y (1 + qψ(n) + q2ψ(n) + · · · + qαψ(n)) n≥1 n≥1 n≥1 n≥1 (1 + q(α+1)ψ(n) + q(α+1)2ψ(n) + · · · + q(α+1)αψ(n)) × y n≥1 ... y n≥1 (1 − q(α+1)i+1ψ(n)) = y (1 − qψ(n)) n≥1 × i y i=0 for all i ∈ n. so, (1 + q(α+1)iψ(n) + · · · + q(α+1)iαψ(n)) y n≥1 1 = lim i→∞ y n≥1 (1 − q(α+1)i+1ψ(n)) (1 − qψ(n)) = y n≥1 ∞ y i=0 (1 + q(α+1)iψ(n) + · · · + q(α+1)iαψ(n)). y n≥1 therefore, y n≥1 and 1 1 − qψ(n) = y y i≥0 n≥1 (1 + q(α+1)iψ(n) + · · · + q(α+1)iαψ(n)) x n≥0 pa(n)qn = y (x i≥0 n≥0 α (n)q(α+1)in). pa if one expands the above inﬁnite product to an inﬁnite series, one can express pa(n) by pa α (1) and one can see that the coeﬃcient of qn is related with non-negative integer solutions of n = pi≥0(α + 1)ini (cid:3) α (n − 1), · · · , pa α (n), pa 2. inverse identities and some other similar identities in section 1, we found the identities which express pa(n) by pa α (n), pa α (n−1), · · · , pa α (1). now, we will ﬁnd the inverse identities. deﬁnition. let ea(n) be the number of even partitions of n without a restric- tion of the number of equal parts and oa(n) be the number of odd partitions of n without a restriction of the number of equal parts. then we deﬁne and ¯pa(0) := 1. ¯pa(n) := ea(n) − oa(n) 6 kim, bongju deﬁnition. let n = (α+1)pi≥0 2ini be the indeterminate equation for n, α ∈ n\{0}. if this equation has solutions (bn,α where bn,α 22 , bn,α ij ∈ n, then we deﬁne the solution matrix of this equation by 13 , · · · ), (bn,α 12 , bn,α 11 , bn,α 21 , bn,α 23 , · · · ), · · · bn,α := (bn,α ij ). deﬁnition. let n, α ∈ n \ {0}. if n = (α + 1)pi≥0 2ini has non-negative integer solutions, we deﬁne α(n) := x γψ i≥1 y j≥1 ¯pa(bn,α ij ) and if n = (α + 1)pi≥0 2ini does not have a non-negative integer solution, we deﬁne γψ α(n) := 0. for n = 0, we deﬁne γψ α(0) := 1. theorem 2.1. let ψ be an one to one function such that ψ : n → n. then pa α (n) = n x i=0 pa(n − i)γψ α(i) for all n, α ∈ n. proof. for |q| < 1, (1 − q2(α+1)ψ(n)) = y (1 − q(α+1)ψ(n)) y (1 + q(α+1)ψ(n)) (1 − q4(α+1)ψ(n)) = y (1 − q(α+1)ψ(n)) y n≥1 y n≥1 × y (1 + q(α+1)ψ(n)) y (1 + q2(α+1)ψ(n)) n≥1 n≥1 (1 − q2i+1(α+1)ψ(n)) = y (1 − q(α+1)ψ(n)) y n≥1 i y i=0 (1 + q2i(α+1)ψ(n)) y n≥1 for all i ∈ n. so, 1 = lim i→∞ y n≥1 (1 − q2i+1(α+1)ψ(n)) (1 − q(α+1)ψ(n)) = y n≥1 ∞ y i=0 (1 + q2i(α+1)ψ(n)) y n≥1 and y n≥1 1 − q(α+1)ψ(n) 1 − qψ(n) = y n≥1 1 1 − qψ(n) y i≥0 1 1 + q2i(α+1)ψ(n) . y n≥1 n≥1 n≥1 n≥1 n≥1 ... partition number identities which are true for all set of parts 7 on the other hand, y n≥1 1 1 + q2i(α+1)ψ(n) ¯pa(n)q2i(α+1)n. = x n≥0 therefore, if we deﬁne y i≥0 y n≥1 then 1 1 + q2i(α+1)ψ(n) := x γψ α(n)qn, n≥0 α(n) = x γψ i≥1 y j≥1 ¯pa(bn,α ij ) when n = (α + 1)pi≥0 2ini has non-negative integer solutions and γψ when n = (α + 1)pi≥0 2ini does not have a non-negative integer solution. finally, α(n) = 0 x n≥0 α (n)qn = y pa n≥1 1 y n≥1 1 − qψ(n) y pa(n)qn)(x i≥0 n≥0 = (x n≥0 γψ α(n)qn) 1 1 + q2i(α+1)ψ(n) and pa α (n) = n x i=0 pa(n − i)γψ α(i). next, we prove two similar theorems. deﬁnition. let ea α (n) be the number of even partitions of n such that the number of equal parts is less than or equals to α ∈ n and oa α (n) be the number of odd partitions of n such that the number of equal parts is less than or equals to α ∈ n \ {0}. then we deﬁne (cid:3) ¯pa α (n) := ea α (n) − oa α (n) and ¯pa α (0) := 1. theorem 2.2. let an = (an let ψ be an one to one function such that ψ : n → n. then ij ) be the solution matrix of n = pi≥0 2ini and 1 (n) = x ¯pa i≥1 y j≥1 ¯pa(an ij ) for all n ∈ n. proof. we proved that for |q| < 1, 1 = y n≥1 (1 − qψ(n)) ∞ y i=0 (1 + q2iψ(n)) y n≥1 8 kim, bongju in the proof of theorem 2.1. so, (1 − qψ(n)) = y n≥1 ∞ y i=0 y n≥1 1 (1 + q2iψ(n)) and x n≥0 1 (n)qn = y ¯pa (x i≥0 n≥0 ¯pa(n)q2in). this proves the theorem. (cid:3) theorem 2.3. let an,α = (an,α and let ψ be an one to one function such that ψ : n → n. then ij ) be the solution matrix of n = pi≥0(α+1)ini ¯pa(n) = x i≥1 y j≥1 α (an,α ¯pa ij ) for all n ∈ n and for all even natural number α. proof. let α be an even natural number. if |q| < 1, (1 + q(α+1)i+1ψ(n)) y n≥1 (1 + qψ(n)) = y n≥1 i y i=0 (1 − q(α+1)iψ(n) + q(α+1)i2ψ(n) − · · · + q(α+1)iαψ(n)) y n≥1 for all i ∈ n. so, 1 = lim i→∞ y n≥1 (1 + q(α+1)i+1ψ(n)) (1 + qψ(n)) = y n≥1 ∞ y i=0 (1 − q(α+1)iψ(n) + q(α+1)i2ψ(n) − · · · + q(α+1)iαψ(n)). y n≥1 therefore, 1 (1 + qψ(n)) = ∞ y i=0 y n≥1 and (1 − q(α+1)iψ(n) + q(α+1)i2ψ(n) − · · · + q(α+1)iαψ(n)) y n≥1 x n≥0 ¯pa(n) = y i≥0 (x n≥0 α (n)q(α+1)in). ¯pa this proves the theorem. (cid:3) partition number identities which are true for all set of parts 9 3. appendix: examples in this appendix, we will consider two identities for pa(10) and pa i) let us consider the expression of pa(10) by pa 1 (10). 1 s. the indeterminate equation for this identity is the solution matrix of this equation and the identity for pa(10) are 10 = n0 + 2n1 + 4n2 + · · · .   1 0 1 0 3 0 5 0 2 2 4 2 0 2 0 2 1 4 3 4 2 6 0 6 8 1 10 0 0 1 2 0 1 0 0 0 1 0 0 0 0 1 2 0 1 0 0 0 0 0 1 0 0 0 0 0 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·   , pa(10) = pa +pa +pa +pa +pa +pa 1 (1)pa 1 (5) + pa 1 (2)pa 1 (2)pa 1 (4)pa 1 (8)pa 1 (1) + pa 1 (2)pa 1 (4) + pa 1 (2) + pa 1 (3) + pa 1 (1) + pa 1 (1)pa 1 (2)pa 1 (2)pa 1 (4)pa 1 (6)pa 1 (10). 1 (2) + pa 1 (1) 1 (1) 1 (1)pa 1 (1) 1 (2) + pa 1 (1)pa 1 (3) 1 (6)pa 1 (1) now, we calculate pa(10) for three set of parts. 1) ψ(n) = {p | p is a prime} let us calculate partition numbers. 10 = 5 + 5 3 + 2 + 5 3 + 3 + 2 + 2 7 + 3 2 + 2 + 2 + 2 + 2 , pa 1 (1) = 0 pa 1 (2) = 1 pa pa 1 (3) = 1 pa pa pa 1 (4) = 0 pa 1 (5) = 2 1 (6) = 0 1 (8) = 1 1 (10) = 2. so, pa(10) = 5 and since pa pa(10) = pa 1 (5) + pa 1 (1) = pa 1 (2)pa 1 (4) = pa 1 (2) + pa 1 (6) = 0, 1 (10) = 2 + 1 + 2 = 5. 2) ψ(n) = {n2 | n ∈ n}. let us calculate the partition numbers. 10 = 1 + 9 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 4 + 1 + 1 + 1 + 1 + 1 + 1 4 + 4 + 1 + 1 , so, pa(10) = 4 and since pa pa(10) = pa 1 (2) = pa 1 (1) + pa = 1 + 1 + 1 + 1 = 4. 1 (1)pa 1 (3) = pa 1 (5) + pa 1 (6) = pa 1 (4)pa 1 (1)pa pa 1 (1) = 1 pa 1 (2) = 0 pa pa 1 (3) = 0 pa pa 1 (4) = 1 pa pa 1 (8) = 0, 1 (5) = 1 1 (6) = 0 1 (8) = 0 1 (10) = 1. 1 (1) + pa 1 (10) 10 kim, bongju 3) ψ(n) = {n | n is an odd number}. let us calculate the partition numbers. 10 = 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 3 + 1 + 1 + 1 + 1 + 1 + 1 + 1 5 + 1 + 1 + 1 + 1 + 1 7 + 1 + 1 + 1 9 + 1 3 + 3 + 1 + 1 + 1 + 1 3 + 3 + 3 + 1 5 + 5 7 + 3 3 + 5 + 1 + 1 so, pa(10) = 10 and since pa 1 (2) = 0, , pa 1 (1) = 1 pa pa 1 (2) = 0 pa pa 1 (3) = 1 pa pa 1 (4) = 1 pa 1 (5) = 1 1 (6) = 1 1 (8) = 2 1 (10) = 2. pa(10) = pa 1 (3) + pa 1 (4)pa + pa 1 (3) + pa 1 (1) + pa = 1 + 1 + 1 + 1 + 1 + 1 + 2 + 2 1 (1) + pa 1 (1)pa 1 (1) + pa 1 (1)pa 1 (1) + pa 1 (8)pa 1 (1)pa 1 (4)pa 1 (6)pa 1 (5) 1 (10) = 10. ii) let us consider the identity for pa 1 (10). the indeterminate equation for this identity is and solutions of this equation are n = 2n0 + 4n1 + 8n2 + · · · solutions n (1, 0, 0, · · · ) 2 (2, 0, 0, · · · ), (0, 1, 0, · · · ) 4 (3, 0, 0, · · · ), (1, 1, 0, · · · ) 6 (4, 0, 0, · · · ), (2, 1, 0, · · · ), (0, 0, 1, · · · ), (0, 2, 0, · · · ) 8 10 (5, 0, 0, · · · ), (1, 2, 0, · · · ), (1, 0, 1, · · · ), (3, 1, 0, · · · ) and ∅ for n = odd number. so, n 0 2 4 6 8 10 1 (n) γψ 1 ¯pa(1) ¯pa(2) + ¯pa(1) ¯pa(3) + ¯pa(1)¯pa(1) ¯pa(4) + ¯pa(2)¯pa(1) + ¯pa(2) + ¯pa(1) ¯pa(5) + ¯pa(2)¯pa(1) + ¯pa(1)¯pa(1) + ¯pa(3)¯pa(1) partition number identities which are true for all set of parts 11 and γψ therefore, if we denote pa(n) by pa(n) and ¯pa(n) by ¯pa(n), 1 (n) = 0 for n = odd number. pa 1 (10) = pa(10) + pa(8)¯pa(1) + pa(6)[¯pa(2) + ¯pa(1)] +pa(4)[¯pa(3) + ¯pa(1)¯pa(1)] +pa(2)[¯pa(4) + ¯pa(2)¯pa(1) + ¯pa(2) + ¯pa(1)] +¯pa(5) + ¯pa(2)¯pa(1) + ¯pa(1)¯pa(1) + ¯pa(3)¯pa(1). 1 (10) = 2 and now, we claulate for two set of parts {p | p is a prime} and {n2 | n ∈ n}. 1) for {p | p is a prime}, pa pa(2) = 1 pa(4) = 1 pa(6) = 2 pa(8) = 3 pa(10) = 5 if we calculate pa ¯pa(1) = 0 ¯pa(2) = −1 ¯pa(3) = −1 ¯pa(4) = 1 ¯pa(5) = 0. 1 (10), pa 1 (10) = 2 = 5 + 3 × 0 + 2 × [(−1) + 0] + 1 × [(−1) + 0 × 0] + 1 × [1 + (−1) × 0 + (−1) + 0] + 0 + 0 × (−1) + 0 × 0 + 0 × (−1). 1 (10) = 1 and 2) for {n2 | n ∈ n}, pa ¯pa(1) = −1 ¯pa(2) = 1 ¯pa(3) = −1 ¯pa(4) = 0 ¯pa(5) = 0. pa(2) = 1 pa(4) = 2 pa(6) = 2 pa(8) = 3 pa(10) = 4 if we calculate pa 1 (10), pa 1 (10) = 1 = 4 + 3 × (−1) + 2 × [1 + (−1)] + 2 × [(−1) + (−1) × (−1)] + 1 × [0 + 1 × (−1) + 1 + (−1)] + 0 + (−1) × 1 + (−1) × (−1) + (−1) × (−1). references "
1803.08096, we extend the caﬀarelli–kohn–nirenberg type partial regularity theory for the steady 5- dimensional fractional navier–stokes equations with external force to the hyperdissipative set- ting. in our argument we use the methods of colombo–de lellis–massaccesi to apply a blowup procedure adapted from work of ladyzhenskaya–seregin. 1 
1803.08097," safety-critical software systems are those whose failure or malfunc- tion could result in casualty and/or serious financial loss. in such systems, safety assurance cases (sacs) are an emerging approach that adopts a proactive strategy to produce structuralized safety jus- tifications and arguments. while sacs are recommended in many software-intensive safety-critical domains, the lack of knowledge regarding the practitioners’ perspectives on using sacs hinders effective adoption of this approach. to gain such knowledge, we interviewed nine practitioners and safety experts who focused on safety-critical software systems. in general, our participants found the sac approach beneficial for communication of safety arguments and management of safety issues in a multidisciplinary setting. the challenges they faced when using sacs were primarily associated with (1) a lack of tool support, (2) insufficient process integration, and (3) scarcity of experienced personnel. to overcome those chal- lenges, our participants suggested tactics that focused on creating direct safety arguments. process and organizational adjustments are also needed to streamline sac analysis and creation. finally, our participants emphasized the importance of knowledge sharing about sacs across software-intensive safety-critical domains. ccs concepts • general and reference → empirical studies; • software and its engineering → software safety; keywords safety-critical systems, safety assurance case, practitioner perspec- tives, empirical study. acm reference format: jinghui cheng, micayla goodrum, ronald metoyer, and jane cleland-huang. 2018. how do practitioners perceive assurance cases in safety-critical software systems?. in chase’18: ieee/acm 11th international workshop on cooperative and human aspects of software, may 27, 2018, gothenburg, permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. ing with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. chase’18, may 27, 2018, gothenburg, sweden © 2018 copyright held by the owner/author(s). publication rights licensed to the association for computing machinery. acm isbn 978-1-4503-5725-8/18/05...$15.00 https://doi.org/10.1145/3195836.3195838 sweden. acm, new york, ny, usa, 4 pages. https://doi.org/10.1145/3195836. 3195838 "
1803.08099," we have studied the p → γ(cid:63)γ(cid:63) transition form factors (p = π0, η, η(cid:48)) within a chiral invariant framework that allows us to relate the three form-factors and evaluate the corresponding contributions to the muon anomalous magnetic moment aµ = (gµ − 2)/2, through pseudoscalar pole contributions. we use a chiral invariant lagrangian to describe the inter- actions between the pseudo-goldstones from the spontaneous chiral sym- metry breaking and the massive meson resonances. we will consider just the lightest vector and pseudoscalar resonance multiplets. photon inter- actions and u (3) ﬂavor breaking eﬀects are accounted for in this covariant framework. this article studies the most general corrections of order m2 p within this setting. requiring short-distance constraints ﬁxes most of the parameters entering the form-factors, consistent with previous determi- nations. the remaining ones are obtained from a ﬁt of these form-factors to experimental measurements in the space-like (q2 ≤ 0) region of photon momenta. no time-like observable is included in our ﬁts. the combination of data, chiral symmetry relations between form-factors and high-energy constraints allows us to determine with improved precision the on-shell p - pole contribution to the hadronic light-by-light scattering of the muon = (8.47 ± 0.16) · 10−10 anomalous magnetic moment: we obtain ap,hlbl for our best ﬁt. this result was obtained excluding babar π0 data, which our analysis ﬁnds in conﬂict with the remaining experimental inputs. this study also allows us to determine the parameters describing the η−η(cid:48) sys- tem in the two-mixing angle scheme and their correlations. µ 1 contents "
1803.08100,"— uncertainty in modeling real world physics makes transferring traditional open-loop motion planning techniques from simulation to the real world particularly challenging. available closed-loop policy learning approaches, for physics- based manipulation tasks, typically either focus on single object manipulation, or rely on imitation learning, which inherently constrains task generalization and performance to the available demonstrations. in this work, we propose an approach to learn a policy for physics-based manipulation in clutter, which enables the robot to react to the uncertain dynamics of the real world. we start with presenting an imitation learning technique which compiles demonstrations from a sampling-based planner into an action-value function encoded as a deep neural network. we then use the learned action-value function to guide a look-ahead planner, giving us a control policy. lastly, we propose to reﬁne the deep action-value function through reinforcement learning, taking advantage of the look-ahead planner. we evaluate our approach in a physics-enabled simulation environment with artiﬁcially injected uncertainty, as well as in a real world task of manipulation in clutter. "
1803.08102,". mixing by cutting-and-shuﬄing can be understood and predicted using dynamical systems based tools and techniques. in existing studies, mixing is generated by maps that repeat the same cut-and-shuﬄe process at every iteration, in a “ﬁxed” manner. however, mixing can be greatly improved by varying the cut-and-shuﬄe parameters at each step, using a “variable” approach. to demonstrate this approach, we show how to optimize mixing by cutting-and-shuﬄing on the one-dimensional line interval, known as an interval exchange transformation (iet). mixing can be signiﬁcantly improved by optimizing variable protocols, especially for initial conditions more complex than just a simple two-color line interval. while we show that optimal variable iets can be found analytically for arbitrary numbers of iterations, for more complex cutting-and-shuﬄing systems, computationally expensive numerical optimization methods would be required. furthermore, the number of control parameters grows linearly with the number of iterations in variable systems. therefore, optimizing over large numbers of iterations is generally computationally prohibitive. we demonstrate an ad hoc approach to cutting-and-shuﬄing that is computationally inexpensive and guarantees the mixing metric is within a constant factor of the optimum. this ad hoc approach yields signiﬁcantly better mixing than ﬁxed iets which are known to produce weak-mixing, because cut pieces never reconnect. the heuristic principles of this method can be applied to more general cutting-and-shuﬄing systems. key words. mixing optimization, cutting-and-shuﬄing, interval exchange transformation ams subject classiﬁcations. 37a25, 49j21, 49k21, 49n90 "
1803.08104,"—in the future, sensor nodes or internet of things (iots) will be tasked with sampling the environment. these nodes/devices are likely to be powered by a hybrid access point (hap) wirelessly, and may be programmed by the hap with a sampling time to collect sensory data, carry out computation, and transmit sensed data to the hap. a key challenge, however, is random channel gains, which cause sensor nodes to receive varying amounts of radio frequency (rf) energy. to this end, we formulate a stochastic program to determine the charging time of the hap and sampling time of sensor nodes. our objective is to minimize the expected penalty incurred when sensor nodes experience an energy shortfall. we consider two cases: single and multi time slots. in the former, we determine a suitable hap charging time and nodes sampling time on a slot-by-slot basis whilst the latter considers the best charging and sampling time for use in the next t slots. we conduct experiments over channel gains drawn from the gaussian, rayleigh or rician distribution. numerical results conﬁrm our stochastic program can be used to compute good charging and sampling times that incur the minimum penalty over the said distributions. index terms—wireless sensor networks, wireless charging, stochastic programs, sample average approximation (saa). "
1803.08105," we present a 45 ks chandra observation of the quasar ulas j1342+0928 at z = 7.54. we detect 14.0+4.8 −3.7 counts from the quasar in the observed-frame energy range 0.5–7.0 kev (6σ detection), representing the most distant non-transient astronomical source identiﬁed in x-rays to date. the present data are suﬃcient only to infer rough constraints on the spectral parameters. we ﬁnd an x-ray hardness ratio of hr = −0.51+0.26 −0.28 between the 0.5–2.0 kev and 2.0–7.0 kev ranges and derive a power-law photon index of γ = 1.95+0.55 −0.53. assuming a typical value for high-redshift quasars of γ = 1.9, ulas j1342+0928 has a 2–10 kev rest-frame x-ray luminosity of l2−10 = 11.6+4.3 −3.5 × 1044 erg s−1. its x-ray-to-optical power-law slope is αox = −1.67+0.16 −0.10, consistent with the general trend indicating that the x-ray emission in the most bolometrically powerful quasars is weaker relative to their optical emission. keywords: cosmology: observations — cosmology: early universe — quasars: individual (ulas j134208.10+092838.61) 8 1 0 2   r a   m 1 2     ] . a g h p - o r t s a [     1 v 5 0 1 8 0 . 3 0 8 1 : v i x r a corresponding author: eduardo ba˜nados ebanados@carnegiescience.edu ∗ carnegie-princeton fellow 2 ba˜nados et al. "
1803.08106,". we consider general symmetric systems of ﬁrst order linear partial dif- ferential operators on domains ω ⊂ rd, and we seek suﬃcient conditions on the coeﬃcients which ensure essential self-adjointness. the coeﬃcients of the ﬁrst order terms are only required to belong to c 1(ω) and there is no ellipticity condition. our criterion writes as the completeness of an associated riemannian structure which en- codes the propagation velocities of the system. as an application we obtain suﬃcient conditions for conﬁnement of energy for some wave propagation problems of classical physics. "
1803.08107,". in this article, we consider compact surfaces σ having constant mean curvature h (h- surfaces) whose boundary γ = ∂σ ⊂ m0 = m × f {0} is transversal to the slice m0 of the warped product m × f r, here m denotes a hadamard surface. we obtain height estimate for a such surface σ having positive constant mean curvature involving the area of a part of σ above of m0 and the volume it bounds. also we give general conditions for the existence of rotationally-invariant topological spheres having positive constant mean curvature h in the warped product h × f r, where h denotes the hyperbolic disc. finally we present a non-trivial example of such spheres. "
1803.08111,"— in the standard mechanism design framework (hurwicz-reiter [1]), there is a central authority that gathers agents’ messages and subsequently determines the allocation and tax for each agent. we consider a scenario where, due to communication overhead and other constraints, such broad- casting of messages to a central authority cannot take place. instead, only local message exchange is allowed between agents. as a result, each agent should be able to determine her own allo- cation and tax based on messages in the local neighborhood, as deﬁned by a given message graph describing the communication constraints. this scenario gives rise to a novel research direction that we call “distributed mechanism design”. in this paper, we propose such a distributed mechanism for the problem of rate allocation in a multicast transmission network. the proposed mechanism fully implements the optimal allocation in nash equilibria and its message space dimension is linear with respect to the number of agents in the network. index terms— mechanism design, rate allocation, decentral- ized optimization, strategic users, nash equilibrium "
1803.08112," we age-date the stellar populations associated with 12 historic nearby core-collapse supernovae (ccsne) and 2 supernova impostors, and from these ages, we infer their initial masses and associated uncertainties. to do this, we have obtained new hst imaging covering these ccsne. using these images, we measure resolved stellar photometry for the stars surrounding the locations of the sne. we then ﬁt the color-magnitude distributions of this photometry with stellar evolution models to determine the ages of any young existing populations present. from these age distributions, we infer the most likely progenitor mass for all of the sne in our sample. we ﬁnd ages between 4 and 50 myr, corresponding to masses from 7.5 to 59 solar masses. there were no sne that lacked a young population within 50 pc. our sample contains 4 type ib/c sne; their masses have a wide range of values, suggesting that the progenitors of stripped-envelope sne are binary systems. both impostors have masses constrained to be <∼ 7.5 solar masses. in cases with precursor imaging measurements, we ﬁnd that age-dating and precursor imaging give consistent progenitor masses. this consistency implies that, although the uncertainties for each technique are signiﬁcantly diﬀerent, the results of both are reliable to the measured uncertainties. we combine these new measurements with those from our previous work and ﬁnd that the distribution of 25 core-collapse sne progenitor masses is consistent with a standard salpeter power-law mass function, no upper mass cutoﬀ, and an assumed minimum mass for core-collapse of 7.5 m(cid:12). the distribution is consistent with a minimum mass <9.5 m(cid:12). subject headings: supernovae — 1. "
1803.08114,". in applications like medical imaging, error correction, and sensor networks, one needs to solve large-scale linear systems that may be corrupted by a small number of, but arbitrarily large, corruptions. we consider solving such large-scale systems of linear equations ax = b that are inconsistent due to corruptions in the measurement vector b. with this as our motivating ex- ample, we develop an approach for this setting that allows detection of the corrupted entries and thus convergence to the “true” solution of the original system. we provide analytical justiﬁcation for our approaches as well as experimental evidence on real and synthetic systems. 8 1 0 2   r a   m 1 2     ] . a n h t a m [     1 v 4 1 1 8 0 . 3 0 8 1 : v i x r a "
1803.08115," the physics of superconducting ﬁlms, and especially the role of remanent magne- tization has a deﬁning inﬂuence on the magnetic ﬁelds used to hold and manipu- late atoms on superconducting atomchips. we magnetically trap ultracold 87rb atoms on a 200 µm wide and 500 nm thick cryogenically cooled niobium z-wire structure. by measuring the distance of the atomcloud to the trapping wire for diﬀerent transport currents and bias ﬁelds, we probe the trapping characteristics of the niobium superconducting structure. at distances closer than the trapping wire width, we observe a diﬀerent behaviour than that of normal conducting wire traps. furthermore, we measure a stable magnetic trap at zero transport cur- rent. these observations point to the presence of a remanent magnetization in our niobium ﬁlm which is induced by a transport current. this current-induced magnetization deﬁnes the trap close to the chip surface. our measurements agree very well with an analytic prediction based on the critical state model (csm). our results provide a new tool to control atom trapping on superconducting atomchips by designing the current distribution through its current history. contents "
1803.08118," seglearn is an open-source python package for machine learning time series or sequences using a sliding window segmentation approach. the implementation provides a ﬂexible pipeline for tackling classiﬁcation, regression, and forecasting problems with multivariate sequence and contextual data. this package is compatible with scikit-learn and is listed under scikit-learn ”related projects”. the package depends on numpy, scipy, and scikit-learn. seglearn is distributed under the bsd 3-clause license. documenta- tion includes a detailed api description, user guide, and examples. unit tests provide a high degree of code coverage. source code and documentation can be download from https://github.com/dmbee/seglearn. keywords: machine-learning, time-series, sequences, python "
1803.08119," a z2 symmetry that extends the weak interaction, su (2)l → su (2)l × su (2)(cid:48), and the higgs sector, h(2) → h(2, 1) + h(cid:48)(1, 2), yields a standard model quartic coupling that vanishes at scale v(cid:48) = (cid:104)h(cid:48) (cid:105) (cid:29) (cid:104)h(cid:105). near v(cid:48), theories either have a “prime” sector, or possess “left-right” (lr) symmetry with su (2)(cid:48) = su (2)r. if the z2 symmetry incorporates spacetime parity, these theories can solve the strong cp problem. the lr theories have all quark and lepton masses arising from operators of dimension 5 or more, requiring froggatt-nielsen structures. two-loop contributions to ¯θ are estimated and typically lead to a neutron electric dipole moment of order 10−27e cm that can be observed in future experiments. minimal models, with gauge group su (3) × su (2)l × su (2)l × u (1)b−l, have precise gauge coupling uniﬁcation for v(cid:48) = 1010±1 gev, successfully correlating gauge uniﬁcation with the observed higgs mass of 125 gev. with su (3) × u (1)b−l embedded in su (4), the central value of the uniﬁcation scale is reduced from 1016−17 gev to below 1016 gev, improving the likelihood of proton decay discovery. uniﬁed theories based on so(10) × cp are constructed that have h + h(cid:48) in a 16 or 144 and generate higher-dimensional ﬂavor operators, while maintaining perturbative gauge couplings. 8 1 0 2   r a   m 1 2     ] h p - p e h [     1 v 9 1 1 8 0 . 3 0 8 1 : v i x r a 1 contents "
1803.08120,". we study the stochastic six vertex model and prove that under weak asymmetry scaling (i.e., when the parameter ∆ → 1+ so as to zoom into the ferroelectric/disordered phase critical point) its height function ﬂuctuations converge to the solution to the kardar–parisi–zhang (kpz) equation. we also prove that the one-dimensional family of stochastic gibbs states for the symmetric six vertex model converge under the same scaling to the stationary solution to the stochastic burgers equation. we achieve this through a new markov duality method which shows how duality can be leveraged to prove previously inaccessible results in hydrodynamics and spde limits. our starting point is an exact microscopic hopf–cole transform for the stochastic six vertex model which follows from the model’s known one-particle markov self-duality. given this transform, the crucial step is to establish self-averaging for speciﬁc quadratic function of the transformed height function. we use the model’s two-particle self-duality to produce explicit expressions (as bethe ansatz contour integrals) for conditional expectations from which we extract time-decorrelation and hence self-averaging in time. the crux of the markov duality method is that the entire convergence result reduces to precise estimates on the one-particle and two-particle transition probabilities. 8 1 0 2   r p a 3       ] . r p h t a m [     2 v 0 2 1 8 0 . 3 0 8 1 : v i x r a contents "
1803.08121," in high performance computing, scheduling of tasks and allocation to machines is very critical especially when we are dealing with heterogeneous execution costs. simulations can be performed with a large variety of environments and application models. however, this technique is sensitive to bias when it relies on random instances with an uncontrolled distribution. we use methods from the literature to provide formal guarantee on the distribution of the instance. in particular, it is desirable to ensure a uniform distribution among the instances with a given task and machine heterogeneity. in this article, we propose a method that generates instances (cost matrices) with a known distribution where tasks are scheduled on machines with heterogeneous execution costs. 1 "
1803.08125," typical tsallis’ statistical mechanics’ quantiﬁers like the partition function and the mean energy exhibit poles. we are speaking of the partition function z and the mean energy < u >. the poles ap- pear for distinctive values of tsallis’ characteristic real parameter q, at a numerable set of rational numbers of the q−line. these poles are dealt with dimensional regularization resources. the physical eﬀects of these poles on the speciﬁc heats are studied here for the two-body classical gravitation potential. keywords: tsallis entropy, divergences, dimensional regular- ization, speciﬁc heat. 1 1 "
1803.08127,". recently burkhardt et. al. introduced the k-checkerboard random matrix ensembles, which have a split limiting behavior of the eigenvalues (in the limit all but k of the eigenvalues are on the order of √n and converge to semi-circular behavior, with the remaining k of size n and converging to hollow gaussian ensembles). we generalize their work to consider non-hermitian ensembles with complex eigenvalues; instead of a blip new behavior is seen, ranging from multiple satellites to annular rings. these results are based on moment method techniques adapted to the complex plane as well as analysis of singular values. contents "
1803.08128,": in this paper we propose a ﬂexible cure rate model with frailty term in latent risk, which is obtained by incorporating a frailty term in risk function of latent competing causes. the number of competing causes of the event of interest follows negative binomial distribution and the frailty variable follows power variance function distribution, in which includes other frailty models such as gamma, positive stable and inverse gaussian frailty models as special cases. the proposed model takes into account the presence of covariates and right-censored survival data suitable for populations with a cure rate. besides, it allows to quantify the degree of unobserved heterogeneity induced by unobservable risk factors, in which is important to explain the survival time. once the posterior distribution has not close form, markov chain monte carlo simulations are consider for estimation procedure. we performed several simulation studies and the practical relevance of the proposed model is demonstrated in a real data set. keywords: bayesian model; competing causes; cure rate models; frailty models; power vari- ance function. "
1803.08134,". although deep nets have resulted in high accuracies for various vi- sual tasks, their computational and space requirements are prohibitively high for inclusion on devices without high-end gpus. in this paper, we introduce a neu- ron/ﬁlter level pruning framework based on fisher’s lda which leads to high accuracies for a wide array of facial trait classiﬁcation tasks, while signiﬁcantly reducing space/computational complexities. the approach is general and can be applied to convolutional, fully-connected, and module-based deep structures, in all cases leveraging the high decorrelation of neuron activations found in the pre-decision layer and cross-layer deconv dependency. experimental results on binary and multi-category facial traits from the lfwa and adience datasets il- lustrate the framework’s comparable/better performance to state-of-the-art prun- ing approaches and compact structures (e.g. squeezenet, mobilenet). ours suc- cessfully maintains comparable accuracies even after discarding most parameters (98%-99% for vgg-16, 82% for googlenet) and with signiﬁcant flop reduc- tions (83% for vgg-16, 64% for googlenet). keywords: deep nets pruning; facial trait classiﬁcation; fisher lda 8 1 0 2   r a   m 1 2     ] v c . s c [     1 v 4 3 1 8 0 . 3 0 8 1 : v i x r a "
1803.08135,". we discuss the pistone-sempi exponential manifold on the ﬁnite-dimensional gaussian space. we consider the role of the entropy, the continuity of translations, poincar´e-type inequalities, the generalized diﬀerentiability of probability densities of the gaussian space. keywords: information geometry, pistone-sempi exponential mani- fold, gaussian orlicz space, gaussian orlicz-sobolev space 1 "
1803.08136," various types of equilibrium processes involve electric ﬁelds. in some cases, the electrical energy appears to be negative (e.g. if the voltage is ﬁxed by an external source). this pa- per explains how to derive the correct thermo-dynamic potential for electro-static phenomena, whether the voltage is ﬁxed, or the charge is ﬁxed, or some combination is ﬁxed. in particular, we explain, in complete detail, why ﬁxing the voltage introduces “a minus sign” in the electrical energy. two explanations are given. the ﬁrst explanation is based on a lumped-parameter argument (i.e. a lumped-capacitor model). the second explanation uses a distributed parameter model (i.e. a partial diﬀerential equation (pde) model) of a dielectric medium; in this case, we allow for non-linearity and external polarization eﬀects. connections with legendre (duality) transforms are also discussed. 1 "
1803.08137," we revisit the blind deconvolution problem with a focus on understanding its robustness and con- vergence properties. provable robustness to noise and other perturbations is receiving recent interest in vision, from obtaining immunity to adversarial attacks to assessing and describing failure modes of algorithms in mission critical applications. further, many blind deconvolution methods based on deep architectures internally make use of or optimize the basic formulation, so a clearer under- standing of how this sub-module behaves, when it can be solved, and what noise injection it can tolerate is a ﬁrst order requirement. we derive new insights into the theoretical underpinnings of blind deconvolution. the algorithm that emerges has nice convergence guarantees and is provably robust in a sense we formalize in the paper. interestingly, these technical results play out very well in practice, where on standard datasets our algorithm yields results competitive with or superior to the state of the art. keywords: blind deconvolution, robust continuous optimization "
1803.08139,"—in the past few decades, ﬁbre bragg grating (fbg) sensors have gained a lot of attention in the ﬁeld of distributed point strain measurement. one of the most interesting properties of these sensors is the presumed linear relationship between the strain and the peak wavelength shift of the fbg reﬂected spectra. however, subjecting sensors to a non-uniform stress ﬁeld will in general result in a strain estimation error when using this linear relationship. in this paper we propose a new strain estimation algorithm that accurately estimates the mean strain value in the case of non-uniform strain distributions. to do so, we ﬁrst introduce an approximation of the classical transfer matrix model, which we will refer to as the approximated transfer matrix model (atmm). this model facilitates the analysis of fbg reﬂected spectra under arbitrary strain distributions. based on this new formulation, we derive a maximum likelihood estimator of the mean strain value. the algorithm is validated using both computer simulations and experimental fbg measurements. compared to state-of-the-art methods, which typically introduce errors of tens of microstrains, the proposed method is able to compensate for this error. in the typical examples that were analysed in this study, mean strain errors of around 30µε were compensated. index terms—ﬁber bragg grating, fbg, ﬁber optic sensing, reﬂected spectra, strain distribution, transfer matrix model. "
1803.08140,". the frequency of occurrence of “locally repeated” values of arithmetic functions is a common theme in analytic number theory, for instance in the erd˝os-mirsky problem on coincidences of the divisor function at consecutive integers, the analogous problem for the euler totient function, and the quantitative conjectures of erd˝os, pomerance and sark˝ozy and of graham, holt and pomerance on the frequency of occurrences. in this paper we introduce the corresponding problems in the setting of polynomials over a ﬁnite ﬁeld, and completely solve them in the large ﬁnite ﬁeld limit. "
1803.08141," one of the phenomena that inﬂuences signiﬁcantly the performance of low-density parity-check codes is known as trapping sets. an (a, b) elementary trapping set, or simply an ets where a is the size and b is the number of degree-one check nodes and b a < 1, causes high decoding failure rate and exert a strong inﬂuence on the error ﬂoor. in this paper, we provide sufﬁcient conditions for exponent matrices to have fully connected (3, n)-regular qc-ldpc codes with girths 6 and 8 whose tanner graphs are free of small etss. applying sufﬁcient conditions on the exponent matrix to remove some 8-cycles results in removing all 4-cycles, 6-cycles as well as some small elementary trapping sets. for each girth we obtain a lower bound on the lifting degree and present exponent matrices with column weight three whose corresponding tanner graph is free of certain etss. index terms ldpc codes, girth, tanner graph, trapping set. "
1803.08145," a word is called β-free if it has no factors of exponent greater than or equal to β. the repetition threshold rt(k) is the inﬁmum of the set of all β such that there are arbitrarily long k-ary β-free words (or equivalently, there are k-ary β-free words of every suﬃciently large length, or even every length). these three equivalent deﬁnitions of the repetition threshold give rise to three natural deﬁnitions of a repetition threshold for circular words. the inﬁmum of the set of all β such that (a) there are arbitrarily long k-ary β-free circular words is called the weak circular repetition threshold, denoted crtw(k); (b) there are k-ary β-free circular words of every suﬃciently large length is called the intermediate circular repetition threshold, denoted crti(k); (c) there are k-ary β-free circular words of every length is called the strong circular repetition threshold, denoted crts(k). 2 and crts(5) = 4 we prove that crts(4) = 3 3 , conﬁrming a conjecture of gorbunova and providing the last unknown values of the strong circular repetition threshold. we also prove that crti(3) = crtw(3) = rt(3) = 7 4 . msc 2010: 68r15 keywords: circular words; repetition threshold; circular repetition threshold 1 "
1803.08151,"—in this paper, we enunciate the theorem of secrecy in tagged protocols using the theory of witness-functions and we run a formal analysis on a new tagged version of the needham-schroeder public-key protocol using this theorem. we discuss the signiﬁcance of tagging in securing cryptographic protocols as well. index terms—cryptographic protocols, intruder, secrecy, secu- rity, tag, uniﬁcation, witness-function. notice 1 c(cid:13) 2018 ieee. personal use of this material is permitted. permission from ieee must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. "
1803.08152,"—this paper presents a distributed position synchro- nization strategy that also preserves the initial communication links for single-integrator multi-agent systems with time-varying delays. the strategy employs a coordinating proportional control derived from a speciﬁc type of potential energy, augmented with damping injected through a dynamic ﬁlter. the injected damping maintains all agents within the communication distances of their neighbours, and asymptotically stabilizes the multi-agent system, in the presence of time delays. regarding the closed-loop single- integrator multi-agent system as a double-integrator system suggests an extension of the proposed strategy to connectivity- preserving coordination of euler-lagrange networks with time- varying delays. lyapunov stability analysis and simulation results validate the two designs. index terms—connectivity preservation, distributed coordina- tion, multi-agent systems, time-varying delays. "
1803.08153," considered as a data-driven approach, fingerprinting localization solutions (fpss) en- joy huge popularity due to their good performance and minimal environment information requirement. this papers addresses applications of artiﬁcial intelligence to solve two prob- lems in received signal strength indicator (rssi) based fps, ﬁrst the cumbersome training database construction and second the extrapolation of ﬁngerprinting algorithm for similar buildings with slight environmental changes. after a concise overview of deep learning design techniques, two main techniques widely used in deep learning are exploited for the above mentioned issues namely data augmentation and transfer learning. we train a multi-layer neural network that learns the mapping from the observations to the locations. a data aug- mentation method is proposed to increase the training database size based on the structure of rssi measurements and hence reducing eﬀectively the amount of training data. then it is shown experimentally how a model trained for a particular building can be transfered to a similar one by ﬁne tuning with signiﬁcantly smaller training numbers. the paper implicitly discusses the new guidelines to consider about deep learning designs when they are employed in a new application context. "
1803.08154,". this paper provides a method to construct simultaneous conﬁdence bands for quantile functions and quantile eﬀects in nonlinear network and panel models with unobserved two-way eﬀects, strictly exogenous covariates, and possibly discrete outcome variables. the method is based upon projection of simultaneous conﬁdence bands for distribution functions constructed from ﬁxed eﬀects distribution regression estimators. these ﬁxed eﬀects estimators are bias corrected to deal with the incidental parameter problem. under asymptotic sequences where both dimensions of the data set grow at the same rate, the conﬁdence bands for the quantile functions and eﬀects have correct joint coverage in large samples. an empirical application to gravity models of trade illustrates the applicability of the methods to network data. "
1803.08155,". despite major methodological developments, bayesian inference for gaussian graphical models remains challenging in high dimension due to the tremendous size of the model space. this article proposes a method to infer the marginal and conditional independence structures between variables by multiple testing of hypotheses. speciﬁcally, we introduce closed-form bayes factors under the gaussian conjugate model to evaluate the null hypotheses of marginal and conditional independence between variables. their computation for all pairs of variables is shown to be extremely eﬃcient, thereby allowing us to address large problems with thousands of nodes. moreover, we derive exact tail probabilities from the null distributions of the bayes factors. these allow the use of any multiplicity correction procedure to control error rates for incorrect edge inclusion. we demonstrate the proposed approach to graphical model selection on various simulated examples as well as on a large gene expression data set from the cancer genome atlas. 1 "
1803.08157,"— we consider the problem of computing certain (mvoe) parameterized minimum volume outer ellipsoidal approximation of the minkowski sum of a ﬁnite number of ellipsoids. we clarify connections among several param- eterizations available in the literature, obtain novel analysis results regarding the conditions of optimality, and based on the same, propose two new algorithms for computing the pa- rameterized mvoe. numerical results reveal faster runtime for the proposed algorithms than the state-of-the-art semideﬁnite programming approach of computing the same. "
1803.08159,"—this paper presents a globally stable teleopera- tion control strategy for systems with time-varying delays that eliminates the need for velocity measurements through novel augmented immersion and invariance velocity observers. the new observers simplify a recent constructive immersion and in- variance velocity observer to achieve globally convergent velocity estimation with only n+2 states, where n is the number of degrees of freedom of the master and slave robots. they introduce dy- namic scaling factors to accelerate the speed of convergence of the velocity estimates and, thus, to limit the energy generated by the velocity estimation errors and to guarantee sufﬁcient estimate- based damping injection to dissipate the energy generated by the time-varying delays. the paper shows that proportional plus damping control with the simpliﬁed and augmented immersion and invariance-based velocity observers can synchronize the free master and slave motions in the presence of time-varying delays without using velocity measurements. numerical results illustrate the estimation performance of the new observers and the stability of a simulated two degrees-of-freedom nonlinear teleoperation system with time-varying delays under the proposed output feedback proportional plus damping control. "
1803.08160," we introduce a new diﬀusion process {xt}t≥0 to describe asset prices within an economic bubble cycle. the main feature of the process, which diﬀers from existing models, is the drift term where a mean-reversion is taken based on an exponential decay of the scaled price. our study shows the scaling factor on {xt}t≥0 is crucial for modelling economic bubbles as it mitigates the dependence structure between the price and parameters in the model. we prove both the process and its ﬁrst passage time are well-deﬁned. an eﬃcient calibration scheme, together with the probability density function for the process are given. moreover, by employing the perturbation technique, we deduce the closed-form density for the downward ﬁrst passage time, which therefore can be used in estimating the burst time of an economic bubble. the object of this study is to understand the asset price dynamics when a ﬁnancial bubble is believed to form, and correspondingly provide estimates to the bubble’s crash time. calibration examples on the us dot-com bubble and the 2007 chinese stock market crash verify the eﬀectiveness of the model itself. the example on bitcoin prediction conﬁrms that we can provide meaningful estimate on the downward probability for asset prices. keywords: economic bubbles, diﬀusion process, first passage time, perturbation, cryptocurrency 1 "
1803.08163," we conﬁrm the trend in the radial velocity data for epsilon indi a suggesting a long- period planetary companion and ﬁnd signiﬁcant curvature is present, suﬃcient to quantify epsilon indi ab as a cold jupiter with a minimum mass of 2.71+2.19 −0.44 mjup on a nearly circular orbit with a semi-major axis of 12.82+4.18 −0.71 au and an orbital period of 52.62+27.70 −4.12 yr. we also identify other signiﬁcant signals in the radial velocity data. we investigate a variety of spectral diagnostics and interpret these signals as arising from activity-induced radial velocity variations. in particular, the 2500 and 278 d signals are caused by magnetic cycles. while a planetary signal might be present in the 17.8 d signal, the origin of 17.8 and 11 d signals are most easily interpreted as arising in the rotation of the star with a period of about 35 d. we ﬁnd that traditional activity indicators have a variety of sensitivities. in particular, the sodium lines and cahk index are sensitive to all activity-induced signals. the line bisector measurement is sensitive to stellar rotation signal while hα is sensitive to the secondary magnetic cycle. in general, because of their diﬀerent sensitivities these activity indicators introduce extra noise if included in the noise model whereas diﬀerential rvs provide a robust proxy to remove wavelength-dependent noise eﬃciently. based on these analyses, we propose an activity diagnostics procedure for the detection of low amplitude signals in high precision radial velocity data. thus the epsilon indi system comprises of at least epsilon indi a, ab as well as a long period brown dwarf binary ba and bb; so it provides a benchmark case for our understanding of the formation of gas giants and brown dwarfs. key words: methods: statistical – methods: data analysis – techniques: radial ve- locities – stars: individual: epsilon indi a "
1803.08166," we consider a retailer who buys energy in the wholesale market and resells it to ﬁnal consumers. the retailer has to decide when to intervene to change the price he asks to his customers, in order to maximize his income. we model the problem as an inﬁnite-horizon stochastic impulse control problem. we characterize an optimal price strategy and provide analytical existence results for the equations involved. we then investigate the dependence on the intervention cost. in particular, we prove that the measure of the continuation region is asymptotic to the fourth root of the cost. finally, we provide some numerical results and consider a suitable extension of the model. msc classiﬁcation: 93e20, 91b70, 91b24. key words: price management, energy markets. impulse controls, quasi-variational inequality, asymptotic estimates, 1 "
1803.08167," electroactive polymers such as dielectric elastomers (des) have attracted signiﬁcant attention in recent years. computational techniques to solve the coupled electromechanical system of equations for this class of mate- rials have universally centered around fully coupled monolithic formulations, which while generating good accuracy requires signiﬁcant computational ex- pense. however, this has signiﬁcantly hindered the ability to solve large scale, fully three-dimensional problems involving complex deformations and electromechanical instabilities of des. in this work, we provide theoretical basis for the eﬀectiveness and accuracy of staggered explicit-implicit ﬁnite element formulations for this class of electromechanically coupled materials, and elicit the simplicity of the resulting staggered formulation. we demon- strate the stability and accuracy of the staggered approach by solving com- plex electromechanically coupled problems involving electroactive polymers, where we focus on problems involving electromechanical instabilities such as creasing, wrinkling, and bursting drops. in all examples, essentially identical results to the fully monolithic solution are obtained, showing the accuracy of the staggered approach at a signiﬁcantly reduced computational cost. keywords: staggered, explicit-implicit, creasing, dielectric elastomer, wrinkling, surface tension ∗corresponding author email address: parkhs@bu.edu (harold s. park) preprint submitted to cmame march 23, 2018 "
1803.08169," we extend analytic large network results on default contagion in random graphs to capture a pronounced block model structure. this includes as a special case the core-periphery network structure, which plays a prominent role in recent research on systemic risk. further, in the existing literature on systemic risk using random graph methods the problematic assumption that the distribution of liabilities solely depends on the creditor type seems to persist. under this assumption a straightforward application of the law of large numbers allows to turn edge related random elements into deterministic vertex properties. here we study a general setting in which the liabilities may depend on both the creditor and the debtor where this argument breaks down and a direct asymptotic analysis of the edge weighted random graph becomes necessary. among several other applications our results allow us to obtain resilience conditions for the entire network (for example the global ﬁnancial network) based only on subnetwork conditions. contrasting earlier research we also give an example that demonstrates how reshuﬄing edge weights to form blocks can in fact impact resilience even for otherwise very homogeneous networks. keywords: systemic risk, ﬁnancial contagion, inhomogeneous random graphs, weighted random graphs, directed random graphs, stochastic block model, core-periphery, assortative random graphs, counterparty dependent exposures 1 "
1803.08170," in the context of a sequential search problem, i explore large-generations learning dynamics for agents who suﬀer from the “gambler’s fallacy” — the statistical bias of anticipating too much regression to the mean for realizations of independent random events. searchers are uncertain about search pool qualities of diﬀerent periods but infer these fundamentals from search outcomes of the previous generation. searchers’ stopping decisions impose a censoring eﬀect on the data of their successors, as the val- ues they would have found in later periods had they kept searching remain unobserved. while innocuous for rational agents, this censoring eﬀect interacts with the gambler’s fallacy and creates a feedback loop between distorted stopping rules and pessimistic beliefs about search pool qualities of later periods. in general settings, the stopping rules used by diﬀerent generations monotonically converge to a steady-state rule that stops searching earlier than optimal. in settings where true pool qualities increase over time — so there is option value in rejecting above-average early draws — learning is monotonically harmful and welfare strictly decreases across generations. 1 "
1803.08171," in organizational and commercial settings, people often have clear roles and workﬂows against which functional and non-functional requirements can be extracted. however, in more social settings, such as platforms for enhancing social interaction, successful applica- tions are driven more by user emotional engagement than functionality, the drivers of user engagement are diﬃcult to identify. a key challenge is to understand people’s emotional goals so that they can be incorporated into the design. this paper proposes a novel frame- work called the emotional attachment framework that is based on existing models and theories of emotional attachment. its aim is to facilitate the process of capturing emotional goals in software engineering. to demonstrate the framework in use, emotional goals are elicited for a software application that aims to provide help for homeless people. the out- comes are evaluated by domain experts and compared with an alternative approach. the results indicate that the emotional attachment framework has the potential to help system analysts uncover additional emotional goals as well as provide valuable insights into these emotional goals. keywords: people-oriented software, requirements engineering, emotional goals, emotional attachment framework "
1803.08173," the james webb space telescope (jwst) will measure exoplanet transmission and eclipse spectroscopy at un- precedented precisions to better understand planet structure, dynamics, chemistry and formation. these are essential tools on the march toward biosignature searches on potentially habitable planets. we explore a range of exoplanet atmospheric conditions and forecast the expected results with jwst. we take realistic chimera models that match existing spitzer and hst results and simulate the spectra achievable with the jwst miri + nircam guaranteed time observations (gto) survey, which includes observations of hd 189733 b, wasp-80 b, hat-p-19 b, wasp- 107 b, gj 436 b and hat-p-26 b. we then retrieve atmospheric parameters from these spectra to estimate the precision to which the planets’ atmospheric compositions can be measured. we ﬁnd that emission spectra have well- constrained unimodal solutions but transmission spectra near 10× solar abundance and solar c/o ratios can suﬀer from bimodal solutions. broad wavelength coverage as well as higher precision data can resolve bimodal solutions and provide dramatically better atmospheric parameter constraints. we ﬁnd that metallicities can be measured to within 20% to 170%, which approaches the precisions on solar system planets, and c/o ratios can be constrained to ∼10% to 60%, assuming that observers can leverage short wavelength data to select the correct solution from the bimodal posteriors. these compositional precisions are suﬃcient to validate or refute predictions from disk formation models on ﬁnal atmospheric abundances as long as their history is not erased by planet evolution processes. we also show the extent to which eclipse mapping with jwst is possible on our brightest system hd 189733 b. keywords: planets and satellites: atmospheres — planets and satellites: composition 8 1 0 2   y a m   1 2 .     ] p e h p - o r t s a [     2 v 3 7 1 8 0 . 3 0 8 1 : v i x r a corresponding author: everett schlawin eas342 at email dot arizona .edu 2 schlawin et al. "
1803.08175," we place constraints on the propagation velocity of bulk turbulence within the intr- acluster medium of three clusters and an elliptical galaxy. using reﬂection grating spectrometer measurements of turbulent line broadening, we show that for these clus- ters, the 90% upper limit on turbulent velocities when accounting for instrumental broadening is too low to propagate energy radially to the cooling radius of the clus- ters within the required cooling time. in this way, we extend previous hitomi-based analysis on the perseus cluster to more clusters, with the intention of applying these results to a future, more extensive catalog. these results constrain models of turbu- lent heating in agn feedback by requiring a mechanism which can not only provide suﬃcient energy to oﬀset radiative cooling, but resupply that energy rapidly enough to balance cooling at each cluster radius. key words: intergalactic medium – intracluster medium – cooling ﬂows – turbulence. "
1803.08176," recent work on four dimensional eﬀective descriptions of the heterotic string has identiﬁed the moduli of such systems as being given by kernels of maps between ordinary dolbeault cohomology groups. the maps involved are deﬁned by the supergravity data of the background solutions. such structure is seen both in the case of calabi-yau compactiﬁcations with non-trivial constraints on moduli arising from the gauge bundle and in the case of some non-k¨ahler compactiﬁcations of the theory. this description of the moduli has allowed the explicit computation of the moduli stabilization eﬀects of a wide range of non-trivial gauge bundles on calabi-yau three-folds. in this paper we examine to what extent the ideas and techniques used in this work can be extended to the case of type iib string theory. certain simpliﬁcations arise in the type iib case in comparison to the heterotic situation. however, complications also arise due to the richer supergravity data of the theory inducing a more involved map structure. we illustrate our discussion with several concrete examples of compactiﬁcation of type iib string theory on conformal cicy three-folds with ﬂux. 8 1 0 2   r a   m 1 2     ] h t - p e h [     1 v 6 7 1 8 0 . 3 0 8 1 : v i x r a jamesgray@vt.edu varzi61@vt.edu contents "
1803.08177,"—the growing popularity of virtual and augmented reality communications and 360◦ video streaming is moving video communication systems into much more dynamic and resource-limited operating settings. the enormous data volume of 360◦ videos requires an efﬁcient use of network bandwidth to maintain the desired quality of experience for the end user. to this end, we propose a framework for viewport-driven rate-distortion optimized 360◦ video streaming that integrates the user view navigation pattern and the spatiotemporal rate-distortion charac- teristics of the 360◦ video content to maximize the delivered user quality of experience for the given network/system resources. the framework comprises a methodology for constructing dynamic heat maps that capture the likelihood of navigating different spatial segments of a 360◦ video over time by the user, an analysis and characterization of its spatiotemporal rate-distortion characteristics that leverage preprocessed spatial tilling of the 360◦ view sphere, and an optimization problem formulation that characterizes the delivered user quality of experience given the user navigation patterns, 360◦ video encoding decisions, and the available system/network resources. our experimental results demonstrate the advantages of our framework over the conventional approach of streaming a monolithic uniformly- encoded 360◦ video and a state-of-the-art reference method. considerable video quality gains of 4 - 5 db are demonstrated in the case of two popular 4k 360◦ videos. "
1803.08178," there has recently been a steadily increase in the iterative ap- proaches to boosted density estimation and sampling, usually pro- ceeding by adding candidate “iterate” densities to a model that gets more accurate with iterations. the relative accompanying burst of formal convergence results has not yet changed a striking picture: all results essentially pay the price of heavy assumptions on iterates, often unrealistic or hard to check, and oﬀer a blatant contrast with the original boosting theory where such assumptions would be the weakest possible. in this paper, we show that all that suﬃces to achieve boost- ing for density estimation is a weak learner in the original boosting theory sense, that is, an oracle that supplies classiﬁers. we provide converge rates that comply with boosting requirements, being better and / or relying on substantially weaker assumptions than the state of the art. one of our rates is to our knowledge the ﬁrst to rely on not just weak but also empirically testable assumptions. we show that the model ﬁt belongs to exponential families, and obtain in the course of our results a variational characterization of f -divergences better than f -gan’s. experimental results on several simulated problems display signiﬁcantly better results than adagan during early boosting rounds, in particular for mode capture, and using architectures less than the ﬁfth’s of adagan’s size. ∗data61 & the australian national university †data61, the australian national university & the university of sydney 1 1 "
1803.08181,"— 3d lidars and 2d cameras are increasingly being used alongside each other in sensor rigs for perception tasks. before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. a vast majority of existing calibration techniques require signiﬁcant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. we address this gap with calibnet: a self-supervised deep network capable of automatically estimating the 6-dof rigid body transformation between a 3d lidar and a 2d camera in real-time. calibnet alleviates the need for calibration targets, thereby resulting in signiﬁcant savings in calibration efforts. during training, the network only takes as input a lidar point cloud, the corresponding monocular image, and the camera calibration matrix k. at train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration ganesh iyer, karnik ram r., and k. madhava krishna are with the robotics research center at the international institute of information tech- nology, hyderabad, india. j. krishna murthy is with the montreal institute for learning algorithms, universit´e de montr´eal, quebec, canada. authors’ email: giyer2309@gmail.com, karnikram@gmail.com, mkrishna@iiit.ac.in parameters, for example). instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. calibnet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration param- eters for a wide range of mis-calibrations, without requiring retraining or domain adaptation. the project page is hosted at https://epiception.github.io/calibnet "
1803.08182," generative adversarial networks (gans) are becoming popular choices for unsu- pervised learning. at the same time there is a concerted eﬀort in the machine learning community to expand the range of tasks in which learning can be applied as well as to utilize methods from other disciplines to accelerate learning. with this in mind, in the current work we suggest ways to enforce given constraints in the output of a gan both for interpolation and extrapolation. the two cases need to be treated diﬀerently. for the case of interpolation, the incorporation of constraints is built into the train- ing of the gan. the incorporation of the constraints respects the primary game- theoretic setup of a gan so it can be combined with existing algorithms. however, it can exacerbate the problem of instability during training that is well-known for gans. we suggest adding small noise to the constraints as a simple remedy that has performed well in our numerical experiments. the case of extrapolation (prediction) is more involved. first, we employ a modiﬁed interpolation training process that uses noisy data but does not necessarily enforce the constraints during training. second, the resulting modiﬁed interpolator is used for extrapolation where the constraints are enforced after each step through projection on the space of constraints. 1 "
1803.08184,"—one of the greatest challenges in applying compres- sive sensing (cs) signal processing techniques to electromagnetic imaging applications is designing a sensing matrix that has good reconstruction capabilities. compressive reﬂector antennas (cra) are a class of antennas that have been shown to provide enhanced image reconstruction performance over traditional reﬂector antennas (tra) when cs techniques are employed. in this paper, we present a uniﬁed cra design method, which considers both the sensing capacity and efﬁciency of the antenna, and can be used for both compressive imaging and multiple- input multiple-output (mimo) communication applications. the uniﬁed design method is assessed for a cra conﬁguration in which dielectric scatterers are added to the surface of a tra. the design results demonstrate the ability of the uniﬁed design method to enhance the cs reconstruction capabilities of the cra. index terms—compressive sensing, antenna design, coded apertures "
1803.08185," the contribution of line-of-sight peculiar velocities to the observed redshift of objects breaks the translational symmetry of the underlying theory, modifying the predicted 2-point functions. these ‘wide angle eﬀects’ have mostly been studied using linear perturbation theory in the context of the multipoles of the correlation function and power spectrum . in this work we present the ﬁrst calculation of wide angle terms in the zeldovich approximation, which is known to be more accurate than linear theory on scales probed by the next generation of galaxy surveys. we present the exact result for dark matter and perturbatively biased tracers as well as the small angle expansion of the conﬁguration- and fourier-space two-point functions and the connection to the multi-frequency angular power spectrum. we compare diﬀerent deﬁnitions of the line- of-sight direction and discuss how to translate between them. we show that wide angle terms can reach tens of percent of the total signal in a measurement at low redshift in some approximations, and that a generic feature of wide angle eﬀects is to slightly shift the baryon acoustic oscillation scale. "
1803.08186,"—it is well established in the compressive sensing (cs) literature that sensing matrices whose elements are drawn from independent random distributions exhibit enhanced recon- struction capabilities. in many cs applications, such as elec- tromagnetic imaging, practical limitations on the measurement system prevent one from generating sensing matrices in this fashion. although one can usually randomized the measurements to some degree, these sensing matrices do not achieve the same reconstruction performance as the truly randomized sensing matrices. in this paper, we present a novel method, based upon capacity maximization, for designing sensing matrices with enhanced block-sparse signal reconstruction capabilities. through several numerical examples, we demonstrate how our method signiﬁcantly enhances reconstruction performance. index terms—compressive sensing, block compressive sensing, sensing matrix design, nonconvex optimization "
1803.08188,"—the fact that millimeter wave (mmwave) com- is usually perceived as a munication needs to be directional challenge; it enables efﬁcient secret key sharing that are unconditionally secure from passive eavesdroppers, by building on packet erasures. we showcase the potential of our approach in two setups: beam sweeping to align receiver/transmitter pairs, and vehicle platooning. we show that in the ﬁrst case, we can establish a few hundred secret bits at virtually no additional transmission cost; and in the second case, we can potentially establish keys in the order of tenths of mbps. these ﬁrst results are based on some simplifying assumptions, yet we believe they give incentives to further explore such techniques. "
1803.08189,"—in an internet-of-things system where status data are collected from sensors and actuators for time-critical appli- cations, the freshness of data is vital and can be quantiﬁed by the recently proposed age-of-information (aoi) metric. in this paper, we ﬁrst consider a general scenario where multiple terminals share a common channel to transmit or receive randomly gener- ated status packets. the optimal scheduling problem to minimize aoi is formulated as a restless multi-armed bandit problem. to solve the problem efﬁciently, we derive the whittle’s index in closed-form and establish the indexability thereof. compared with existing work, we extend the index policy for aoi opti- mization to incorporate stochastic packet arrivals and optimal packet management (buffering the latest packet). inspired by the index policy which has near-optimal performance but is centralized by nature, a decentralized status update scheme, i.e., the index-prioritized random access policy (ipra), is further proposed, achieving universally near-optimal aoi performance and outperforming state-of-the-arts in the literature. index terms—internet-of-things, age-of-information, markov decision process, restless multi-armed bandit, whittle’s index, random access "
1803.08190,". in this paper, we address the problem of estimating a 3d human pose from a single image, which is important but difﬁcult to solve due to many rea- sons, such as self-occlusions, wild appearance changes, and inherent ambiguities of 3d estimation from a 2d cue. these difﬁculties make the problem ill-posed, which have become requiring increasingly complex estimators to enhance the performance. on the other hand, most existing methods try to handle this prob- lem based on a single complex estimator, which might not be good solutions. in this paper, to resolve this issue, we propose a multiple-partial-hypothesis-based framework for the problem of estimating 3d human pose from a single image, which can be ﬁne-tuned in an end-to-end fashion. we ﬁrst select several joint groups from a human joint model using the proposed sampling scheme, and esti- mate the 3d poses of each joint group separately based on deep neural networks. after that, they are aggregated to obtain the ﬁnal 3d poses using the proposed robust optimization formula. the overall procedure can be ﬁne-tuned in an end- to-end fashion, resulting in better performance. in the experiments, the proposed framework shows the state-of-the-art performances on popular benchmark data sets, namely human3.6m and humaneva, which demonstrate the effectiveness of the proposed framework. keywords: 3d human pose estimation, multiple-partial-hypothesis-based model, articulated pose estimation "
1803.08192,". we study the relationship between singularity categories and relative singularity categories and discuss constructions of diﬀerential graded algebras of relative singularity categories. as consequences, we obtain structural results, which are known or generalise known results, on singularity categories of algebras with radical square zero, of non-commutative deformations of kleinian singularities, of sl3(c)-quotient singularities and of gorenstein toric threefolds. keywords: relative singularity category, singularity category, dg algebra, cluster category, quotient singularity msc 2010: 14b05, 16e45, 18e30 8 1 0 2   r a   m 2 2     ] . g a h t a m [     1 v 2 9 1 8 0 . 3 0 8 1 : v i x r a contents "
1803.08193,". this paper proposes new semantics for nondeterministic pro- gram execution, replacing the standard relational semantics for propo- sitional dynamic logic (pdl). under these new semantics, program ex- ecution is represented as fundamentally deterministic (i.e., functional), while nondeterminism emerges as an epistemic relationship between the agent and the system: intuitively, the nondeterministic outcomes of a given process are precisely those that cannot be ruled out in advance. we formalize these notions using topology and the framework of dynamic topological logic (dtl) [1]. we show that dtl can be used to interpret the language of pdl in a manner that captures the intuition above, and moreover that continuous functions in this setting correspond exactly to deterministic processes. we also prove that certain axiomatizations of pdl remain sound and complete with respect to the corresponding classes of dynamic topological models. finally, we extend the framework to incorporate knowledge using the machinery of subset space logic [2], and show that the topological interpretation of public announcements as given in [3] coincides exactly with a natural interpretation of test programs. 1 "
1803.08196," the relationship between dust polarization and extinction was determined for the cold dense starless molecular cloud core fest 1-457 based on the background star polarimetry of dichroic extinction at near-infrared wavelengths. owing to the known (three-dimensional) magnetic ﬁeld structure, the observed polarizations from the core were corrected by considering (a) the subtraction of the ambient po- larization component, (b) the depolarization eﬀect of inclined distorted magnetic ﬁelds, and (c) the magnetic inclination angle of the core. after these corrections, a linear relationship between polarization and extinction was obtained for the core in the range up to av ≈ 20 mag. the initial polarization vs. extinction diagram changed dramatically after the corrections of (a) to (c), with the correlation coef- ﬁcient being reﬁned from 0.71 to 0.79. these corrections should aﬀect the theo- retical interpretation of the observational data. the slope of the ﬁnally obtained polarization–extinction relationship is ph /eh−ks = 11.00 ± 0.72 % mag−1, which is close to the statistically estimated upper limit of the interstellar polarization eﬃ- ciency (jones 1989). this consistency suggests that the upper limit of interstellar polarization eﬃciency might be determined by the observational viewing angle to- ward polarized astronomical objects. 1 "
1803.08198," we propose and analyze a new stochastic gradient method, which we call stochastic unbiased curvature-aided gradient (sucag), for ﬁnite sum optimization problems. sucag constitutes an unbiased total gradient tracking technique that uses hessian information to accelerate con- vergence. we analyze our method under the general asynchronous model of computation, in which functions are selected inﬁnitely often, but with delays that can grow sublinearly. for strongly convex problems, we establish linear convergence for the sucag method. when the initialization point is suﬃciently close to the optimal solution, the established convergence rate is only dependent on the condition number of the problem, making it strictly faster than the known rate for the saga method. furthermore, we describe a markov-driven approach of implementing the sucag method in a distributed asynchronous multi-agent setting, via gossiping along a random walk on the communication graph. we show that our analysis applies as long as the undirected graph is connected and, notably, establishes an asymptotic linear convergence rate that is robust to the graph topology. numerical results demonstrate the merit of our algorithm over existing methods. 1 "
1803.08199," we detail tentative detections of low-frequency carbon radio recombination lines from within the orion molecular cloud complex observed at 99–129 mhz. these tentative detections include one alpha transition and one beta transition over three locations and are located within the diﬀuse regions of dust observed in the infrared at 100 µm, the hα emission detected in the optical, and the synchrotron radiation observed in the radio. with these observations, we are able to study the radiation mechanism transition from collisionally pumped to radiatively pumped within the hii regions within the orion molecular cloud complex. keywords: astrochemistry – atomic data – atomic processes – ism: hii regions – ism: atoms "
1803.08200," we consider coordinate descent (cd) methods with exact line search on convex quadratic problems. our main focus is to study the performance of the cd method that use random permutations in each epoch and compare it to the performance of the cd methods that use deterministic orders and random sampling with replacement. we focus on a class of convex quadratic problems with a diagonally dominant hessian matrix, for which we show that using random permutations instead of random with-replacement sampling improves the performance of the cd method in the worst-case. furthermore, we prove that as the hessian matrix becomes more diagonally dominant, the performance improvement attained by using random permuta- tions increases. we also show that for this problem class, using any ﬁxed deterministic order yields a superior performance than using random permutations. we present detailed theoretical analyses with respect to three diﬀerent convergence criteria that are used in the literature and support our theoretical results with numerical experiments. 1 "
1803.08201,". the suslin hypothesis states that there are no nonseparable complete dense linear orderings without endpoints which have the countable chain condition. zf + ad+ + v = l(p(r)) proves the suslin hypothesis. in particular, if l(r) |= ad, then l(r) satisﬁes the suslin hypothesis, which answers a question of foreman. "
1803.08202," a wide range of human-robot collaborative applications in industry, search and rescue operations, healthcare, and social interactions require an autonomous robot to follow its human companion. different operating mediums and applications pose diverse challenges by adding constraints on the choice of sensors, the degree of autonomy, and dynamics of the person following robot. researchers have addressed these challenges in many ways and contributed to the development of a large body of literature. this paper provides a comprehensive overview of the literature by categorizing different aspects of person-following by autonomous robots. also, the corresponding operational challenges are identiﬁed based on various design choices for ground, underwater, and aerial scenarios. in addition, state-of-the-art methods for perception, planning, control, and interaction are elaborately discussed, and their feasibilities are evaluated in terms of standard operational and performance metrics. furthermore, several prospective application areas are identiﬁed, and open problems are highlighted for future research. keywords person following robot, human-robot interaction "
1803.08203," while training error of most deep neural networks degrades as the depth of the network increases, residual networks appear to be an exception. we show that the main reason for this is the lyapunov stability of the gradient descent algorithm: for an arbitrarily chosen step size, the equilibria of the gradient descent are most likely to remain stable for the parametrization of residual networks. we then present an architecture with a pair of residual networks to approximate a large class of functions by decomposing them into a convex and a concave part. some parameters of this model are shown to change little during training, and this imper- fect optimization prevents overﬁtting the data and leads to solutions with small lipschitz constants, while providing clues about the generalization of other deep networks. "
1803.08204," we investigate, in the probe limit, the negative refraction in the generalized superconductors with the born-infeld electrodynamics. we observe that the system has a negative depine-lakhtakia index in the superconducting phase at small frequencies and the greater the born-infeld corrections the larger the range of frequencies or the range of temperatures for which the negative refraction occurs. furthermore, we ﬁnd that the tunable born-infeld parameter can be used to improve the propagation of light in the holographic setup. our analysis indicates that the born-infeld electrodynamics plays an important role in determining the optical properties of the boundary theory. pacs numbers: 11.25.tq, 04.70.bw, 74.20.-z 8 1 0 2   r a   m 2 2     ] h t - p e h [     1 v 4 0 2 8 0 . 3 0 8 1 : v i x r a ∗ panqiyuan@126.com † corresponding author at hwyu@hunnu.edu.cn ‡ jljing@hunnu.edu.cn 2 i. "
1803.08208," recent years have witnessed many exciting achievements for object detection using deep learning techniques. de- spite achieving signiﬁcant progresses, most existing de- tectors are designed to detect objects with relatively low- quality prediction of locations, i.e., often trained with the threshold of intersection over union (iou) set to 0.5 by de- fault, which can yield low-quality or even noisy detections. it remains an open challenge for how to devise and train a high-quality detector that can achieve more precise lo- calization (i.e., iou>0.5) without sacriﬁcing the detection performance. in this paper, we propose a novel single- shot detection framework of bidirectional pyramid net- works (bpn) towards high-quality object detection, which consists of two novel components: (i) a bidirectional fea- ture pyramid structure for more effective and robust fea- ture representations; and (ii) a cascade anchor reﬁnement to gradually reﬁne the quality of predesigned anchors for more effective training. our experiments showed that the proposed bpn achieves the best performances among all the single-stage object detectors on both pascal voc and ms coco datasets, especially for high-quality detections. "
1803.08209,"— motivated by a high demand for automated in- spection of civil infrastructure, this paper presents a novel de- sign and development of a tank-like robot for structural health monitoring. unlike most existing magnetic wheeled mobile robot designs, which is only suitable for climbing on ﬂat steel surface, our proposed tank-like robot design uses reciprocating mechanism and roller-chains to make it capable of climbing on different structural shapes (e.g., cylinder, cube) with coated or non-coated steel surfaces. the robot can transition from one surface to the other (e.g., from ﬂat surface to curving surface). taking into account of several strict considerations (including tight dimension, efﬁcient adhesion and climbing ﬂexibility) to adapt with variety shapes of steel structures, a prototype tank- like robot incorporating multiple sensors (hall effects, sonars, inertial measurement unit and camera), has been developed. rigorous analysis of robot kinematics, adhesion force, sliding failure and turn-over failure has been conducted to demonstrate the stability of the proposed design. experimental results and ﬁeld deployments conﬁrm the adhesion and climbing capability of the developed robot. "
1803.08210," new expressions are given for the fourier expansions of non-holomorphic eisenstein series with weight k. among other applications, this leads to non-holomorphic analogs of formulas of ramanujan, grosswald and berndt containing eichler integrals of holomorphic eisenstein series. "
1803.08212," using a lattice model of polymers in a tube, we deﬁne one way to characterise different conﬁgurations of a given knot as either “local” or “non-local” and, for several ring polymer models, we provide both theoretical and numerical evidence that, at equilibrium, the non- local conﬁgurations are more likely than the local ones. these characterisations are based on a standard approach for measuring the “size” of a knot within a knotted polymer chain. the method involves associating knot-types to subarcs of the chain, and then identifying a knotted subarc with minimal arclength; this arclength is then the knot-size. if the resulting knot-size is small relative to the whole length of the chain, then the knot is considered to be localised or “local”. if on the other hand the knot-size is comparable to the length of the chain, then the knot is considered to be “non-local”. using this deﬁnition, we establish that all but exponentially few sufﬁciently long self- avoiding polygons (closed chains) in a tubular sublattice of the simple cubic lattice are “non-locally” knotted. this is shown to also hold for the case when the same polygons are subject to an external tensile force, as well as in the extreme case when they are as compact as possible (no empty lattice sites). we also provide numerical evidence for small tube sizes that at equilibrium non-local knotting is more likely than local knotting, regardless of the strength of the stretching or compressing force. we note however that because of the tube conﬁnement, the occurrence of non-local knotting in walks (open chains) is signiﬁcantly different than for polygons. the relevance of these results to recent experiments involving dna knots in solid-state nanopores is also discussed. "
1803.08214," inspired by some recent works of tippett-tsang and mallary-khanna-price, we present a new spacetime model containing closed timelike curves (ctcs). this model is obtained postulating an ad hoc lorentzian metric on r4, which diﬀers from the minkowski metric only inside a spacetime region bounded by two concentric tori. the resulting spacetime is topologically trivial, free of curvature singularities and is both time and space orientable; besides, the inner region enclosed by the smaller torus is ﬂat and displays geodesic ctcs. our model shares some similarities with the time machine of ori and soen but it has the advantage of a higher symmetry in the metric, allowing for the explicit computation of a class of geodesics. the most remarkable feature emerging from this computation is the presence of future-oriented timelike geodesics starting from a point in the outer minkowskian region, moving to the inner spacetime region with ctcs, and then returning to the initial spatial position at an earlier time; this means that time travel to the past can be performed by free fall across our time machine. the amount of time travelled into the past is determined quantitatively; this amount can be made arbitrarily large keeping non-large the proper duration of the travel. an important drawback of the model is the violation of the classical energy conditions, a common feature of most time machines. other problems emerge from our computations of the required (negative) energy densities and of the tidal accelerations; these are small only if the time machine is gigantic. keywords: general relativity, closed timelike curves, time machines, energy conditions. ams subject classiﬁcations: 83c05, 83c20 . pacs: 04.20.cv, 04.20.gz, 04.90.+e . 8 1 0 2   r a   m 8 2     ] c q - r g [     2 v 4 1 2 8 0 . 3 0 8 1 : v i x r a 1 "
1803.08215," we present a drag force model for evolving chaotic few-body interactions with the inclusion of orbital energy losses, such as tidal dissipation and gravitational wave (gw) emission. the main eﬀect from such losses is the formation of two-body captures, that for compact objects result in gw mergers, and for stars lead to either compact binaries, mergers or disruptions. studying the inclusion of energy loss terms in few-body interactions is therefore likely to be important for modeling and understanding the variety of transients that soon will be observed by current and upcoming surveys. however, including especially tides in few-body codes has been shown to be technically diﬃcult and computationally heavy, which has lead to very few systematic tidal studies. in this paper we derive a drag force term that can be used to model the eﬀects from tidal, as well as other, energy losses in few-body interactions, if the two-body orbit averaged energy loss is known a priori. this drag force model is very fast to evolve, and gives results in agreement with other approaches, including the impulsive and aﬃne tide approximations. key words: gravitation – methods: numerical – stars: black holes – stars: kinematics and dynamics "
1803.08216,". for a smooth projective variety x, we consider when the diagonal ∆x is nef as a cycle on x × x. in particular, we give a classiﬁcation of complete intersections and smooth del pezzo varieties where the diagonal is nef. we also study the nefness of the diagonal for spherical varieties. "
1803.08221,"—secure message dissemination is an important issue in vehicular networks, especially considering the vulnerability of vehicle to vehicle (v2v) message dissemination to malicious attacks. traditional security mechanisms, largely based on mes- sage encryption and key management, can only guarantee secure message exchanges between known source and destination pairs. in vehicular networks however, every vehicle may learn its surrounding environment and contributes as a source, while in the meantime act as a destination or a relay of information from other vehicles, message exchanges often occur between “stranger” vehicles. this makes secure message dissemination against malicious tampering much more intricate. for secure message dissemination in vehicular networks against insider attackers, who may tamper the content of the disseminated messages, ensuring the consistency and integrity of the transmit- ted messages becomes a major concern that traditional message encryption and key management based approaches fall short to provide. however, it is challenging for a vehicle to distinguish which message is true when its received messages from multiple nearby vehicles are conﬂicting. in this paper, by incorporating the underlying network topology information, we propose an optimal decision algorithm that is able to maximize the chance of making a correct decision on the message content, assuming the prior knowledge of the percentage of malicious vehicles in the network. furthermore, a novel heuristic decision algorithm is proposed that can make decisions without the aforementioned knowledge of the percentage of malicious vehicles. simulations are conducted to compare the security performance achieved by our proposed decision algorithms with that achieved by existing ones that do not consider or only partially consider the topological information, to verify the effectiveness of the algorithms. our results show that by incorporating the network topology information, the security performance can be much improved. this work shed light on the optimum algorithm design for secure message dissemination. index terms—vehicular networks, security, message dissemi- nation, decision algorithm. "
1803.08223," we investigate theoretically the goos-h¨anchen (gh) shift of a p-polarized terahertz beam incident on a 2d material surface with complex conductiv- ity. taking monolayer graphene to be the model material, we determine the dependence of gh shifts on the fermi level and incident frequency. both spatial and angular gh shifts are present. for both gh shifts in general, we ﬁnd that increasing the fermi level shifts the incident angle at which the maximum gh shifts arise. moreover, we see that at higher frequencies, the amount of beam shift decreases with the fermi level when the incident frequency is changed. at lower frequencies, however, the shift becomes pro- portional with the fermi level. upon obtaining the measurable shifts, the angular gh shift dominates the spatial gh shift given appropriate experi- mental parameters. our results may pave the way for these material’s use in optoelectronics devices, and fundamentally, to determine properties of 2d materials with complex conductivity. keywords: optics at surfaces, physical optics, nanomaterials, materials and process characterization "
1803.08224,". we study a new construction of bodies from a given convex body in rn which are isomorphic to (weighted) ﬂoating bodies. we establish several properties of this new construction, including its relation to p-aﬃne surface areas. we show that these bodies are related to ulam’s long-standing ﬂoating body problem which asks whether euclidean balls are the only bodies that can ﬂoat, without turning, in any orientation. "
1803.08225,". we present a box-free bottom-up approach for the tasks of pose estimation and instance segmentation of people in multi-person im- ages using an eﬃcient single-shot model. the proposed personlab model tackles both semantic-level reasoning and object-part associations using part-based modeling. our model employs a convolutional network which learns to detect individual keypoints and predict their relative displace- ments, allowing us to group keypoints into person pose instances. fur- ther, we propose a part-induced geometric embedding descriptor which allows us to associate semantic person pixels with their corresponding person instance, delivering instance-level person segmentations. our sys- tem is based on a fully-convolutional architecture and allows for eﬃcient inference, with runtime essentially independent of the number of people present in the scene. trained on coco data alone, our system achieves coco test-dev keypoint average precision of 0.665 using single-scale inference and 0.687 using multi-scale inference, signiﬁcantly outperform- ing all previous bottom-up pose estimation systems. we are also the ﬁrst bottom-up method to report competitive results for the person class in the coco instance segmentation task, achieving a person category av- erage precision of 0.417. keywords: person detection and pose estimation, segmentation and grouping. "
1803.08228,"—future terabit networks are committed to dra- matically improving big data motion between geographically dispersed hpc data centers. the scientiﬁc community takes advantage of the terabit networks such as doe’s esnet and accelerates the trend to build a small world of collaboration between geospatial hpc data centers. it improves information and resource sharing for joint simulation and analysis between the hpc data centers. in this paper, we propose to build scispace (scientiﬁc collaboration workspace) for collaborative data centers. it provides a global view of information shared from multiple geo-distributed hpc data centers under a single workspace. scispace supports native data-access to gain high- performance when data read or write is required in native data center namespace. it is accomplished by integrating a metadata export protocol. to optimize scientiﬁc collaborations across hpc data centers, scispace implements search and discovery service. to evaluate, we conﬁgured two geo-distributed small-scale hpc data centers connected via high-speed inﬁniband network, equipped with lustrefs. we show the feasibility of scispace using real scientiﬁc datasets and applications. the evaluation results show average 36% performance boost when the proposed native-data access is employed in collaborations. "
1803.08229," in this paper, an algorithm based on polyphase matrix for constructing a pair of orthogonal wavelet frames is suggested, and a general form for all orthogonal tight wavelet frames on local ﬁelds of positive characteristic is described. moreover, we investigate their properties by means of the fourier transform. keywords wavelet frame; orthogonality; framelet symbol; polyphase matrix; extension principle; fourier transform; local ﬁeld mathematics subject classiﬁcation (2000) primary 42c40; secondary 42c15. 43a70. 11s85 "
1803.08230, we study the conditions for the adiabatic resonant conversion of the cold dark matter (cdm) axions into photons in existence of the astrophysically sourced strong magnetic ﬁelds such as those in the neutron star magnetosphere. we demonstrate the possibility that the forthcoming radio telescopes such as the ska (square kilometre array) can probe those photon signals from the cdm axions. 1 
1803.08234," we present the discovery of new star clusters in the central plane region (|l| < 30◦ and |b| < 6◦) of the milky way. in order to overcome the extinction problem and the spatial limit of previous surveys, we use the wide-ﬁeld infrared survey explorer (wise) data to ﬁnd clusters. we also use other infrared survey data in the archive for additional analysis. we ﬁnd 923 new clusters, of which 202 clusters are embedded clusters. these clusters are concentrated toward the galactic plane and show a symmetric distribution with respect to the galactic latitude. the embedded clusters show a stronger concentration to the galactic plane than the non-embedded clusters. the new clusters are found more in the ﬁrst galactic quadrant, while previously known clusters are found more in the fourth galactic quadrant. the spatial distribution of the combined sample of known clusters and new clusters is approximately symmetric with respect to the galactic longitude. we estimate reddenings, distances, and relative ages of the 15 class a clusters using theoretical isochrones. ten of them are relatively old (age > 800 myr) and ﬁve are young (age ≈ 4 myr). keywords: catalogs — galaxy: disk — (galaxy:) open clusters and associations: general — galaxy: structure — infrared: stars 8 1 0 2   r a   m 2 2     ] . a g h p - o r t s a [     1 v 4 3 2 8 0 . 3 0 8 1 : v i x r a corresponding author: myung gyoon lee mglee@astro.snu.ac.kr ryujh@astro.snu.ac.kr 2 ryu & lee "
1803.08235," in our previous works, we have analyzed the evolution of bulk viscous matter dominated ¨a ˙a and also universe with a more general form for bulk viscous coeﬃcient, ζ = ζ0 + ζ1 carried out the dynamical system analysis. we found that the model reasonably describes the evolution of the universe if the viscous coeﬃcient is a constant. in the present work we are contrasting this model with the standard λcdm model of the universe using the bayesian method. we have shown that, even though the viscous model gives a reasonable back ground evolution of the universe, the bayes factor of the model indicates that, it is not so superior over the λcdm model, but have a slight advantage over it. ˙a a + ζ2 1 "
1803.08236,. we propose a uniﬁed approach to the study of isometries on algebras of vector- valued lipschitz maps and those of continuously diﬀerentiable maps by means of the notion of natural c(y )-valuezations that take values in unital commutative c ∗-algebras. a precise proof of a theorem of jarosz [17] is exhibited. 
1803.08237," the recent advancement of high-intensity lasers has made all-optical compton scattering become a promising way to produce ultra-short brilliant γ-rays in an ultra-compact system. however, so far achieved compton γ-ray sources are severely limited by low conversion efﬁciency (lower than 10−5) and spectral intensity (∼ 104 photons/0.1%bw). here we present a highly efﬁcient gamma photon emitter obtained by irradiating a high-intensity laser pulse on a miniature plasma device consisting of a plasma lens and a plasma mirror. this concept exploits strong spatiotemporal laser-shaping process and high-charge electron acceleration process in the plasma lens, as well as an efﬁcient nonlinear compton scattering process enabled by the plasma mirror. our particle-in-cell simulations demonstrate that in this novel scheme, brilliant γ-rays with very high conversion efﬁciency (higher than 10−2) and spectral intensity (∼ 109 photons/0.1%bw) can be achieved by employing currently available petawatt-class lasers with intensity of 1021 w/cm2. such efﬁcient and intense γ-ray sources would ﬁnd applications in wide-ranging areas. "
1803.08238,". we show that, for a closed orientable n-manifold, with n not congruent to 3 modulo 4, the existence of a cr-regular embedding into complex (n − 1)-space ensures the existence of a totally real embedding into complex n-space. this implies that a closed orientable (4k + 1)-manifold with non-vanishing kervaire semi-characteristic possesses no cr-regular embedding into complex 4k-space. we also pay special attention to the cases of cr-regular embeddings of spheres and of simply-connected 5-manifolds. "
1803.08239," we have studied the scenario of baryogenesis via leptogenesis in an a4 ﬂavor symmetric framework considering type i seesaw as the origin of neutrino mass. because of the presence of the ﬁfth generation right handed neutrino the model naturally generates non-zero reactor mixing angle. we have considered two vev alignments for the extra ﬂavon η and studied the consequences in detail. as a whole the additional ﬂavon along with the extra right handed neutrinos allow us to study thermal leptogenesis by the decay of the lightest right handed neutrino present in the model. we have computed the matter-antimatter asymmetry for both ﬂavor dependent and ﬂavor independent leptogenesis by considering a considerably wider range of right handed neutrino mass. finally, we correlate the baryon asymmetry of the universe (bau) with the model parameters and light neutrino masses. pacs numbers: 14.60.pq, 11.30.qc 8 1 0 2   r a   m 2 2     ] h p - p e h [     1 v 9 3 2 8 0 . 3 0 8 1 : v i x r a ∗electronic address: ananyam@tezu.ernet.in †electronic address: mkdas@tezu.ernet.in ‡electronic address: jks@tezu.ernet.in 1 i. "
1803.08240," many of the leading approaches in language mod- eling introduce novel, complex and specialized architectures. we take existing state-of-the-art word level language models based on lstms and qrnns and extend them to both larger vocabular- ies as well as character-level granularity. when properly tuned, lstms and qrnns achieve state- of-the-art results on character-level (penn tree- bank, enwik8) and word-level (wikitext-103) datasets, respectively. results are obtained in only 12 hours (wikitext-103) to 2 days (enwik8) us- ing a single modern gpu. "
1803.08243,"—speech derverberation using a single microphone is addressed in this paper. motivated by the recent success of the fully convolutional networks (fcn) in many image process- ing applications, we investigate their applicability to enhance the speech signal represented by short-time fourier transform (stft) images. we present two variations: a “u-net” which is an encoder-decoder network with skip connections and a generative adversarial network (gan) with u-net as generator, which yields a more intuitive cost function for training. to evaluate our method we used the data from the reverb challenge, and compared our results to other methods under the same conditions. we have found that our method outperforms the competing methods in most cases. "
1803.08244,". the task of three-dimensional (3d) human pose estimation from a single image can be divided into two parts: (1) two-dimensional (2d) human joint detection from the image and (2) estimating a 3d pose from the 2d joints. herein, we focus on the second part, i.e., a 3d pose estimation from 2d joint locations. the problem with existing methods is that they require either (1) a 3d pose dataset or (2) 2d joint locations in consecutive frames taken from a video sequence. we aim to solve these problems. for the ﬁrst time, we propose a method that learns a 3d human pose without any 3d datasets. our method can predict a 3d pose from 2d joint locations in a single image. our system is based on the generative adversarial networks, and the networks are trained in an unsupervised manner. our primary idea is that, if the network can predict a 3d human pose correctly, the 3d pose that is projected onto a 2d plane should not collapse even if it is rotated perpendicularly. we evaluated the performance of our method using human3.6m and the mpii dataset and showed that our network can predict a 3d pose well even if the 3d dataset is not available during training. keywords: 3d human pose estimation, unsupervised learning "
1803.08246," based on the exact analytical time-varying solution of a conducting ring embedded in a textured static/dynamic electric ﬁeld, we investigate the time-resolved quantum spin transport in the struc- ture. it is shown that the interference patterns are governed by not only the aharonov-casher phase but also the instantaneous phase diﬀerence of spin precession. this spin precessing phase has strong eﬀect on the spin conductance and results in a high-frequency oscillation mode with respect to the strength of electric ﬁeld, especially in the weak value cases. our studies suggest that a quasi-static moderate ﬁeld is favor to modulate the spin polarization orientation of incident electron. 8 1 0 2   r a   m 3 2     ] l l a h - s e m . t a m - d n o c [     2 v 6 4 2 8 0 . 3 0 8 1 : v i x r a 1 "
1803.08249,". in this paper, we study the mixed dispersion fourth order nonlinear helmholtz equation ∆2u − β∆u + αu = γ|u|p−2u in rn , for positive, bounded and zn -periodic functions γ in the following three cases: (a) α < 0, β ∈ r or (b) α > 0, β < −2√α or (c) α = 0, β < 0. using the dual method of evéquoz and weth, we ﬁnd solutions to this equation and establish some of their qualitative properties. "
1803.08250,"—millimeter wave (mmwave) technology is expected to dominate the future 5g networks mainly due to large spectrum available at these frequencies. however, coverage deteriorates signiﬁcantly at mmwave frequencies due to higher path loss, es- pecially for the non-line-of-sight (nlos) scenarios. in this work, we explore the use of passive reﬂectors for improving mmwave signal coverage in nlos indoor areas. measurements are carried out using the pxi-based mmwave transceiver platforms from national instruments operating at 28 ghz, and the results are compared with the outcomes of ray tracing (rt) simulations in a similar environment. for both the measurements and ray tracing simulations, different shapes of metallic passive reﬂectors are used to observe the coverage (signal strength) statistics on a receiver grid in an nlos area. for a square metallic sheet reﬂector of size 24 × 24 in2 and 33 × 33 in2, we observe a signiﬁcant increase in the received power in the nlos region, with a median gain of 20 db when compared to no reﬂector case. the cylindrical reﬂector shows more uniform coverage on the receiver grid as compared to ﬂat reﬂectors that are more directional. index terms—coverage, electromagnetic waves, mmwave, non-line-of-sight (nlos), pxi, ray tracing, reﬂector. "
1803.08251," with the wide adoption of the multi-community setting in many popular social media platforms, the increasing user en- gagements across multiple online communities warrant re- search attention. in this paper, we introduce a novel analogy between the movements in the cyber space and the physical space. this analogy implies a new way of studying human on- line activities by modelling the activities across online com- munities in a similar fashion as the movements among loca- tions. first, we quantitatively validate the analogy by com- paring several important properties of human online activi- ties and physical movements. our experiments reveal striking similarities between the cyber space and the physical space. next, inspired by the established methodology on human mobility in the physical space, we propose a framework to study human “mobility” across online platforms. we discover three interesting patterns of user engagements in online com- munities. furthermore, our experiments indicate that people with different mobility patterns also exhibit divergent prefer- ences to online communities. this work not only attempts to achieve a better understanding of human online activities, but also intends to open a promising research direction with rich implications and applications. "
1803.08252,"—unmanned aerial vehicles (uavs) are envisioned to be an integral part of future 5g communication systems. the agile nature of uavs for serving users at different locations can help to dynamically optimize coverage and quality-of-service (qos) in future networks. in this work, we explore the small scale temporal and spatial characteristics of mmwave air-to- ground (ag) line-of-sight (los) propagation channels at 28 ghz in different environmental scenarios: dense-urban, suburban, rural, and over sea using omni-directional antennas employing wireless insite ray tracing software. we classify the received multipath components (mpcs) into persistent and non-persistent components. the small scale temporal and spatial characteristics of the ag propagation channel are found to be dependent on the scatterer properties: number, distribution, and geometry. additionally, clustering of mpcs in the time and spatial domain for different environments is found to be dependent on the scatterer properties and receiver sensitivity. when the height of the uav is comparable to the height of the scatterers, we observe large temporal and angular spreads. index terms—5g communications, air-to-ground (ag), drone, line-of-sight (los) channel, mmwave communications, multipath components (mpcs), unmanned aerial vehicle (uav). sented in [12], while [13] provides a comprehensive survey on ag propagation channels for uavs. this paper is an extension of [5], where, time dispersion and large scale propagation characteristics at 28 ghz are studied for different environmental scenarios. here, small scale characteristics of the mmwave propagation channel in both time and spatial domains are analyzed (in contrast to [5] which considers only time domain characteristics) in dense- urban, suburban, rural and over sea scenarios using wireless insite ray tracing software. the analysis of the mmwave ag propagation channel reveals that the received multipath com- ponents (mpcs) can be grouped into persistent (comprising of los and ground reﬂected component (grc)) and non- persistent (comprising of all other non-los (nlos) compo- nents). the characteristics of persistent components are mainly dependent on the geometry of the setup and are predictable for a given trajectory of the uav. on the other hand, statistics of the non-persistent components follow a random process along the uav trajectory and are dependent on the properties of the terrain cover in addition to the geometry of the setup. "
1803.08254,". begin we study the wave equation in an interval with two linearly moving endpoints. we give the exact solution by a series formula, then we show that the energy of the solution decay at the rate 1/t. we also establish observability results, at one or two endpoints, in a sharp time. moreover, using the hilbert uniqueness method, we derive exact boundary controllability results. "
1803.08255," dropout represents a typical issue to be addressed when dealing with longitudinal studies. if the mechanism leading to missing information is non-ignorable, inference based on the observed data only may be severely biased. a frequent strategy to obtain reliable parameter estimates is based on the use of individual-speciﬁc random coeﬃ- cients that help capture sources of unobserved heterogeneity and, at the same time, deﬁne a reasonable structure of dependence between the longitudinal and the missing data process. we refer to elements in this class as random coeﬃcient based dropout models (rcbdms). we propose a dynamic, semi-parametric, version of the standard rcbdm to deal with discrete time to event. time-varying random coeﬃcients that evolve over time according to a non-homogeneous hidden markov chain are considered to model dependence between longitudinal responses recorded from the same subject. a separate set of random coeﬃcients is considered to model dependence between miss- ing data indicators. last, the joint distribution of the random coeﬃcients in the two equations helps describe the dependence between the two processes. to ensure model ﬂexibility and avoid unveriﬁable assumptions, we leave the joint distribution of the ran- dom coeﬃcients unspeciﬁed and estimate it via nonparametric maximum likelihood. the proposal is applied to data from the leiden 85+ study on the evolution of cognitive functioning in the elderly. keywords: dropout; finite mixture, latent markov model; missingness; nonparamet- 1 ric maximum likelihood. 1 "
1803.08256," many countries today have “country-centric mobile apps” which are mobile apps that are primarily used by residents of a specific country. many of these country-centric apps also include a location- based service which takes advantage of the smartphone’s api access to the smartphone’s current gps location. in this paper, we investi- gate how such country-centric apps with location-based services can be employed to study the diaspora associated with ethnic and cultural groups. our methodology combines gps hacking, auto- mated task tools for mobile phones, and ocr to generate migration statistics for diaspora. as a case study, we apply our methodology to wechat, an enor- mously popular app within china and among ethnic chinese world- wide. using wechat, we collect data about the chinese diaspora in 32 cities. we also collect data about chinese businesses in each of the cities using the google places api. the combined data provides interesting insights to the modern chinese diaspora and how it has changed in recent years. keywords chinese diaspora, mobile app, global migration "
1803.08257,". let p ⊂ a be an inclusion of σ-unital c*-algebras with a ﬁnite index in the sense of izumi. then we introduce the rokhlin property for a conditional expectation e from a onto p and show that if a is simple and satisﬁes any of the property (1) ∼ (12) listed in the below, and e has the rokhlin property, then so does p . (1) simplicity; (2) nuclearity; (3) c*-algebras that absorb a given strongly self-absorbing c*-algebra d; (4) c*-algebras of stable rank one; (5) c*-algebras of real rank zero; (6) c*-algebras of nuclear dimension at most n, where n ∈ z+; (7) c*-algebras of decomposition rank at most n, where n ∈ z+; (8) separable simple c*-algebras that are stably isomorphic to af algebras; (9) separable simple c*-algebras that are stably isomorphic to ai algebras; (10) separable simple c*-algebras that are stably isomorphic to at algebras; (11) separable simple c*-algebras that are stably isomorphic to sequential direct limits of one dimensional nccw complexes; (12) separable c*-algebras with strict comparison of positive elements. in particular, when α : g → aut(a) is an action of a ﬁnite group g on a with the rokhlin property in the sense of nawata, the properties (1) ∼ (12) are inherited to the ﬁxed point algebra aα and the crossed product algebra a ⋊α g from a. "
1803.08263," we report on the results from deep h i observations, performed with the karoo array telescope and with the karl g. jansky very large array of the loose galaxy group centred on the early-type galaxy ic 1459. the main result from our observations is the detection of a nearly continuous, 500-kpc long h i tail which crosses the entire group. earlier observations with the australia telescope compact array had shown the presence of a large h i tail in this galaxy group, but because of the much larger coverage of the new data, the full extent of this tail is now visible. the h i mass of this structure is 3.1 ± 0.3 × 109 m(cid:12). based on its morphology and kinematics, we conclude that the tail consists of gas stripped from ngc 7418 through tidal interactions, with ram-pressure aﬀects playing at most a minor role. optical images of the ic 1459 group do not show many indications that galaxy interactions are common in this group. the h i data reveal a very diﬀerent picture and show that almost all gas-rich galaxies in the ic 1459 group have a distorted h i distribution indicating that many interactions are occurring in this group. this high number of interactions shows that the processes that drive galaxy transformation are also occurring in fairly loose galaxy groups. key words. ism: evolution – galaxies: intergalactic medium – galaxies: interactions – galaxies: evolution "
1803.08264," purpose the data which is available to surgeons before, during and after surgery is steadily increasing in quantity as well as diversity. when plan- ning a patient’s treatment, this large amount of information can be diﬃcult to interpret. to aid in processing the information, new methods need to be found to present multi-modal patient data, ideally combining textual, imagery, temporal and 3d data in a holistic and context-aware system. methods we present an open-source framework which allows handling of pa- tient data in a virtual reality (vr) environment. by using vr technology, the workspace available to the surgeon is maximized and 3d patient data is ren- dered in stereo, which increases depth perception. the framework organizes the data into workspaces and contains tools which allow users to control, ma- nipulate and enhance the data. due to the framework’s modular design, it can easily be adapted and extended for various clinical applications. results the framework was evaluated by clinical personnel (77 participants). the majority of the group stated that a complex surgical situation is easier to comprehend by using the framework, and that it is very well suited for ed- ucation. furthermore, the application to various clinical scenarios - including the simulation of excitation-propagation in the human atrium - demonstrated the framework’s adaptability. as a feasibility study, the framework was used during the planning phase of the surgical removal of a large central carcinoma (cid:63) these authors contributed equally to this work. micha pfeiﬀer, stefanie speidel national center for tumor diseases, dresden, germany e-mail: micha.pfeiﬀer@nct-dresden.de matthias huber karlsruhe institute of technology, institute for anthropomatics and robotics hannes kenngott, anas preukschas, lisa bettscheider, beat m¨uller-stich heidelberg university hospital, department of general-, visceral- and transplant surgery 2 micha pfeiﬀer et al. fig. 1 left: traditionally, 2d slices of ct scans are used for planning. right: in complex scenarios such as this one, where the tumor (yellow) and the various vessel trees (veins, arteries and bile ducts) lie close together, a 3d visualization could greatly beneﬁt physicians in analyzing patient data (screenshot of the virtual reality application). from a patient’s liver. conclusion the clinical evaluation showed a large potential and high accep- tance for the vr environment in a medical context. the various applications conﬁrmed that the framework is easily extended and can be used in real-time simulation as well as for the manipulation of complex anatomical structures. keywords virtual reality · surgical planning · advanced medical visual- ization "
1803.08265,". the enumeration of planar maps equipped with an eulerian orientation has at- tracted attention in both combinatorics and theoretical physics since at least 2000. the case of 4-valent maps is particularly interesting: these orientations are in bijection with properly 3-coloured quadrangulations, while in physics they correspond to conﬁgurations of the ice model. we solve both problems – namely the enumeration of planar eulerian orientations and of 4-valent planar eulerian orientations – by expressing the associated generating functions as the inverses (for the composition of series) of simple hypergeometric series. using these ex- pressions, we derive the asymptotic behaviour of the number of planar eulerian orientations, thus proving earlier predictions of kostov, zinn-justin, elvey price and guttmann. this behaviour, µn/(n log n)2, prevents the associated generating functions from being d-ﬁnite. still, these generating functions are diﬀerentially algebraic, as they satisfy non-linear diﬀer- ential equations of order 2. diﬀerential algebraicity has recently been proved for other map problems, in particular for maps equipped with a potts model. our solutions mix recursive and bijective ingredients. in the 4-valent case, we also ob- serve an unexpected connection with the enumeration of maps equipped with a spanning tree that is internally inactive in the sense of tutte. this connection remains to be explained combinatorially. "
1803.08266," this paper analyzes the approximation properties of spaces of piece- wise tensor product polynomials over box meshes with a focus on ap- plication to isogeometric analysis (iga). the errors are measured in lebesgue norms. estimates of diﬀerent types are considered: local and global, with full or reduced sobolev seminorms. attention is also paid to the dependence on the degree and exponential convergence is proved for the approximation of analytic functions. 1 "
1803.08268,". we study the morse index of self-shrinkers for the mean curvature ﬂow and, more generally, of f -minimal hypersurfaces in a weighted euclidean space endowed with a convex weight. when the hypersurface is compact, we show that the index is bounded from below by an aﬃne function of its ﬁrst betti number. when the ﬁrst betti number is large, this improves index estimates known in literature. in the complete non-compact case, the lower bound is in terms of the dimension of the space of weighted square summable f -harmonic 1-forms; in particular, in dimension 2, the procedure gives an index estimate in terms of the genus of the surface. 8 1 0 2   r a   m 2 2     ] . g d h t a m [     1 v 8 6 2 8 0 . 3 0 8 1 : v i x r a contents "
1803.08269," a persistence diagram characterizes robust geometric and topological features in data. data, which will be treated here, are assumed to be drawn from a probability distribution and then the corresponding persistence diagrams have randomness. this paper reveals relationships between prob- ability distributions and persistence diagrams in the viewpoint of (1) the strong law of large numbers and the central limit theorem, (2) conﬁdence intervals, and (3) stability theorems via the persistence weighted gaussian kernel which is a statistical method for persistence diagrams. in numerical ex- periments for distributions, our method is compared against other statistical methods for persistence diagrams. "
1803.08270," rotation and magnetism are increasingly recognized as important phenomena in stellar evolution. surface magnetic ﬁelds from a few to 20,000 g have been observed and models have suggested that magnetohydrodynamic transport of angular momentum and chemical composition could explain the peculiar composition of some stars. stellar remnants such as white dwarfs have been observed with ﬁelds from a few to more than 109 g. we investigate the origin of and the evolution, on thermal and nuclear rather than dynamical time-scales, of an averaged large-scale magnetic ﬁeld throughout a star’s life and its coupling to stellar rotation. large-scale magnetic ﬁelds sustained until late stages of stellar evolution with conservation of magnetic ﬂux could explain the very high ﬁelds observed in white dwarfs. we include these eﬀects in the cambridge stellar evolution code using three time-dependant advection-diﬀusion equations coupled to the structural and composition equations of stars to model the evolution of angular momentum and the two components of the magnetic ﬁeld. we present the evolution in various cases for a 3 m⊙ star from the beginning to the late stages of its life. our particular model assumes that turbulent motions, including convection, favour small-scale ﬁeld at the expense of large-scale ﬁeld. as a result the large-scale ﬁeld concentrates in radiative zones of the star and so is exchanged between the core and the envelope of the star as it evolves. the ﬁeld is sustained until the end of the asymptotic giant branch, when it concentrates in the degenerate core. key words: (stars:) white dwarfs stars: evolution, stars: general, stars: magnetic ﬁelds, stars: rotation, "
1803.08273," we present a quantum algorithm for simulating the dynamics of hamiltonians that are not necessarily sparse. our algorithm is based on the assumption that the entries of the hamiltonian are stored in a data structure that allows for the eﬃcient preparation of states that encode the rows of the hamiltonian. we use a linear combination of quantum walks to achieve a poly-logarithmic dependence on the precision. the time complexity measured in nkhkpolylog(n, tkhk, 1/)), where t is the terms of circuit depth of our algorithm is o(t evolution time, n is the dimension of the system, and  is the error in the ﬁnal state, which we call precision. our algorithm can directly be applied as a subroutine for unitary hamiltonians √ n) dependence for both applications. and solving linear systems, achieving a eo( "
1803.08275,". many one-dimensional lattice particle models with open boundaries, like the paradigmatic asymmetric simple exclusion process (asep), have their stationary states represented in the form of a matrix product, with matrices that do not explicitly depend on the lattice site. in contrast, the stationary state of the open one-dimensional zero-range process (zrp) takes an inhomogeneous factorized form, with site-dependent probability weights. we show that in spite of the absence of correlations, the stationary state of the open zrp can also be represented in a matrix product form, where the matrices are site-independent, non-commuting and determined from algebraic relations resulting from the master equation. we recover the known distribution of the open zrp in two diﬀerent ways: ﬁrst, using an explicit representation of the matrices and boundary vectors; second, from the sole knowledge of the algebraic relations satisﬁed by these matrices and vectors. finally, an interpretation of the relation between the matrix product form and the inhomogeneous factorized form is proposed within the framework of hidden markov chains. "
1803.08276," speaker clustering is the task of diﬀerentiating speakers in a recording. in a way, the aim is to answer ""who spoke when"" in audio recordings. a common method used in industry is feature extraction directly from the recording thanks to mfcc features, and by using well-known techniques such as gaussian mixture models (gmm) and hidden markov models (hmm). in this paper, we studied neural networks (especially cnn) followed by clustering and audio processing in the quest to reach similar accuracy to state-of-the-art methods. i. "
1803.08277,"— synchronization in the networks of coupled oscil- lators is a widely studied topic in different areas. it is well- known that synchronization occurs if the connectivity of the network dominates heterogeneity of the oscillators. despite the extensive study on this topic, the quest for sharp closed-form synchronization tests is still in vain. in this paper, we present an algorithm for ﬁnding the taylor expansion of the inverse kuramoto map. we show that this taylor series can be used to obtain a hierarchy of increasingly accurate approximate tests with low computational complexity. these approximate tests are then used to estimate the threshold of synchronization as well as the position of the synchronization manifold of the network. "
1803.08279," improper aﬃne spheres have played an important role in the development of geometric methods for the study of the hessian one equation. here, we review most of the advances we have made in this direction during the last twenty years. "
1803.08281," a detailed-level collisional-radiative model for the m1 transition spectrum of the ca-like w54+ ion as observed in an electron beam ion trap (ebit) was constructed based on atomic data calculated by the relativistic conﬁguration interaction method and distorted wave theory. the present calculated tran- sition energy, rate and intensity of w54+ m1 transitions are compared with previous theoretical and experimental values. the results are in reasonable agreement with the available experimental and theoretical data. the synthetic spectrum explained the ebit spectrum in the 12-20 nm region, while a new possibly strong transition has been predicted to be observable with an appro- priate electron beam energy. the present work provides accurate atomic data that may be used in plasma diagnostics applications. keywords: collisional-radiative model, ca-like tungsten, relativistic conﬁguration interaction, ebit spectrum simulation ✩please cite this article in press as: x.ding et al., collisional radiative model for the m1 transition spectrum of the highly-charged w54+ ions, phys. lett. a (2018), https://doi.org/10.1016/j.physleta.2018.05.046 . ∗corresponding author. email addresses: dingxb@nwnu.edu.cn (xiaobin ding), dongcz@nwnu.edu.cn (chenzhong dong) preprint submitted to physics letters a june 4, 2018 "
1803.08283," we revisit the problem of deriving local gauge invariance with spontaneous symmetry breaking in the context of an eﬀective ﬁeld theory. previous derivations were based on the condition of tree- order unitarity. however, the modern point of view considers the standard model as the leading order approximation to an eﬀective ﬁeld theory. as tree-order unitarity is in any case violated by higher-order terms in an eﬀective ﬁeld theory, it is instructive to investigate a formalism which can be also applied to analyze higher-order interactions. in the current work we consider an eﬀective ﬁeld theory of massive vector bosons interacting with a massive scalar ﬁeld. we impose the conditions of generating the right number of constraints for systems with spin-one particles and perturbative renormalizability as well as the separation of scales at one-loop order. we ﬁnd that the above conditions impose severe restrictions on the coupling constants of the interaction terms. except for the strengths of the self-interactions of the scalar ﬁeld, that can not be determined at this order from the analysis of three- and four-point functions, we recover the gauge-invariant lagrangian with spontaneous symmetry breaking taken in the unitary gauge as the leading order approximation to an eﬀective ﬁeld theory. we also outline the additional work that is required to ﬁnish this program. pacs numbers: 04.60.ds, 11.10.gh, 03.70.+k, keywords: eﬀective ﬁeld theory; quantization; constraints; renormalization 8 1 0 2   r a   m 2 2     ] h t - p e h [     1 v 3 8 2 8 0 . 3 0 8 1 : v i x r a 1 i. "
1803.08284,". we observe that automorphism groups of right-angled artin groups contain nilpotent non-abelian subgroups, namely h3(z) the three- dimensional integer heisenberg group, provided they admit a certain type of element, called an adjacent transvection. this represents a (mi- nor) extension of a result of charney-vogtmann. "
1803.08286," the harborth constant for a ﬁnite group g, denoted g(g), is the smallest integer k such that every subset {g1, . . . , gk} ⊂ g of cardinality k over g admits an ordered subsequence j=1 gij = 1. the harborth constant has been well studied for several classes of abelian groups but not so well for nonabelian groups. in this short paper, we determine the harborth constant for the dihedral group explicitly. (gi1 , . . . , gil) with l = exp(g) such that ql keywords: zero-sum problems, harborth constant, dihedral group. 2010 ams classiﬁcation code: 11b30,11b75, 20d60 "
1803.08287,"— learning-based methods have been successful in solving complex control tasks without signiﬁcant prior knowl- edge about the system. however, these methods typically do not provide any safety guarantees, which prevents their use in safety-critical, real-world applications. in this paper, we present a learning-based model predictive control scheme that provides provable high-probability safety guarantees. to this end, we exploit regularity assumptions on the dynamics in terms of a gaussian process prior to construct provably accurate conﬁdence intervals on predicted trajectories. unlike previous approaches, we do not assume that model uncertainties are independent. based on these predictions, we guarantee that trajectories satisfy safety constraints. moreover, we use a terminal set constraint to recursively guarantee the existence of safe control actions at every iteration. in our experiments, we show that the resulting algorithm can be used to safely and efﬁciently explore and learn about dynamic systems. "
1803.08288,"— this paper presents a novel control strategy for the coordination of a multi-agent system subject to high- level goals expressed as linear temporal logic formulas. in particular, each agent, which is modeled as a sphere with 2nd order dynamics, has to satisfy a given local temporal logic speciﬁcation subject to connectivity maintenance and inter-agent collision avoidance. we propose a novel continuous control protocol that guarantees navigation of one agent to a goal point, up to a set of collision-free initial conﬁgurations, while maintaining connectivity of the initial neighboring set and avoiding inter-agent collisions. based on that, we develop a hybrid switching control strategy that ensures that each agent satisﬁes its temporal logic task. simulation results depict the validity of the proposed scheme. "
1803.08291," we study a coupled bulk-surface allen–cahn system with an aﬃne linear trans- mission condition, that is, the trace values of the bulk variable and the values of the surface variable are connected via an aﬃne relation, and this serves to generalize the usual dynamic boundary conditions. we tackle the problem of well-posedness via a penalization method using robin boundary conditions. in particular, for the relax- ation problem, the strong well-posedness and long-time behavior of solutions can be shown for more general and possibly nonlinear relations. new diﬃculties arise since the surface variable is no longer the trace of the bulk variable, and uniform estimates in the relaxation parameter are scarce. nevertheless, weak convergence to the orig- inal problem can be shown. using the approach of colli and fukao (math. models appl. sci. 2015), we show strong existence to the original problem with aﬃne linear relations, and derive an error estimate between solutions to the relaxed and original problems. key words. allen–cahn equation, maximal monotone graphs, dynamic boundary con- ditions, well-posedness, penalization via robin boundary conditions. ams subject classiﬁcation. 35b40, 35d35, 35k20, 35k61, 35k86 "
1803.08292," the electronic structure, magnetic properties and phase formation of hexagonal ferromagnetic fe3sn-based alloys have been studied from ﬁrst principles and by experiment. the pristine fe3sn compound is known to fulﬁll all the requirements for a good permanent magnet, except for the magnetocrystalline anisotropy energy (mae). the latter is large, but planar, i.e. the easy mag- netization axis is not along the hexagonal c direction, whereas a good permanent magnet requires the mae to be uniaxial. here we consider fe3sn0.75m0.25, where m= si, p, ga, ge, as, se, in, sb, te and bi, and show how diﬀerent dopants on the sn sublattice aﬀect the mae and can alter it from planar to uniaxial. the stability of the doped fe3sn phases is elucidated theoretically via the calculations of their formation enthalpies. a micromagnetic model is developed in order to estimate the energy density product (bh)max and coercive ﬁeld µ0hc of a potential magnet made of fe3sn0.75sb0.25, the most promising candidate from theoretical studies. the phase sta- bility and magnetic properties of the fe3sn compound doped with sb and mn has been checked experimentally on the samples synthesised using the reactive crucible melting technique as well as by solid state reaction. the fe3sn-sb compound is found to be stable when alloyed with mn. it is shown that even small structural changes, such as a change of the c/a ratio or volume, that can be induced by, e.g., alloying with mn, can inﬂuence anisotropy and reverse it from planar to uniaxial and back. pacs numbers: 75.50.ww, 75.30.gw, 75.20.en ∗ olga.vekilova@physics.uu.se 2 i. "
1803.08293," we study the convex hull of the ﬁrst n steps of a planar random walk, and present large-n asymptotic results on its perimeter length ln, diameter dn, and shape. in the case where the walk has a non-zero mean drift, we show that ln/dn → 2 a.s., and give distributional limit theorems and variance asymptotics for dn, and in the zero-drift case we show that the convex hull is inﬁnitely often arbitrarily well- approximated in shape by any unit-diameter compact convex set containing the origin, and then lim inf n→∞ ln/dn = 2 and lim supn→∞ ln/dn = π, a.s. among the tools that we use is a zero-one law for convex hulls of random walks. key words: random walk; convex hull; perimeter length; diameter; shape; zero-one law. ams subject classiﬁcation: 60g50 (primary) 60d05; 60f05; 60f15; 60f20 (secondary) 1 model and main results 1.1 "
1803.08294,". the interface region imaging spectrograph (iris) reveals numerous small-scale (sub-arcsecond) brightenings that appear as bright dots sparkling the solar transition region in active regions. here, we report a statistical study on these transition- region bright dots. we use an automatic approach to identify 2742 dots in a si iv raster image. we ﬁnd that the average spatial size of the dots is 0.8 arcsec2 and most of them are located in the faculae area. their doppler velocities obtained from the si iv 1394 å line range from −20 to 20 km s−1. among these 2742 dots, 1224 are predominantly blue-shifted and 1518 are red-shifted. their non- thermal velocities range from 4 to 50 km s−1 with an average of 24 km s−1. we speculate that the bright dots studied here are small-scale impulsive energetic events that can heat the active region corona. "
1803.08295,". we introduce a notion of weak anticommutativity for a pair (s, t ) of self-adjoint regular operators in a hilbert c∗-module e. we prove that the sum s + t of such pairs is self-adjoint and regular on the intersection of their domains. a similar result then holds for the sum s2 + t 2 of the squares. we show that our deﬁnition is closely related to the connes-skandalis positivity criterion in kk-theory. as such we weaken a sufﬁcient condition of kucerovsky for repre- senting the kasparov product. our proofs indicate that our conditions are close to optimal. 8 1 0 2   r a   m 2 2     ] . a o h t a m [     1 v 5 9 2 8 0 . 3 0 8 1 : v i x r a contents "
1803.08297," nonlinear optical phenomena are widely used for the study of semiconductor materials. the paper presents an overview of experimental and theoretical studies of excitons by the method of optical second and third harmonics generation in various bulk semiconductors (gaas, cdte, znse, zno, cu2o, (cd,mn)te, eute, euse), and low-dimensional heterostructures znse/bete. particular attention is paid to the role of external electric and magnetic ﬁelds that modify the exciton states and induce new mechanisms of optical harmonics generation. microscopic mechanisms of harmonics generation based on the stark eﬀect, the spin and orbital zeeman eﬀects, and on the magneto-stark eﬀect speciﬁc for excitons moving in an external magnetic ﬁeld are considered. this approach makes it possible to study the properties of excitons and to obtain new information on their energy and spin structure that is not available when the excitons are investigated by linear optical spectroscopy. as a result of these studies, a large amount of information was obtained, which allows us to conclude on the establishing of a new ﬁeld of research (cid:22) exciton spectroscopy by the method of optical harmonics generation. this work was supported by the deutsche forschungsgemeinschaft (grant icrc trr160, project c8 and grant trr142, projects b01 and b04) and the russian foundation for basic research (grants 15-52-12015 and 16-02-00377). 8 1 0 2   r p a 2       ] i c s - l r t m . t a m - d n o c [     2 v 7 9 2 8 0 . 3 0 8 1 : v i x r a i. "
1803.08298,"—in this paper, we study the effects of the variations of the propagation delay over large-scale antenna-arrays used in massive multiple-input multiple-output (mimo) wideband communication systems on the statistical properties of the channel. due to its simplicity and popularity, the elliptical geometry-based stochastic channel model (gbsm) is employed to demonstrate new non-stationary properties of the channel in the frequency and spatial domains caused by the drift of delays. in addition, we show that the time of travel of multi-path components (mpcs) over large-scale arrays may result in overlooked frequency and spatial decorrelation effects. these are theoretically demonstrated by deriving the space-time- frequency correlation functions (stfcfs) of both narrowband and wideband elliptical models. closed-form expressions of the array-variant frequency correlation function (fcf), power delay proﬁle (pdp), mean delay, and delay spread of single- and multi-confocal elliptical models are derived when the angles of arrival (aoas) are von mises distributed. in such conditions, we ﬁnd that the large dimensions of the antenna array may limit the narrowband characteristic of the single-ellipse model and alter the wideband characteristics (pdp and fcf) of the multi-confocal elliptical channel model. although we present and analyze numerical and simulation results for a particular gbsm, similar conclusions can be extended to other gbsms. keywords – massive mimo, channel modeling, spatial non- stationarity, array-variant delay. "
1803.08299," deregulated energy markets, demand forecasting, and the continuously increasing share of renewable energy sources call—among others—for a structured consideration of uncertainties in optimal power ﬂow problems. the main challenge is to guarantee power balance while maintaining economic and secure operation. in the presence of gaussian uncertainties aﬃne feedback policies are known to be viable options for this task. the present paper advocates a general framework for chance-constrained opf problems in terms of continuous random variables. it is shown that, irrespective of the type of distribution, the random-variable minimizers lead to aﬃne feedback policies. introducing a three-step methodology that exploits polynomial chaos expansion, the present paper provides a constructive approach to chance-constrained optimal power ﬂow problems that does not assume a speciﬁc distribution, e.g. gaussian, for the uncertainties. we illustrate our ﬁndings by means of a tutorial example. keywords: chance-constrained optimal power ﬂow, uncertainties, aﬃne policies, polynomial chaos 8 1 0 2   r a   m 2 2     ] . c o h t a m [     1 v 9 9 2 8 0 . 3 0 8 1 : v i x r a list of symbols "
1803.08300," we demonstrate for the ﬁrst time using a robust bayesian approach to analyse the populations of radio-quiet (rq) and radio-loud (rl) gamma-ray pulsars. we quantify their diﬀerences and obtain their distributions of the radio-cone opening half-angle δ and the magnetic inclination angle α by bayesian inference. in contrast to the conventional frequentist point estimations that might be non-representative when the distribution is highly skewed or multi-modal, which is often the case when data points are scarce, bayesian statistics displays the complete posterior distribution that the uncertainties can be readily obtained regardless of the skewness and modality. we found that the spin period, the magnetic ﬁeld strength at the light cylinder, the spin-down power, the gamma-ray-to-x-ray ﬂux ratio, and the spectral curvature signiﬁcance of the two groups of pulsars exhibit signiﬁcant diﬀerences at the 99% level. using bayesian inference, we are able to infer the values and uncertainties of δ and α from the distribution of rq and rl pulsars. we found that δ is between 10◦ and 35◦ and the distribution of α is skewed towards large values. keywords: gamma rays; stars — pulsars: general; methods: statistical 8 1 0 2   r a   m 2 2 .     ] e h h p - o r t s a [     1 v 0 0 3 8 0 . 3 0 8 1 : v i x r a corresponding author: hoi-fung yu dy@kth.se 2 yu et al. "
1803.08301," i=1, αi ∈ g, is a coset partition of g, then let g be a group and h1,...,hs be subgroups of g of indices d1,...,ds respectively. in 1974, m. herzog and j. sch¨onheim con- jectured that if {hiαi}i=s d1,..,ds cannot be distinct. we consider the herzog-sch¨onheim con- jecture for free groups of ﬁnite rank and develop a new combinatorial approach, using covering spaces. we give some suﬃcient conditions on the coset partition that ensure the conjecture is satisﬁed. further- more, under a certain assumption, we show there is a ﬁnite number of cases to study in order to show the conjecture is true for every coset partition. since every ﬁnitely generated group is a quotient of a free group of ﬁnite rank, we show these results extend to ﬁnitely generated groups. 1 "
1803.08302,". given a numerical semigroup ring r = k[[s]], an ideal e of s and an odd element b ∈ s, the numerical duplication s ✶b e is a numerical semigroup, whose associated ring k[[s ✶b e]] shares many properties with the nagata’s idealization and the amalgamated duplication of r along the monomial ideal i = (te | e ∈ e). in this paper we study the associated graded ring of the numerical duplication characterizing when it is cohen-macaulay, gorenstein or complete intersection. we also study when it is a homogeneous numerical semigroup, a property that is related to the fact that a ring has the same betti numbers of its associated graded ring. on the way we also characterize when grm(i) is cohen-macaulay and when grm(ωr) is a canonical module of grm(r) in terms of numerical semigroup’s properties, where ωr is a canonical module of r. "
1803.08303,". this work is entirely devoted to construct huge families of indecomposable arithmeti- cally cohen-macaulay (resp. ulrich) sheaves e of arbitrary high rank on a general standard (resp. linear) determinantal scheme x ⊂ pn of codimension c ≥ 1, n − c ≥ 1 and deﬁned by the maximal minors of a t × (t + c − 1) homogeneous matrix a. the sheaves e are constructed as iterated extensions of sheaves of lower rank. as applications: (1) we prove that any general standard de- terminantal scheme x ⊂ pn is of wild representation type provided the degrees of the entries of the matrix a satisfy some weak numerical assumptions; and (2) we determine values of t, n and n − c for which a linear standard determinantal scheme x ⊂ pn is of wild representation type with respect to the much more restrictive category of its indecomposable ulrich sheaves, i.e. x is of ulrich wild representation type. contents "
1803.08304," persistent entropy of persistence barcodes, which is based on the shannon entropy, has been recently deﬁned and successfully applied to diﬀerent scenarios: characterization of the idiotypic immune network, detection of the transition between the preictal and ictal states in eeg signals, or the classiﬁcation problem of real long-length noisy signals of dc electrical motors, to name a few. in this paper, we study properties of persistent entropy and prove its stability under small perturbations in the given input data. from this concept, we deﬁne three summary functions and show how to use them to detect patterns and topological features. "
1803.08305," we study the interactions of a relativistic jet with a dense turbulent gaseous disk of radius ∼ 2 kpc. we have performed a suite of simulations with diﬀerent mean den- sity, jet power and orientation. our results show that: a) the relativistic jet couples strongly with the gas in the inner kpc, creating a cavity and launching outﬂows. b) the high pressure bubble inﬂated by the jet and its back-ﬂow compresses the disk at the outer edges, driving inﬂows within the disk. c) jets inclined towards the disk couple more strongly with the disk and launch sub-relativistic, wide-angle outﬂows along the minor axis. d) shocks driven directly by the jet and the jet-driven energy bubble raise the velocity dispersion throughout the disk by several times its initial value. e) compression by the jet-driven shocks can enhance the star formation rate in the disk, especially in a ring-like geometry close to the axis. however, enhanced turbulent dispersion in the disk also leads to quenching of star formation. whether positive or negative feedback dominates depends on jet power, ism density, jet orien- tation with respect to the disc, and the time-scale under consideration. qualitatively, our simulations compare favourably with kinematic and morphological signatures of several observed galaxies such as ngc 1052, ngc 3079, 3c 326 and 3c 293. key words: galaxies: jets – galaxies: ism – hydrodynamics – galaxies: evolution – galaxies: high-redshift – methods: numerical "
1803.08310," magnetic imprints, the rapid and irreversible evolution of photospheric magnetic ﬁelds as a feedback from ﬂares in the corona, have been conﬁrmed by many previous studies. these studies showed that the horizontal ﬁeld will permanently increase near the polarity inversion line (pil) after eruptions, indicating that a more horizontal topology of photospheric magnetic ﬁeld will be reconstructed. in this study, we analyze 17 near- disk x-class ﬂares in 13 active regions (ars) with heliographic angle no greater than 45o since the launch of the solar dynamics observatory (sdo). we ﬁnd that conﬁned ﬂares without or with very weak cmes tend to show very weak magnetic imprints on the photosphere. the imprint regions of the horizontal ﬁeld could locate not only near the pil but also near sunspot umbrae with strong vertical ﬁelds. making use of the observed cme mass and speed, we ﬁnd that the cmes with larger momentums will bring into stronger magnetic imprints. furthermore, a linear relationship, with a conﬁdence coefﬁcient 0.82, between the cme momentum and the change of lorentz force is revealed. based on that, we quantify the back reaction time to be 336 s, which could be further applied to independently estimate the cme mass. keywords: sun: ﬂares — sun: magnetic ﬁelds — sun: photosphere "
1803.08311," we prove the existence of scarred eigenstates for star graphs with scattering matri- ces at the central vertex which are either a fourier transform matrix, or a matrix that prohibits back-scattering. we prove the existence of scars that are half-delocalised on a single bond. moreover we show that the scarred states we construct are maximal in the sense that it is impossible to have quantum eigenfunctions with a signiﬁcantly lower entropy than our examples. these scarred eigenstates are on graphs that exhibit generic spectral statistics of random matrix type in the large graph limit, and, in contrast to other constructions, correspond to non-degenerate eigenvalues; they exist for almost all choices of lengths. 1 "
1803.08312," interventional cancer clinical trials are generally too restrictive and some patients are often excluded from them on the basis of comorbidity, past or concomitant treatments and the fact that they are over a certain age. the eﬃcacy and safety of new treatments for patients with these characteristics are not, therefore, deﬁned. in this work, we build a model with which to automatically predict whether short clinical statements were considered in- clusion or exclusion criteria. we used clinical trials protocols on cancer that have been available in public registries for the last 18 years to train word em- beddings, and constructed a dataset of 6m short free-texts labeled as eligible or not eligible. we then trained and validated a text classiﬁer, using deep neural networks with pre-trained word-embedding as its inputs, to predict whether or not short free-text statements describing clinical information were considered eligible. the best model achieved an f-measure of 0.91 and an almost perfect agreement when employing a validation set of 800k labeled statements. the trained model was also tested on an independent set of clinical statements mimicking those used in routine clinical practice, yielding a consistent performance. we additionally analyzed the semantic reasoning of the word embedding representations obtained, and were able to identify equivalent treatments for a type of tumor in an analogy with the drugs used to treat other tumors. the present work shows that representation learning using neural networks can be successfully leveraged to extract the medical knowledge available on clinical trial protocols and potentially assist practi- ∗corresponding author. email addresses: aurelia@medbravo.org (aurelia bustos), pertusa@dlsi.ua.es (antonio pertusa) preprint submitted to journal of biomedical informatics march 26, 2018 tioners when prescribing treatments. keywords: clinical trials, clinical decision support system, natural language processing, word embeddings, deep neural networks "
1803.08313,". this is the ﬁrst in a series of three notes on an investigation into core regular double stone algebras, crdsa [2], which are meant to be read in order. in [2] many useful results regarding the center and core of a crdsa that indicate the nearly boolean nature of crdsa which we extend here. these notes were born out of a model of network security where individual nodes are considered to be in one of 3 states. let j be any non-empty set of network nodes, not necessarily ﬁnite. we deﬁne the node set bounded dis- tributive lattice through the pairwise disjoint subsets of j with the well known binary operations of ternary set partitions and note j = 1 is our minimal case. we then show the resultant bounded distributive lattice is isomorphic to c j 3 where c3 is the 3 element chain crdsa. we then derive that every crdsa is a subdirect product of c3, similarly as for boolean algebras and c2. we use these results along with a few known results to show the main re- sult, namely every boolean algebra is the center of some core regular double stone algebra, crdsa. we then use that result to characterize all subalgebras of a ﬁnite core regular double stone algebras, namely a crdsa a is a subalgebra of cj 3 for some k ≤ j. applications of these results are being explored, results of at least one of which will be published at a later date. 3 for some ﬁnite j ↔ a ∼= ck lastly, we show that c3 is primal which implies that the variety gener- ated by c3 is dually equivalent to the category of stone spaces and hence the category of boolean algebras. in some sense this is a last step towards our goal of establishing crdsa as nearly boolean, but leaves us a bit dissatisﬁed as to our understanding of crdsa in the dual topological cate- gory. the following two notes, exploring core regular double stone algebras, crdsa, ii,iii, will establish a duality between the category of crdsa and speciﬁcally crafted bi-topological spaces that enables better understanding of the ”nearly boolean” nature of crdsa in the dual category. in this note we prove the following result: "
1803.08314,". the aim of image captioning is to generate similar captions by machine as human do to describe image contents. despite many ef- forts, generating discriminative captions for images remains non-trivial. most traditional approaches imitate the language structure patterns, thus tend to fall into a stereotype of replicating frequent phrases or sentences and neglect unique aspects of each image. in this work, we propose an image captioning framework with a self-retrieval module as training guidance, which encourages generating discriminative captions. it brings unique advantages: (1) the self-retrieval guidance can act as a metric and an evaluator of caption discriminativeness to assure the qual- ity of generated captions. (2) the correspondence between generated captions and images are naturally incorporated in the generation pro- cess without human annotations, and hence our approach could utilize a large amount of unlabeled images to boost captioning performance with no additional laborious annotations. we demonstrate the eﬀectiveness of the proposed retrieval-guided method on ms-coco and flickr30k captioning datasets, and show its superior captioning performance with more discriminative captions. keywords: image captioning, language and vision, text-to-image re- trieval "
1803.08316," motivated by the idea of “smart” metasurfaces, we will demonstrate a graphene- based tunable absorber in which perfect absorption can be achieved for all angles of in- cidence, only by tuning the fermi level of graphene. we place an unpatterned graphene sheet on a mushroom-type high impedance surface whose resonant frequency is stable for all incident angles. for tm-polarization, perfect absorption can be realized from normal to grazing incidence at the same frequency when modulating the fermi level of graphene from 0.18 ev to 1 ev. "
1803.08317,". inspired by the algorithm of barnsley’s chaos game, we construct an open quantum system model based on the repeated interaction process. we shown that the quantum dynamics of the appropriate fermionic/bosonic system (in interaction with an environment) provides a physical model of the chaos game. when considering fermionic operators, we follow the system’s evolution by focusing on its reduced density matrix. the system is shown to be in a gaussian state (at all time t) and the average number of particles is shown to obey the chaos game equation. considering bosonic operators, with a system initially prepared in coherent states, the evolution of the system can be tracked by investigating the dynamics of the eigenvalues of the annihilation operator. this quantity is governed by a chaos game-like equation from which diﬀerent scenarios emerge. "
1803.08319,". multi-people tracking in an open-world setting requires a special eﬀort in precise detection. moreover, temporal continuity in the detection phase gains more importance when scene cluttering introduces the challenging problems of occluded targets. for the purpose, we pro- pose a deep network architecture that jointly extracts people body parts and associates them across short temporal spans. our model explicitly deals with occluded body parts, by hallucinating plausible solutions of not visible joints. we propose a new end-to-end architecture composed by four branches (visible heatmaps, occluded heatmaps, part aﬃnity ﬁelds and temporal aﬃnity ﬁelds) fed by a time linker feature extractor. to overcome the lack of surveillance data with tracking, body part and oc- clusion annotations we created the vastest computer graphics dataset for people tracking in urban scenarios by exploiting a photorealistic videogame. it is up to now the vastest dataset (about 500.000 frames, more than 10 million body poses) of human body parts for people track- ing in urban scenarios. our architecture trained on virtual data exhibits good generalization capabilities also on public real tracking benchmarks, when image resolution and sharpness are high enough, producing reliable tracklets useful for further batch data association or re-id modules. keywords: pose estimation, tracking, surveillance, occlusions "
1803.08320," a new cuda-c code for tracing orbits around non-charged black holes is presented. this code, named malbec, take advantage of the graphic process- ing units and the cuda platform for tracking null and timelike test particles in schwarzschild and kerr. also, a new general set of equations that describe the closed circular orbits of any timelike test particle in the equatorial plane is de- rived. these equations are extremely important in order to compare the analytical behavior of the orbits with the numerical results and verify the correct implementa- tion of the runge-kutta algorithm in malbec. finally, other numerical tests are performed, demonstrating that malbec is able to reproduce some well-known results in these metrics in a faster and more efﬁcient way than a conventional cpu implementation. keywords: cuda-c, gpu, timelike test particles, geodesics around black holes. pacs numbers:02.60.cb, 04.25.dg, 04.70.-s 1 "
1803.08322," using high resolution powder x-ray and neutron diﬀraction experiments, we determined the oﬀ-centered displacement of the ions within a unit cell and magnetoelectric coupling in nanoscale bifeo3 (≈20-200 nm). we found that both the oﬀ-centered displacement of the ions and magnetoelectric coupling exhibit nonmonotonic variation with particle size. they increase as the particle size reduces from bulk and reach maximum around 30 nm. with further decrease in particle size, they decrease precipitously. the magnetoelectric coupling is determined by the anomaly in oﬀ-centering of ions around the magnetic transition temperature (tn ). the ions, in fact, exhibit large anomalous displacement around the tn which is analyzed using group theoretical approach. it underlies the nonmonotonic particle-size-dependence of oﬀ-centre displacement of ions and magnetoelectric coupling. the nonmonotonic variation of magnetoelectric coupling with particle size is further veriﬁed by direct electrical measurement of remanent ferroelectric hysteresis loops at room temperature under zero and ∼20 koe mag- netic ﬁeld. competition between enhanced lattice strain and compressive pressure appears to be causing the nonmonotonic particle-size-dependence of oﬀ-centre displacement while coupling between piezo and magnetostriction leads to nonmonotonicity in the variation of magnetoelec- tric coupling. "
1803.08323," in this work, we propose a novel approach to prioritize the depth map computation of multi-view stereo (mvs) to obtain compact 3d point clouds of high quality and completeness at low computational cost. our prioritization approach operates before the mvs algorithm is executed and consists of two steps. in the ﬁrst step, we aim to ﬁnd a good set of matching partners for each view. in the second step, we rank the resulting view clusters (i.e. key views with matching partners) according to their impact on the fulﬁllment of desired quality parameters such as completeness, ground resolution and accuracy. additional to geometric analysis, we use a novel machine learning technique for training a conﬁdence predictor. the purpose of this conﬁdence predictor is to estimate the chances of a successful depth reconstruction for each pixel in each image for one speciﬁc mvs algorithm based on the rgb images and the image constellation. the underlying machine learning technique does not require any ground truth or manually labeled data for training, but instead adapts ideas from depth map fusion for providing a supervision signal. the trained conﬁdence predictor allows us to evaluate the quality of image constellations and their potential impact to the resulting 3d reconstruction and thus builds a solid foundation for our prioritization approach. in our experiments, we are thus able to reach more than 70% of the maximal reachable quality fulﬁllment using only 5% of the available images as key views. for evaluating our approach within and across diﬀerent domains, we use two completely diﬀerent scenarios, i.e. cultural heritage preservation and reconstruction of single family houses. keywords: multi-view stereo, machine learning, conﬁdence measures, view prioritization, image clustering, view cluster ranking "
1803.08326,". we propose a novel statistical color constancy method, especially suit- able for the camera-agnostic color constancy, i.e. the scenario where nothing is known a priori about the capturing devices. the method, called dichromatic gray pixel, or dgp, relies on a novel gray pixel detection algorithm derived us- ing the dichromatic reﬂection model. dgp is suitable for camera-agnostic color constancy, since varying devices are set to make achromatic pixels look gray un- der standard neutral illumination. in the camera-agnostic scenario, the proposed method outperforms, on standard benchmarks, both state-of-the-art learning-based and statistical methods. dgp is simple, literally dozens of lines of code, and fast, processing a 1080p image in 0.4 seconds with unoptimized matlab code run- ning in a cpu intel i7 2.5 ghz. keywords: illumination estimation, color constancy, dichromatic reﬂection model, gray pixel "
1803.08328,"— in this paper we consider a distributed convex optimization problem over time-varying networks. we propose a dual method that converges r-linearly to the optimal point given that the agents’ objective functions are strongly convex and have lipschitz continuous gradients. the proposed method requires half the amount of variable exchanges per iterate than methods based on diging, and yields improved practical performance as empirically demonstrated. "
1803.08329," we consider the contributions upto the d10r4 terms in the low momentum ex- pansion of the two loop four graviton amplitude in maximal supergravity that arise in the ﬁeld theory limit of genus two modular graph functions that result from the low momentum expansion of the four graviton amplitude in toroidally compactiﬁed type ii string theory, using the worldline formalism of the ﬁrst quantized superparticle. the expression for the two loop supergravity amplitude in the worldline formalism allows us to obtain contributions from the individual graphs, unlike the expression for the same amplitude obtained using unitarity cuts which only gives the total con- tribution from the sum of all the graphs. our two loop analysis is ﬁeld theoretic, and does not make explicit use of the genus two string amplitude. 8 1 0 2   r a   m 2 2     ] h t - p e h [     1 v 9 2 3 8 0 . 3 0 8 1 : v i x r a 1email address: anirbanbasu@hri.res.in 1 "
1803.08331,". we classify certain cases when the wreath products of distinct pairs of groups generate the same variety. this allows us to investigate the subvarieties of some nilpotent- by-abelian product varieties uv with the help of wreath products of groups. in particular, using wreath products we ﬁnd such subvarieties in nilpotent-by-abelian uv, which have the same nilpotency class, the same length of solubility, and the same exponent, but which still are distinct subvarieties. obtained classiﬁcation strengthens our recent work on varieties generated by wreath products. "
1803.08332,". in this paper, we show that every singular ﬁber of the gelfand–cetlin system on coadjoint orbits of unitary groups is a smooth isotropic submanifold which is diﬀeomorphic to a 2-stage quotient of a compact lie group by free actions of two other compact lie groups. in many cases, these singular ﬁbers can be shown to be homogeneous spaces or even diﬀeomorphic to compact lie groups. we also give a combinatorial formula for computing the dimensions of all singular ﬁbers, and give a detailed description of these singular ﬁbers in many cases, including the so-called (multi-)diamond singularities. these (multi-)diamond singular ﬁbers are degenerate for the gelfand–cetlin system, but they are lagrangian submanifolds diﬀeomorphic to direct products of special unitary groups and tori. our methods of study are based on diﬀerent ideas involving complex ellipsoids, lie groupoids, and also general ideas coming from the theory of singularities of integrable hamiltonian systems. contents "
1803.08334," we develop a machine learning-based framework to predict the hi content of galaxies using more straightforwardly observable quantities such as optical photometry and environmental parameters. we train the algorithm on z = 0 − 2 outputs from the mufasa cosmological hydrodynamic simulation, which includes star formation, feedback, and a heuristic model to quench massive galaxies that yields a reasonable match to a range of survey data including hi. we employ a variety of machine learning methods (regressors), and quantify their performance using the root mean square error (rmse) and the pearson correlation coeﬃcient (r). considering sdss photometry, 3rd nearest neighbor environment and line of sight peculiar velocities as features, we obtain r > 0.8 accuracy of the hi-richness prediction, corresponding to rmse< 0.3. adding near-ir photometry to the features yields some improvement to the prediction. compared to all the regressors, random forest shows the best performance, with r > 0.9 at z = 0, followed by a deep neural network with r > 0.85. all regressors exhibit a declining performance with increasing redshift, which limits the utility of this approach to z (cid:46) 1, and they tend to somewhat over-predict the hi content of low-hi galaxies which might be due to eddington bias in the training sample. we test our approach on the resolve survey data. training on a subset of resolve, we ﬁnd that our machine learning method can reasonably well predict the hi-richness of the remaining resolve data, with rmse∼ 0.28. when we train on mock data from mufasa and test on resolve, this increases to rmse∼ 0.45. our method will be useful for making galaxy-by-galaxy survey predictions and incompleteness corrections for upcoming hi 21cm surveys such as the laduma and mightee surveys on meerkat, over regions where photometry is already available. key words: galaxies: evolution – galaxies: statistics – methods: n-body simulations 1 "
1803.08336,": this paper presents a continuous-time equilibrium model of liquidity provision in a market with multiple strategic investors with intraday trading targets. we show analytically that there are inﬁnitely many nash equilibria. we solve for the welfare-maximizing equilibrium and the competitive equilibrium, and we illustrate that these equilib- ria are diﬀerent. the model is easily computed numerically, and we provide a number of numerical illustrations. keywords: dynamic trading, twap, rebalancing, equilibrium ams subject classifications: 93e20 jel-classification: g12, g11, d53 8 1 0 2   r a m   2 2 .     ] f m n i f - q [     1 v 6 3 3 8 0 . 3 0 8 1 : v i x r a 1 1 "
1803.08337," encoder (ﬁxed) decoder (ﬁne-tuned) classiﬁer (ﬁxed) 8 1 0 2   r a   m 2 2     ] v c . s c [     1 v 7 3 3 8 0 . 3 0 8 1 : v i x r a we propose a novel way to measure and understand con- volutional neural networks by quantifying the amount of in- put signal they let in. to do this, an autoencoder (ae) was ﬁne-tuned on gradients from a pre-trained classiﬁer with ﬁxed parameters. we compared the reconstructed samples from aes that were ﬁne-tuned on a set of image classiﬁers (alexnet, vgg16, resnet-50, and inception v3) and found substantial differences. the ae learns which aspects of the input space to preserve and which ones to ignore, based on the information encoded in the backpropagated gradients. measuring the changes in accuracy when the signal of one classiﬁer is used by a second one, a relation of total order emerges. this order depends directly on each classiﬁer’s in- put signal but it does not correlate with classiﬁcation accu- racy or network size. further evidence of this phenomenon is provided by measuring the normalized mutual informa- tion between original images and auto-encoded reconstruc- tions from different ﬁne-tuned aes. these ﬁndings break new ground in the area of neural network understanding, opening a new way to reason, debug, and interpret their re- sults. we present four concrete examples in the literature where observations can now be explained in terms of the input signal that a model uses. "
1803.08338," at radio wavelengths, solar type stars emit thermal free-free and gyroresonance, gyrosyn- chrotron, and impulsive coherent emission. thermal free-free emission originates at layers where the optical depth is close to unit, while high brightness temperature, variable emission, can be due to ﬂares via gyrosynchrotron emission. we observed the α cen system with the australian telescope compact array at 2 ghz for three days and 17 ghz for one day. both stars have been detected at 17 ghz, while only an upper limit has been obtained at low fre- quency despite the longest integration time. the brightness temperatures are consistent with the temperature of the upper chromosphere of the sun. inverting the formulae of the free-free emission, the average electron density of the plasma has been inferred. the same procedure was applied to the data in the millimetre recently taken with alma. a comparison with the atmospheric solar models reveals a higher level of activity in α cen b, even if still at quies- cent level. the non detection at low frequency allow us to put a lower limit in the ﬁlling factor of active regions. the claimed detection of an earth size planet in close orbit to α cen b, although doubtful, open the opportunity to check the existence of star-planet magnetic inter- action (spmi). this could trigger auroral radio emission due to electron cyclotron maser in the stellar corona, similar to the jupiter-io interaction, which is very intense, polarized and highly beamed. however, no hints of spmi has been found. key words: radio continuum: stars – stars: individual: α centauri a b– stars: chromospheres – planet−star interactions – stars: solar-type – techniques: interferometric. 1 "
1803.08341," fast constant factor approximation algorithms are devised for an np- and w[1]- hard problem of intersecting a set of straight line segments with the smallest cardinality set of disks of ﬁxed radii r > 0, where the set of segments forms a straight line drawing g = (v, e) of a planar graph without edge crossings. exploiting tough connection of the problem with the geometric hitting set prob- lem, an(cid:16)50 + 52q 12 ε3 (cid:17) |e|2 log |e|(cid:17)- 13 + ε(cid:17)-approximate o(cid:16)(cid:16)|e|2 + |e| log |e| time and o(cid:16) |e|2 log |e| (cid:17)-space algorithm is given based on the modiﬁed agarwal- pan algorithm. more accurate (34+24√2+ε)-,(cid:0)12 + 6√3 + ε(cid:1)- and(cid:16)34 + 38q 15 19 + ε(cid:17)- approximate algorithms are also proposed for the case where g is any sub- + log |e| ε2 ε graph of either an outerplane or a gabriel graph or a delaunay triangulation respectively, which work within the same time and space complexity bounds, where ε > 0 is an arbitrary small constant. related work only tackles the case where e consists of axis-parallel segments, resulting in an o(|e| log |e|)-time and o(|e| log |e|)-space 8-approximation. keywords: approximation algorithm, geometric hitting set problem, epsilon net, geometric data structure, delaunay triangulation, gabriel graph, line segments ✩this work was supported by russian science foundation, project 14-11-00109. email address: kobylkinks@gmail.com (konstantin kobylkin) url: wwwrus.imm.uran.ru (konstantin kobylkin) preprint submitted to computational geometry: theory and applications may 23, 2018 "
1803.08342,". we study the dual relationship between quantum group convolution maps l1(g) → l∞(g) and completely bounded multipliers of bg. for a large class of locally compact quantum groups g we completely isomorphically identify the mapping ideal of row hilbert space factoriz- able convolution maps with mcb(l1( bg)), yielding a quantum gilbert representation for completely bounded multipliers. we also identify the mapping ideals of completely integral and completely nuclear convolution maps, the latter case coinciding with ℓ1(cbg), where bg is the quantum bohr compactiﬁcation of g. for quantum groups whose dual has bounded degree, we show that the completely compact convolution maps coincide with c(bg). our techniques comprise a mixture of operator space theory and  harmonic analysis, including fubini tensor products, the non- commutative grothendieck inequality, quantum eberlein compactiﬁcations, and a suitable notion of quasi-sin quantum group, which we introduce and exhibit examples from the bicrossed product construction. our main results are new even in the setting of group von neumann algebras v n (g) for quasi-sin locally compact groups g. "
1803.08343,"— cultural adaptation, i.e., the matching of a robot’s behaviours to the cultural norms and preferences of its user, is a well known key requirement for the success of any assistive application. however, culture-dependent robot behaviours are often implicitly set by designers, thus not allowing for an easy and automatic adaptation to different cultures. this paper presents a method for the design of culture-aware robots, that can automatically adapt their behaviour to conform to a given culture. we propose a mapping from cultural factors to related parameters of robot behaviours which relies on linguistic variables to encode heterogeneous cultural factors in a uniform formalism, and on fuzzy rules to encode qualitative relations among multiple variables. we illustrate the approach in two practical case studies. "
1803.08344," we report a possible dipole anisotropy on acceleration scale g† with 147 rotationally supported galaxies in local universe. it is found that a monopole and dipole correction for the radial accel- eration relation can better describe the sparc data set. the monopole term is negligible but the dipole magnitude is signiﬁcant. it is also found that the dipole correction is mostly induced by the anisotropy on the acceleration scale. the magnitude of ˆg†-dipole reaches up to 0.25 ± 0.04, and its direction is aligned to (l, b) = (171.30◦ ±4.87◦), which is very close to the maximum ±7.18◦,−15.41◦ anisotropy direction from the hemisphere comparison method. furthermore, robust check shows that the dipole anisotropy couldn’t be reproduced by isotropic mock data set. however, it is still premature to claim that the universe is anisotropic due to the small data samples and uncertainty in the current observations. 8 1 0 2   y a m 0 3       ] . o c h p - o r t s a [     2 v 4 4 3 8 0 . 3 0 8 1 : v i x r a 1 "
1803.08345,". we establish the mean-ﬁeld convergence for systems of points evolving along the gradient-ﬂow of their interaction energy when the interaction is the coulomb potential or a super-coulombic riesz potential, for the ﬁrst time in arbitrary dimension. the proof is based on a modulated energy method using a coulomb or riesz distance, assumes that the solutions of the limiting equation are regular enough and exploits a weak-strong stability property for them. the method applies as well to conservative and mixed ﬂows. 1.1. problem and background. we study here the large n limit of gradient ﬂow evolutions "
1803.08348," random sequential adsorption (rsa) is a time-dependent packing process, in which particles of certain shapes are randomly and sequentially placed into an empty space without overlap. in the inﬁnite-time limit, the density approaches a “saturation” limit. although this limit has attracted particular research interest, the majority of past studies could only probe this limit by extrapolation. we have previously found an algorithm to reach this limit using ﬁnite computational time for spherical particles, and could thus determine the saturation density of spheres with high accuracy. in this paper, we generalize this algorithm to generate saturated rsa packings of two- dimensional polygons. we also calculate the saturation density for regular polygons of three to ten sides, and obtain results that are consistent with previous, extrapolation-based studies. pacs numbers: 05.10.-a, 45.70.-n, 05.20.-y 8 1 0 2   r p a 0 3       ] t f o s . t a m - d n o c [     2 v 8 4 3 8 0 . 3 0 8 1 : v i x r a ∗ gezhang@alumni.princeton.edu 1 i. "
1803.08349," let mg,n be the moduli space of n-pointed stable genus g curves, and let mg,n be the moduli space of n-pointed smooth curves of genus g. in this paper, we obtain an asymptotic expansion for the characteristic of the free modular operad mv generated by a stable s-modulev, allowing to eﬀectively compute sn-equivariant euler characteristics of mg,n in terms of sn′-equivariant euler characteristics of mg′,n′ with 0≤ g′≤ g, max{0, 3−2g′}≤ n′≤ 2(g−g′)+n. the modular operad mv eﬀective. to illustrate how the asymptotic expansion is used, we give formulas expressing the generating series of the sn-equivariant euler characteristics of mg,n, for g= 0, 1 and 2, in terms of the corresponding this answers a question posed by getzler and kapranov by making their integral representation of the characteristic of generating series associated with mg,n. 8 1 0 2   r a m   2 2     ] . g a h t a m [     1 v 9 4 3 8 0 . 3 0 8 1 : v i x r a contents "
1803.08350," curvature k satisfying (cid:16) λ λ+1(cid:17)2 of closed geodesics is ﬁnite, then there exist [ dim m ] non-hyperbolic closed geodesics. moreover, 2 < k ≤ 1, there exist [ dim m +1 2 ] closed geodesics. if the number there are 3 closed geodesics on (m, f ) satisfying the above pinching condition when dim m = 3. key words: finsler manifolds, closed geodesics, index iteration, morse theory. ams subject classiﬁcation: 53c22, 53c60, 58e10. running head: closed geodesics on finsler manifolds 1 "
1803.08351,". it is known that for a conditional quasi-greedy basis b in a banach space x, the associated sequence (km[b])∞ m=1 of its conditionality constants veriﬁes the estimate km[b] = o(log m) and that if the reverse inequality log m = o(km[b]) holds then x is non-superreﬂexive. indeed, it is known that a quasi-greedy basis in a superreﬂexive quasi-banach space fulﬁls the estimate km[b] = o(log m)1−ǫ for some ǫ > 0. however, in the existing literature one ﬁnds very few instances of spaces possessing quasi- greedy basis with conditionality constants “as large as possible.” our goal in this article is to ﬁll this gap. to that end we enhance and exploit a technique developed by dilworth et al. in [16] and craft a wealth of new examples of both non-superreﬂexive classical banach spaces having quasi-greedy bases b with km[b] = o(log m) and superreﬂexive classical banach spaces having for every ǫ > 0 quasi-greedy bases b with km[b] = o(log m)1−ǫ. moreover, in most cases those bases will be almost greedy. "
1803.08352,". let (x, d, t ) be a topological dynamical system with speci- ﬁcation property. for α ∈ r+ and any x0 ∈ x, deﬁne dx0 α :=nx ∈ x : lim ǫ→0 lim sup n→∞ max{t ∈ n : t n(x) ∈ bt(x0, ǫ)} n ≥ αo. htop(t ) then we have hb 1 + α bowen topological entropy of dx0 α . top(t, dx0 α ) = , where hb top(t, dx0 α ) denotes the keywords and phrases: shadowing time, topological entropy, topological pressure. 8 1 0 2   r a   m 1 2     ] s d h t a m . [     1 v 2 5 3 8 0 . 3 0 8 1 : v i x r a 1 "
1803.08354,". user modeling is a very important task for making relevant suggestions of venues to the users. these suggestions are often based on matching the venues’ features with the users’ preferences, which can be collected from previously visited locations. in this paper, we present a set of relevance scores for making personalized suggestions of points of interest. these scores model each user by focusing on the diﬀerent types of information extracted from venues that they have previously visited. in particular, we focus on scores extracted from social information available on location-based social networks. our experiments, conducted on the dataset of the trec contextual suggestion track, show that social scores are more eﬀective than scores based venues’ content. 1 "
1803.08355," motivated by supervised opinion analysis, we propose a novel framework devoted to structured output learning with abstention (sola). the structure prediction model is able to abstain from predicting some labels in the structured output at a cost chosen by the user in a ﬂexible way. for that purpose, we decompose the problem into the learning of a pair of predictors, one devoted to structured abstention and the other, to struc- tured output prediction. to compare fully la- beled training data with predictions potentially containing abstentions, we deﬁne a wide class of asymmetric abstention-aware losses. learning is achieved by surrogate regression in an appropriate feature space while prediction with abstention is performed by solving a new pre-image problem. thus, sola extends recent ideas about struc- tured output prediction via surrogate problems and calibration theory and enjoys statistical guar- antees on the resulting excess risk. instantiated on a hierarchical abstention-aware loss, sola is shown to be relevant for ﬁne-grained opinion min- ing and gives state-of-the-art results on this task. moreover, the abstention-aware representations can be used to competitively predict user-review ratings based on a sentence-level opinion predic- tor. 8 1 0 2   r a   m 2 2     ] g l . s c [     1 v 5 5 3 8 0 . 3 0 8 1 : v i x r a "
1803.08357," we prove some strong results on sum-product estimates, and expanding polyno- mials over matrix rings m2(fq) over ﬁnite ﬁelds. we also provide similar results in the special linear group sl2(fq). sum-product graphs over both m2(fq) and sl2(fq), and the special unit-graph over m2(fq) play a crucial role in deriving our results. 1 "
1803.08358,". we consider a hamiltonian describing three quantum particles in dimension one inter- acting through two-body short-range potentials. we prove that, as a suitable scale parameter in the potential terms goes to zero, such hamiltonian converges to one with zero-range (also called delta or point) interactions. the convergence is understood in norm resolvent sense. the two-body rescaled σ(xσ) = ε−1vσ(ε−1xσ), where σ = 23, 12, 31 is an index that runs over potentials are of the form vε all the possible pairings of the three particles, xσ is the relative coordinate between two particles, and ε is the scale parameter. the limiting hamiltonian is the one formally obtained by replacing the potentials vσ with ασδσ, where δσ is the dirac delta-distribution centered on the coincidence hyperplane xσ = 0 and ασ =rr vσdxσ. to prove the convergence of the resolvents we make use of faddeev’s equations. keywords: point interactions; three-body hamiltonian; schr¨odinger operators. msc 2010: 81q10; 81q15; 70f07; 46n50. "
1803.08359,"—in typical software, many comparisons and subsequent branch operations are highly critical in terms of security. examples include password checks, signature checks, secure boot, and user privilege checks. for embedded devices, these security-critical branches are a preferred target of fault attacks as a single bit flip or skipping a single instruction can lead to complete access to a system. in the past, numerous redundancy schemes have been proposed in order to provide control- flow-integrity (cfi) and to enable error detection on processed data. however, current countermeasures for general purpose software do not provide protection mechanisms for conditional branches. hence, critical branches are in practice often simply duplicated. we present a generic approach to protect conditional branches, which links an encoding-based comparison result with the redundancy of cfi protection mecha- nisms. the presented approach can be used for all types of data encodings and cfi mechanisms and maintains their error-detection capabilities throughout all steps of a conditional branch. we demonstrate our approach by realizing an encoded comparison based on an-codes, which is a frequently used encoding scheme to detect errors on data during arithmetic operations. we ex- tended the llvm compiler so that standard code and conditional branches can be protected automatically and analyze its security. our design shows that the overhead in terms of size and runtime is lower than state-of-the-art duplication schemes. index terms—control-flow integrity, branch, fault attacks, countermeasures conditional "
1803.08363," exponential distributions appear in a wide range of applications in- cluding chemistry, nuclear physics, time series analyses, and stock market trends. there are conceivable circumstances in which one would be in- terested in the cumulative probability distribution of the sum of some number of exponential variables, with potentially diﬀering constants in their exponents. in this article we present a pedagogical derivation of the cumulative distribution, which reproduces the known formula from power density analyses in the limit that all of the constants are equal, and which assumes no prior knowledge of combinatorics except for some of the properties of a class of symmetric polynomials in n variables (schur polynomials). "
1803.08365," the explanation of the coronal heating problem potentially lies in the existence of nanoﬂares, nu- merous small-scale heating events occuring across the whole solar disk. in this paper, we present the ﬁrst imaging spectroscopy x-ray observations of three quiet sun ﬂares during the nustar solar campaigns on 2016 july 26 and 2017 march 21, concurrent with sdo/aia observations. two of the three events showed time lags of a few minutes between peak x-ray and extreme ultraviolet (euv) emissions. isothermal ﬁts with rather low temperatures in the range 3.2 − 4.1 mk and emission mea- sures of (0.6 − 15) × 1044 cm−3 describe their spectra well, resulting in thermal energies in the range (2 − 6) × 1026 ergs. nustar spectra did not show any signs of a nonthermal or higher temperature component. however, since the estimated upper limits of (hidden) nonthermal energy are comparable to the thermal energy estimates, the lack of a nonthermal component in the observed spectra is not a constraining result. the estimated goes classes from the ﬁtted values of temperature and emission measure fall between 1/1000 and 1/100 a class level, making them 8 orders of magnitude fainter in soft x-ray ﬂux than the largest solar ﬂares. subject headings: sun: ﬂares — sun: particle emission — sun: x-rays "
1803.08368,". this is an expository plus research paper which mainly exposes pre- liminary connection and contrast between classical complex dynamics and semi- group dynamics of holomorphic functions. classically, we expose some existing results of rational and transcendental dynamics and we see how far these results generalized to holomorphic semigroup dynamics as well as we also see what new phenomena occur. "
1803.08371," the proton spin crisis remains an unsolved problem in physics. in this paper we ﬁnd that due to the conﬁnement of partons inside the hadron the angular momentum sum rule in qcd is violated. hence we ﬁnd that the non-vanishing angular momentum ﬂux contribution of the partons in qcd should be added to the spin and angular momentum of the partons to solve the proton spin crisis. pacs numbers: 11.30.-j, 11.30.cp, 11.15.-q, 12.38.-t 8 1 0 2   y a m 9 2       ] h p - p e h [     2 v 1 7 3 8 0 . 3 0 8 1 : v i x r a ∗g. c. nayak was aﬃliated with c. n. yang institute for theoretical physics in 2004-2007. typeset by revtex 1 i. "
1803.08373, having in mind applications to particle physics we develop the diﬀer- ential calculus over jordan algebras and the theory of connections on jordan modules. in particular we focus on diﬀerential calculus over the exceptional jordan algebra and provide a complete characteriza- tion of the theory of connections for free jordan modules. 1 
1803.08374,"—supervised learning frequently boils down to de- termining hidden and bright parameters in a parameterized hypothesis space based on ﬁnite input-output samples. the hid- den parameters determine the attributions of hidden predictors or the nonlinear mechanism of an estimator, while the bright parameters characterize how hidden predictors are linearly combined or the linear mechanism. in traditional learning paradigm, hidden and bright parameters are not distinguished and trained simultaneously in one learning process. such an one-stage learning (osl) brings a beneﬁt of theoretical analysis but suﬀers from the high computational burden. to overcome this diﬃculty, a two-stage learning (tsl) scheme, featured by learning through random assignment for hidden parameters (ltrahp) is developed. ltrahp assigns randomly the hidden parameters in the ﬁrst stage and determines the bright param- eters by solving a linear least square problem in the second stage. although ltrahp works well in many applications, it suﬀers from an uncertainty problem: its performance can only be guaranteed in a certain statistical expectation sense. in this paper we propose a new tsl scheme, learning through deterministic assignment of hidden parameters (ltdahp), where we suggest to deterministically generate the hidden parameters by using minimal riesz energy points on a sphere and equally spaced points in an interval. we theoretically show that with such deterministic assignment of hidden parameters, ltdahp with a neural network realization almost shares the same generalization performance with that of osl, i.e., it does not degrade the generalization capability of osl. thus, ltdahp provides an eﬀective way to overcome both the high computational burden of osl and the uncertainty problem of ltrahp. we present a series of simulations and application examples to support the outperformance of ltdahp, as compared with the typical osl algorithm: support vector regression (svr) and an typical ltrahp algorithm. the study conducted in this paper is a novel trial to tackle supervised learning problems simply and eﬃciently. index terms—supervised learning, neural networks, hidden parameters, bright parameters, learning rate. "
1803.08375," we introduce the use of rectified linear units (relu) as the classifi- cation function in a deep neural network (dnn). conventionally, relu is used as an activation function in dnns, with softmax function as their classification function. however, there have been several studies on using a classification function other than soft- max, and this study is an addition to those. we accomplish this by taking the activation of the penultimate layer hn−1 in a neu- ral network, then multiply it by weight parameters θ to get the raw scores oi. afterwards, we threshold the raw scores oi by 0, i.e. f (o) = max(0, oi), where f (o) is the relu function. we provide class predictions ˆy through arg max function, i.e. arg max f (x). ccs concepts • computing methodologies → supervised learning by clas- sification; neural networks; keywords artificial intelligence; artificial neural networks; classification; con- volutional neural network; deep learning; deep neural networks; feed-forward neural network; machine learning; rectified linear units; softmax; supervised learning "
1803.08376," although several theories for the origin of cosmic rays in the region between the spectral ‘knee’ and ‘ankle’ exist, this problem is still unsolved. a variety of observations suggest that the transition from galactic to extragalactic sources occurs in this energy range. in this work we examine whether a galactic wind which eventually forms a termination shock far outside the galactic plane can contribute as a possible source to the observed ﬂux in the region of interest. previous work by bustard et al. (2017) estimated that particles can be accelerated up to energies above the ‘knee’ up to rmax = 1016 ev for parameters drawn from a model of a milky way wind (everett et al. 2010). a remaining question is whether the accelerated cosmic rays can propagate back into the galaxy. to answer this crucial question, we simulate the propagation of the cosmic rays using the low energy extension of the crpropa framework, based on the solution of the transport equation via stochastic diﬀerential equations. the setup includes all relevant processes, including three-dimensional anisotropic spatial diﬀusion, advection, and corresponding adiabatic cooling. we ﬁnd that, assuming realistic parameters for the shock evolution, a possible galactic termination shock can contribute signiﬁcantly to the energy budget in the ‘knee’ region and above. we estimate the resulting produced neutrino ﬂuxes and ﬁnd them to be below measurements from icecube and limits by km3net. keywords: cosmic rays — propagation of particles — galactic termination shock — neutrinos "
1803.08377,"—this paper deals with the problem of massive random access for gaussian multiple access channel (mac). we continue to investigate the coding scheme for gaussian mac proposed by a. vem et al in 2017. the proposed scheme consists of four parts: (i) the data transmission is partitioned into time slots; (ii) the data, transmitted in each slot, is split into two parts, the ﬁrst one set an interleaver of the low-density parity- check (ldpc) type code and is encoded by spreading sequence or codewords that are designed to be decoded by compressed sensing type decoding; (iii) the another part of transmitted data is encoded by ldpc type code and decoded using a joint message passing decoding algorithm designed for the t-user binary input gaussian mac; (iv) users repeat their codeword in multiple slots. in this paper we are concentrated on the third part of considered scheme. we generalized the pexit charts to optimize the protograph of ldpc code for gaussian mac. the simulation results, obtained at the end of the paper, were analyzed and compared with obtained theoretical bounds and thresholds. obtained simulation results shows that proposed ldpc code constructions have better performance under joint decoding algorithm over gaussian mac than ldpc codes considered by a. vem et al in 2017, that leads to the better performance of overall transmission system. "
1803.08378," a variety of rating-based recommendation methods have been extensively studied including the well-known col- laborative ﬁltering approaches and some network diﬀusion-based methods, however, social trust relations are not suﬃciently considered when making recommendations. in this paper, we contribute to the literature by proposing a trust-based recommendation method, named cosra+t, after integrating the information of trust relations into the resource-redistribution process. speciﬁcally, a tunable parameter is used to scale the resources received by trusted users before the redistribution back to the objects. interestingly, we ﬁnd an optimal scaling parameter for the proposed cosra+t method to achieve its best recommendation accuracy, and the optimal value seems to be universal under several evaluation metrics across diﬀerent datasets. moreover, results of extensive experiments on the two real-world rating datasets with trust relations, epinions and friendfeed, suggest that cosra+t has a remarkable improvement in overall accuracy, diversity and novelty. our work moves a step towards designing better recommendation algorithms by employing multiple resources of social network information. keywords: recommender system, trust relations, vertex similarity, network diﬀusion, complex networks "
1803.08379,". we study the general properties of certain rank 4 rigid local systems con- sidered by goursat. we analyze when they are irreducible, give an explicit integral description as well as the invariant hermitian form h when it exists. by a computer search we ﬁnd what we expect are all irreducible such systems all whose solutions are algebraic functions and give several explicit examples deﬁned over q. we also exhibit one example with inﬁnite monodromy as arising from a family of genus two curves. "
1803.08381," low-complexity non-smooth convex regularizers are routinely used to impose some structure (such as sparsity or low-rank) on the coefﬁcients for linear methods in supervised learning. model con- sistency (selecting the correct structure, for instance support or rank) is known to hold only if some non-degeneracy condition is imposed. this condition typically fails to hold for highly correlated designs and regularization methods tend to select larger models. in this work, we provide the theo- retical underpinning of this behavior using the notion of mirror-stratiﬁable regularizers. this class of regularizers encompasses most well-known ones in the literature including the (cid:96)1 or trace norms. it enjoys a strong primal-dual relation between the models, which in turn allows one to locate the structure of the solution using a speciﬁc dual certiﬁcate. we also show how this analysis is applica- ble not only to solutions of an optimization problem, but also to the iterates computed by a certain class of stochastic proximal-gradient algorithms. keywords: mirror stratiﬁcation, regularization, lasso, sparsity, low-rank. "
1803.08383,"— head-up displays (huds) were the usual viewpoints of designed originally to present at the pilot the main sensor data during aircraft missions, because of placing instrument information in the forward ﬁeld of view enhances pilots’ ability to utilize both instrument and environmental information simultaneously. the ﬁrst civilian motor vehicle had a monochrome hud that was released in 1988 by general motors as a technological improvement of head-down display (hdd) interface, which is commonly used in automobile industry. the hud reduces the number and duration of the drivers sight deviations from the road, by projecting the required information directly into the drivers line of vision. there are many studies about ways of presenting the information: standard one-earpiece presentation, two-earpiece three-dimensional audio presentation, visual only or audio-visual presentation. results [1] have shown that using a 3d auditory display the time of acquiring targets is approximately 2.2 seconds faster than using a one-earpiece way. nevertheless, a disadvantage [2] is when the driver’s attention unconsciously shifts away from the road and goes focused on processing the information presented by the hud. by this reason, the time, the way and the channel are important to represent the information on a hud. a solution is a context aware multimodal proactive recommended system [3] that features personalized content combined with the use of car sensors to determine when the information has to be presented. fig. 1. jaguar head-up display simultaneously [5]. so it is important to have a suitable information presenting model to avoid distractions of drivers attention and to manage the risk level of each indicator. this model has to be able to select the sensorial channel to use. on this paper, we discuss about the applications of a hud on "
1803.08384," magnetic ﬁeld threading a black hole ergosphere is believed to play the key role in both driving the powerful relativistic jets observed in active galactic nuclei and extracting the rotational energy from a black hole via blandford-znajek process. the magnitude of magnetic ﬁeld and the magnetic ﬂux in the vicinity of a central black hole is predicted by theoretical models. on the other hand, the magnetic ﬁeld in a jet can be estimated through measurements of either the core shift effect or the brightness temperature. in both cases the obtained magnetic ﬁeld is in the radiating domain, so its direct application to the calculation of the magnetic ﬂux needs some theoretical assumptions. in this paper we address the issue of estimating the magnetic ﬂux contained in a jet using the measurements of a core shift effect and of a brightness temperature for the jets, directed almost at the observer. the accurate account for the jet transversal structure allow us to express the magnetic ﬂux through the observed values and an unknown rotation rate of magnetic surfaces. if we assume the sources are in a magnetically arrested disk state, the lower limit for the rotation rate can be obtained. on the other hand, the ﬂux estimate may be tested against the total jet power predicted by the electromagnetic energy extraction model. the resultant expression for power depends logarithmically weakly on an unknown rotation rate. we show that the total jet power estimated through the magnetic ﬂux is in good agreement with the observed power. we also obtain the extremely slow rotation rates, which may be an indication that the majority of the sources considered are not in the magnetically arrested disk state. keywords: active galaxies, jets, bl lacertae objects, non-thermal radiation, magnetic ﬂux "
1803.08386,"—we derive sufﬁcient conditions for the solvability of the state estimation problem for a class of nonlinear control time-varying systems which includes those, whose dynamics have triangular structure. the state estimation is exhibited by means of a sequence of functionals approximating the unknown state of the system on a given bounded time interval. more assumptions guarantee solvability of the state estimation problem by means of a hybrid observer. index terms—nonlinear systems, observability, state estimation, hybrid observers. "
1803.08388, we prove that ricci-ﬂat vacuum solutions are stable under linear perturbations in a new class of weakly non-local gravitational theories ﬁnite at the quantum level. we also show that schwarzschild black holes can form by gravitational collapse. 
1803.08389," hydrodynamics with both vector and axial currents is under study within a simple holo- graphic model, consisting of canonical u (1)v × u (1)a gauge ﬁelds in asymptotically ads5 black brane. when gravitational backreaction is taken into account, the chiral electric sepa- ration eﬀect (cese), namely the generation of an axial current as the response to an external electric ﬁeld, is realized naturally. via ﬂuid/gravity calculation, ﬁrst-order transports in the hydrodynamic constitutive relations are evaluated analytically in terms of vector chemical potential µ and axial chemical potential µ5, as well as the temperature t . apart from the proportionality factor µµ5, the cese conductivity is found to depend on the dimensionless quantities µ/t and µ5/t nontrivially. as a complementary study, frequency-dependence of the transports within this holographic model is revealed through the linear response analysis, demonstrating agreement with the results obtained from ﬂuid/gravity calculation. 8 1 0 2   r a   m 2 2     ] h t - p e h [     1 v 9 8 3 8 0 . 3 0 8 1 : v i x r a ∗ yybu@hit.edu.cn † cairg@itp.ac.cn ‡ yangqing@bnu.edu.cn § yunlong.zhang@apctp.org contents "
1803.08390," using more than 6.7 billions of trades, we explore how the tick-by-tick dynamics of limit order books depends on the aggregate actions of large investment funds on a much larger (quarterly) timescale. in particular, we ﬁnd that the well-established long memory of market order signs is markedly weaker when large investment funds trade either in a directional way and even weaker when their aggregate participation ratio is large. conversely, we investigate to what respect a weaker memory of market order signs predicts that an asset is being actively traded by large funds. theoretical arguments suggest two simple mechanisms that contribute to the observed eﬀect: a larger number of active meta-orders and a modiﬁcation of the distribution of size of meta-orders. empirical evidence suggests that the number of active meta-orders is the most important contributor to the loss of market order sign memory. 1 "
1803.08391,". we study various compactiﬁcations of moduli space of newton maps. mainly, we focus on git compactiﬁaction and deligne-mumford compactiﬁcation. then we explore the relations among these compactiﬁcations. for a degree d ≥ 2 monic polynomial p with distinct roots, its newton maps is deﬁned by "
1803.08392," the prevalent interpretation of g¨odel’s second theorem states that a suﬃciently adequate and consistent theory t does not prove its con- sistency. in this paper, we will re-examine the justiﬁcation of this philo- sophical interpretation. detlefsen’s stability problem challenges such a justiﬁcation by requiring that every sentence (in the language of t ) ex- pressing t -consistency has to be shown to be unprovable in t . we will argue that the usual attempts to meet this challenge do not provide a satisfactory solution, since they employ speciﬁc g¨odel numberings, which can be seen as arbitrarily chosen “coordinate systems” in the process of arithmetisation. a satisfactory solution to the stability problem there- fore has to be based on a more general version of g¨odel’s second theorem independent of such a coordinate system. we will propose such a solu- tion by proving the invariance of g¨odel’s second theorem with regard to acceptable numberings. 1 "
1803.08393,". as the frontiers of applied statistics progress through increas- ingly complex experiments we must exploit increasingly sophisticated inferential models to analyze the observations we make. in order to avoid misleading or outright erroneous inferences we then have to be increasingly diligent in scrutinizing the consequences of those modeling assumptions. fortunately model-based methods of statistical inference naturally deﬁne procedures for quantifying the scope of inferential out- comes and calibrating corresponding decision making processes. in this paper i review the construction and implementation of the particular procedures that arise within frequentist and bayesian methodologies. 8 1 0 2   r a   m 2 2     ] e m . t a t s [     1 v 3 9 3 8 0 . 3 0 8 1 : v i x r a michael betancourt is a research scientist at symplectomorphic, llc. (e-mail: betanalpha@gmail.com). 1 2 betancourt contents inference 1 mathematical preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 model-based sensitivities and calibration . . . . . . . . . . . . . . . . . . . . . . 3.1 the model conﬁguration space . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 model-based loss functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 sensitivity and calibration of frequentist inference . . . . . . . . . . . . . . 3.3.1 frequentist inference . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.2 the frequentist calibration criterion . . . . . . . . . . . . . . . . . 3.3.3 frequentist methods in practice . . . . . . . . . . . . . . . . . . . . 3.4 sensitivity and calibration of bayesian inference . . . . . . . . . . . . . . . 3.4.1 bayesian inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.2 the bayesian calibration criterion . . . . . . . . . . . . . . . . . . . 3.4.3 bayesian methods in practice . . . . . . . . . . . . . . . . . . . . . . 3.5 limitations of model-based calibration . . . . . . . . . . . . . . . . . . . . 4 calibrating discovery claims . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1 partitioning the model conﬁguration space . . . . . . . . . . . . . . . . . . 4.2 frequentist null hypothesis signiﬁcance testing . . . . . . . . . . . . . . . 4.2.1 point hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.2 point null hypotheses and complex alternative hypotheses . . . . 4.2.3 complex hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.4 the likelihood ratio test . . . . . . . . . . . . . . . . . . . . . . . . 4.3 bayesian model comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 posterior probability of the region of practical equivalence . . . . . . . . 4.5 predictive scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 applications to limit setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 frequentist limit setting with anchored conﬁdence intervals . . . . . . . . 5.2 bayesian limit setting with posterior quantiles . . . . . . . . . . . . . . . . 6 conclusions and future directions . . . . . . . . . . . . . . . . . . . . . . . . . . 7 acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . references . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 6 6 8 8 9 9 11 13 13 15 16 17 19 19 22 22 23 26 26 27 30 31 33 33 33 34 35 35 model-based calibration 3 as observations and experiments become more sophisticated, and we ask correspondingly more detailed questions about the world around us, we must consider increasingly more complex inferential models. the more complex the model, however, the more subtle the corresponding inferences, and the decisions informed by those inferences, will behave. consequently understanding how inferences and decisions vary across the the many pos- sible realizations of a measurement becomes a critical aspect in the design and preliminary evaluation of new observational eﬀorts. such sensitivity analyses have a long history in the applied sciences but they are often built upon heuristics. fortunately, formal methods of statistical inference naturally admit procedures for understanding and then calibrating the inferential consequences of measurements within the scope of a statistical model. the ex- act mathematical construction of this calibration, and the crucial implementation details, depend critically on the exact form on inference that we consider. in this paper i review how inferential outcomes are formally calibrated within both the frequentist and bayesian perspectives. i discuss not only the procedures but also the conceptual and practical challenges in implementing these procedures in practice, and demonstrate their application towards calibrating traditional discovery and limit setting results. 1. mathematical preliminaries in order to be as explicit as possible when introducing new functions i will use the conventional mathematical notation. a function, f , that maps points x in a space x to points y = f (x) in a space y is denoted f : x → y x (cid:55)→ f (x). rn . the real number line will be denoted r with the n -dimensional real numbers denoted sets of objects are denoted with curly braces, {. . .}, and a vertical line in between braces denotes a selection condition which deﬁnes a set. for example, {x ∈ x | f (x) = 0} deﬁnes the subset of points in x that satisﬁes the condition f (x) = 0. finally, x ∼ π(x) implies that the space x (cid:51) x is endowed with the probability distribution, π. if the left- hand side is decorated with a tilde, ˜x ∼ π(x) then this implies that ˜x is a sample from the probability distribution π. 4 betancourt measurement process latent system being studied space of observations, y true data generating process, π∗ observation, ˜y inferences and decisions fig 1. inference is an inductive process that aims to quantify which phenomenological behaviors of a latent system are consistent with measured observations. formally we assume that the measurement process deﬁnes a space of observations, y , and a probability distribution quantifying the variation in those observations, π∗. any realization of the measurement process results in an observation, ˜y, from which we base our inferences about the latent system and any resulting decisions about how to interact with the latent system. 2. inference ultimately statistics is a tool to learn about the phenomenological behavior of some latent system, for example the internal structure and dynamics of a subatomic particle, the phenotypical encoding of a genome, or the response of a population of individuals to a particular stimulus. although we cannot observe these phenomena directly we can probe them through measurements of the system, or more precisely measurements of how the system interacts with a surrounding environment. these experimental probes can be passive, assembling and analyzing data collected for other purposes, or active, collecting data from dedicated experiments. formally any measurement process deﬁnes a measurement space, y , containing all of the possible realizations of a measurement. these realizations, or observations are inherently stochastic, varying from measurement to measurement. if we assume that this variation is suﬃciently well-behaved, then we can mathematically quantify it with a probability distri- bution over y . i will refer to any probability distribution over the measurement space as a data generating process. under this assumption observations of a given system are modeled as independent samples from some true data generating process, π∗. inference is any procedure that uses observations to inform our understanding of the latent system and its behaviors (figure 1). because of the inherent stochasticity of the measurement process, however, any ﬁnite observation will convey only limited information. consequently inference fundamentally concerns itself with quantifying the uncertainty in this understanding. in particular, there will be many phenomenological behaviors consistent with a given model-based calibration 5 observation and, in general, those behaviors that appear most consistent will vary along with the form of the observation itself. here i will deﬁne the sensitivity of an experiment as the distribution of inferential outcomes induced by the variation in the observations. ideally our inferences would be accurate and capture the true behavior of the latent system regardless of the details of particular observation, but there are no generic guarantees. consequently in practice we must be careful to study these sensitivities with respect to our inferential goals. sensitivity analyses become even more important when we consider decisions informed by our inferences. conventions in many ﬁelds focus not on reporting uncertainties but rather making explicit claims about the latent system being studied. these claims commonly take the form of discovery, where a particular phenomenon is claimed to exist or not exist. in order to limit the possibility that we falsely claim to have discovered the presence, or absence, of a phenomenon we have to carefully consider the sensitivity of these claims. decisions, however, are not always so obvious. even the simple presentation of our inferences requires implicit decisions in the form of how we summarize and communicate our results. to ensure that we are not biasing our audience or ourselves we have to consider how this presentation would vary with the underlying observations. ultimately, in order to ensure robust analyses we have to carefully calibrate the conse- quences of our inferences. first consider a set of actions that we can take, a ∈ a. assuming a perfect characterization of the latent system of interest we could theoretically quantify the relative loss of taking a given action with the true loss function, l∗ : a → r a (cid:55)→ l∗(a). if we convolving this loss function with an inferential decision-making process that maps observations to actions, ˆa : y → a y (cid:55)→ ˆa(y), we induces a true inferential loss function, ˆa : y → r l∗ y (cid:55)→ l∗(ˆa(y)). a sensitivity analysis considers the distribution of l∗ more optimistic readers might also consider the equivalent utility function, u∗(a) ≡ −l∗(a). ˆa(y) with varying observations, y, while a calibration considers expected values of the loss function over the possible obser- vations. for example, we might aim to calibrate an experiment to ensure that the average loss is below a certain value or that a particular quantile of the loss distribution is below a certain value. 6 betancourt π∗ π∗ py (a) m (b) py fig 2. (a) the true data generating process, π∗, belongs in the space of all possible data generating processes over the measurement space, py . (b) model-based inferences consider a model conﬁguration space, m, which contains only a limited subset of all of those possible data generating processes. these analyses, however, require that we know both the true behavior of the latent system, so that we can quantify the relative loss of each action, and the true data generating process, so that we can quantify the variation of possible observations. in practice we don’t know the true nature of the latent system or the true data generating process of the resulting measurements, but we can model them. statistical models quantify the scope of possible data generating processes, allowing us to quantify the sensitivity of inferences and then construct formal calibrations within that scope. 3. model-based sensitivities and calibration inferences that explicitly model the measurement process, or model-based inferences, naturally deﬁne the scope of the possible observations, leaving practitioners to employ their domain expertise to construct relevant loss functions. in this section i review the model conﬁguration space that underlies model-based infer- ence and how the frequentist and bayesian paradigms utilize this space to deﬁne inference and calibrate inferential outcomes. 3.1 the model conﬁguration space a statistical model establishes a model conﬁguration space, or a collection of data gener- ating processes. ideally the model conﬁguration space is designed to be suﬃciently rich to either contain the true data generating process or, more realistically, contain data generat- ing processes suﬃciently similar to the true data generating process within the resolution of our experimental probe (figure 2). each individual data generating process in the model conﬁguration space will be denoted a model conﬁguration. typically the model conﬁguration space admits a parameterization in which each model conﬁguration can be identiﬁed with a parameter value, θ ∈ θ. often these parameters decompose into components, θ = {θ1, . . . , θn} model-based calibration 7 model conﬁguration space, π(y | θ), θ ∈ θ measurement process inferences and decisions latent system being studied space of observations, y true data generating process, π∗ observation, ˜y fig 3. in model based inference the measurement process is modeled with a model conﬁguration space, θ. inferences identify which model conﬁgurations, θ ∈ θ, are consistent with a given observation, which then informs the properties of the true data generating process and the latent system under investigation. with each component responsible for quantifying only one aspect of the phenomenologi- cal behavior of the latent system, the environment containing the latent system, or the measurement process itself. such parameter decompositions make the model conﬁguration space particularly interpretable. moreover, if each model conﬁguration admits a density with respect to a common ref- erence measure over y then we can fully specify the model conﬁguration space with the family of densities π(y | θ) for y ∈ y and θ ∈ θ. in many applied ﬁelds this family of densities, or even the model conﬁguration space itself, is introduced as the likelihood. un- fortunately, that term has a more precise deﬁnition in the statistics literature: the phrase likelihood is used exclusively to denote the function over the parameter space given by evaluating each density at a particular measurement ˜y ∈ y , l˜y : θ → r θ (cid:55)→ π(˜y | θ). consequently i will avoid the use of likelihood unless the model conﬁguration densities are explicitly being evaluated at a given observation. once we have constructed a model conﬁguration space, inference becomes a means of identifying those model conﬁgurations that are consistent with an observation (figure 3). because there is no unique deﬁnition of this sort of consistency, there are multiple approaches to inference. the two employed most in practice are frequentist and bayesian inference. 8 betancourt 3.2 model-based loss functions given a means of quantifying those model conﬁgurations consistent with an observation, we can use that quantiﬁcation to motivate fruitful decisions about how to best interact with the latent system. for example we may want to intervene with the system or systems like it, introducing a treatment or altering the surrounding environment. alternatively we may want to decide on whether or not to claim the discovery of the absence or presence of a new phenomenon, or whether or not to follow up with another experiment. for model-based inference we can quantify the utility of a set of actions by deﬁning a loss function for each possible data generating process in our model, l : a × θ → r (a, θ) (cid:55)→ l(a, θ). given an inferential decision making process ˆa : y → a y (cid:55)→ ˆa(y), we can then deﬁne a model-based inferential loss function, lˆa : y × θ → r (y, θ) (cid:55)→ l(ˆa(y), θ). presuming that the true data generating process is contained within the model conﬁgu- ration space, sensitivity analyses and calibration within the scope of our model quantiﬁes the actual performance of our decisions. when the true data generating process is close to the model conﬁguration space then this process approximates the actual performance. that said, constructing a loss function alone is insuﬃcient to admit sensitivity analyses and calibrations. before the measurement we are ignorant of not only what the observation will be, and hence what action we will take, but also which model conﬁguration gives the true data generating process. in order to quantify the performance of our decisions we have to deﬁne the scope of possible observations and possible data generating processes. exactly how we do that depends intimately on the formal details of the inferences that we make. 3.3 sensitivity and calibration of frequentist inference frequentist inference (casella and berger, 2002; lehmann and casella, 2006; keener, 2011) derives from the interpretation that probability theory can model only the frequencies of repeatable processes. this deﬁnition is consistent with the use of probabilities to model the inherent variation of observations, but it does not allow us to deﬁne probabilities over the model conﬁguration space itself, as those probabilities would not correspond to the hypothetical frequencies of any repeatable process. ultimately this strong philosophical model-based calibration 9 assumption implies that we cannot use any form of weighting to quantify consistency in the model conﬁguration space because any self-consistent weighting is equivalent to the assignment of probabilities! consequently frequentist inference must take the form of deﬁnite decisions about which parts of the model conﬁguration space are consistent with an observation and which are not. from a frequentist perspective inference and decisions are one in the same! because such deﬁnite decisions can readily exclude the true data generating process, or model conﬁgurations close to the true data generating process, from consideration we have to carefully calibrate these decisions so that such exclusions are suﬃciently rare. ultimately frequentist inference does not deﬁne exactly how an observation informs which parts of the model conﬁguration space to keep and which to discard. rather fre- quentist inference establishes a means of calibrating any such procedure that might be considered. 3.3.1 frequentist inference any procedure that consumes an observation to produce a deﬁnite decision about which parts of the model conﬁguration space are considered con- sistent takes the mathematical form of an estimator. estimators are functions from the measurement space to subsets of the model conﬁguration space, mapping observations to subsets of model conﬁgurations, ˆθ : y → t , where t is the space of well-deﬁned subsets of the model conﬁguration space, θ. a common class of estimators are point estimators that identify a single point in the model conﬁguration space (figure 4a), ˆθ : y → θ, point estimators formalize the intuition of a “best ﬁt”, where inferences are summarized with a single point at the expense of ignoring the uncertainty inherent in learning from ﬁnite observations. the more general class of estimators that identify entire subsets of the model conﬁgu- ration space are known as conﬁdence sets (figure 4b), or conﬁdence intervals if the model conﬁguration space is one-dimensional. the nomenclature is meant to suggest that if a conﬁdence set has been properly constructed then we can be conﬁdent that these sets will contain the true data generating process for suﬃciently many observations. 3.3.2 the frequentist calibration criterion the actual choice of which estimator to employ in a given analysis is left to the practitioner. constraints that enforce desired properties can be imposed to restrict the space of potential estimators, but the choice of these desired properties remains the responsibility of the practitioner. regardless of how we ultimately select an estimator, however, we can use the model conﬁguration space to calibrate the estimator and determine it’s practical utility. 10 betancourt π∗ ˆθ(˜y) m py (a) π∗ ˆθ(˜y) m (b) py fig 4. (a) point estimators identify a single model conﬁguration, ˆθ(˜y), that is ideally close to the true data generating process, θ∗, for any given observation ˜y. (b) conﬁdence sets identify entire subsets of the model conﬁguration space that ideally contain the true data generating process for any given observation. in frequentist inference our actions are deﬁnite quantiﬁcations of the model conﬁguration space and, by construction, estimators are inferential decision making processes. in order to deﬁne a calibration criterion we we must ﬁrst construct a model-based loss function, l(ˆθ, θ), that quantiﬁes how well ˆθ identiﬁes the true data generating process, θ. substituting an estimator yields the model-based inferential loss function, lˆθ(y, θ) ≡ l(ˆθ(y), θ). as with estimators there is no canonical loss function in frequentist inference; instead one must be chosen using whatever domain expertise is available within the context of a particular analysis. in practice this choice often considers the structure of the estimator itself. if we knew that a given θ identiﬁed the true data generating process then the sensitivity of the loss of the estimator over the possible observations could be summarized with an expectation over that conﬁguration. this expectation yields an expected loss for each model conﬁguration, (cid:90) y lˆθ(θ) = dy π(y | θ) lˆθ(y, θ). because we don’t have any information about which model conﬁguration identiﬁes the true data generating process before a measurement is made, the frequentist calibration criterion is deﬁned as the maximum expected loss over all possible model conﬁgurations, ¯lˆθ = max θ lˆθ(θ). if the model conﬁguration space is suﬃciently rich that it contains the true data generating process, then this calibration criterion deﬁnes the worst case loss of our given estimator. model-based calibration 11 bounding the worst case loss of an estimator is an extremely powerful guarantee on its practical performance, but also a very conservative one as bounds can be dominated by unrealistic but not impossible data generating processes towards the boundaries of the model conﬁguration space. a natural loss function for point estimators is the lp distance between the estimated model conﬁguration and the presumed true data generating process, (y, θ) = ((θ − ˆθ(y))2)p/2. lp ˆθ the expected lp=2 loss is known as the variance of an estimator. similarly, a natural natural loss function for conﬁdence sets is inclusion of the presumed true data generating process, where the indicator function, i, is deﬁned as lˆθ(y, θ) = iθ[ˆθ(y)], (cid:26) 1, θ ∈ t 0, else . iθ[t ] = the expected inclusion loss, or coverage is simply how often the conﬁdence set contains the presumed true data generating process. while this calibration procedure can used to analyze the frequentist properties of a given estimator, they can also be used to optimize the choice of estimator. given a family of estimators, {ˆθx}, the optimal estimator will satisfy the minimax criterion, ˆθ∗ = argmin x = argmin x = argmin x l∗ ˆθx max θ max θ (cid:90) y lˆθx (θ) dy π(y | θ) lˆθx (y, θ). for example, a desired coverage might be established initially and then a conﬁdence set engineered to ensure that the coverage is met, or exceeded, for all of the data generating processes in the model conﬁguration space. 3.3.3 frequentist methods in practice aside from the conceptual challenge of choosing a loss function that enforces the needs of a given analysis, the computational burden of frequentist calibration is a signiﬁcant impediment to its application. in particular, even approximately scanning through the model conﬁguration space to identify the maximal expected loss often requires more computational resources than realistically available to a practitioner. many frequentist analyses assume suﬃciently simple model conﬁguration spaces, esti- mators, and loss functions such that the maximum expected loss can be computed ana- lytically. the analytic results allow, for example, optimal estimators to be chosen from 12 betancourt families of candidate estimators with strong guarantees on the performance of the best choice. the practical validity of these guarantees, however, requires that the true data generating process be simple enough that it can be contained within the relatively crude model conﬁguration space. for the complex experiments of applied interest this can be a dangerous assumption. without analytic results one might consider interpolative methods that bound the varia- tion in the expected loss between a grid of points distributed across the model conﬁguration space. at each of these points monte carlo methods can be used to simulate observations and approximate the expected loss, and then the properties of the loss function itself can be used to interpolate the expected loss amidst the grid points. these methods can yield reasonable results for low-dimensional model conﬁguration spaces, but as the dimensional- ity of the model increases even strong smoothness assumptions can become insuﬃcient to inform how to interpolate between the grid points. in order to avoid this curse of dimensionality frequentist analyses unaccommodating to analytic results often resort to asymptotics. asymptotic analyses assume that the model conﬁguration space is suﬃciently regular that as we consider more observations at once the behavior of the model conﬁguration space follows a central limit theorem. under these conditions the likelihood for any observation concentrates in an increasingly small neigh- borhood around the maximum likelihood estimator, θml(y) = argmax θ∈θ π(˜y | θ). moreover, in this limit the breadth of that neighborhood is given by the inverse of the fisher information matrix, i(˜y) = ∇2π(˜y | θ)(cid:12)(cid:12)θ=θml(y) . the concentration in the model conﬁguration space in this asymptotic limit admits conve- nient analytic approximations to the frequentist calibration procedure. asymptotic behavior also motivates the concept of proﬁling, which is of use when the parameter space separates into phenomenological parameters related to the underlying system of interest and nuisance or systematic parameters that are unrelated to that sys- tem but still eﬀect the data generating process. under certain conditions the observations inform the nuisance parameters faster than the the phenomenological parameters; in the asymptotic limit the uncertainty in these parameters becomes negligible and they can be replaced with conditional maximum likelihood estimates. more formally, if the parameterization of the model conﬁguration space decomposes into phenomenological parameters, ϑ, and nuisance parameters, σ, then we deﬁne the condi- tional maximum likelihood estimator as ˆσ(ϑ, ˜y) = argmax σ π(˜y | ϑ, σ) model-based calibration 13 and the corresponding proﬁle likelihood as ˆπ(˜y | ϑ) = π(˜y | ϑ, ˆσ(ϑ, ˜y)). the proﬁle likelihood can then be used to calibrate estimators of the phenomenological parameters, at least in this limit. the utility of these asymptotic methods depends critically on the structure of the model conﬁguration space and its behavior as we consider more observations. simpler models typically converge to the asymptotic limit faster and hence require fewer data for asymp- totic calibrations to be reasonably accurate. more complex models, however, converge more slowly and may require more data than is practical, or they may not satisfy the necessary conditions to converge at all. consequently it is crucial to explicitly verify that the asymptotic regime has been reached in a given analysis. as with analytic methods, one has to be especially careful to not employ an over-simplistic model to facilitate the applicability of the asymptotic results while compromising the practical validity of the resulting calibration. 3.4 sensitivity and calibration of bayesian inference bayesian inference (bernardo and smith, 2009; gelman et al., 2014) broadens the inter- pretation of probability theory, allowing it to be used to not only model inherent variation in observations but also provide a probabilistic quantiﬁcation of consistency between the data generating processes in the model conﬁguration space and observations. this generalization manifests in a unique procedure for constructing inferences which can then be used to inform decisions. ultimately bayesian inference decouples inference from decision making, making the assumptions underlying both more explicit and often easier to communicate. moreover, the fully probabilistic treatment of the bayesian perspective immediate deﬁnes a procedure for constructing sensitivities and calibrations. 3.4.1 bayesian inference bayesian inference compliments the data generating processes in the model conﬁguration space with a prior distribution over the model conﬁguration space itself. the prior distribution quantiﬁes any information on which model conﬁgura- tions are closer to the true data generating process than others that is available before a measurement is made. this information can come from, for example, physical considera- tions, previous experiments, or even expert elicitation. careful choices of the prior distribu- tion can go a long way towards regularizing unwelcome behavior of the model conﬁguration space. together the model conﬁguration space and the prior distribution deﬁne the bayesian joint distribution over the measurement space and the parameter space, π(y, θ) = π(y | θ)π(θ). the titular bayes’ theorem conditions this joint distribution on an observation, ˜y, to give 14 betancourt π∗ π∗ m (a) py m (b) py fig 5. (a) bayesian inference begins with a prior distribution, shown here in dark red, over the model conﬁguration space, m, that quantiﬁes information available before a measurement. (b) information encoded in an observation updates the prior distribution into a posterior distribution that ideally concentrates around the true data generating process, π∗. the posterior distribution, π(θ | ˜y) = π(˜y, θ) π(˜y) ∝ π(˜y | θ) π(θ). in words, the prior distribution quantiﬁes information available before the measurement, the model conﬁguration space decodes the information within an observations, and the posterior distribution combines both sources of information to quantify the information about the latent system being studies after a measurement (figure 5). any well-posed statistical query we might make of our system reduces to interroga- tions of the posterior distribution. mathematically this must take the form of a posterior expectation for some function, f , e[f ] = dθ π(θ | ˜y) f (θ). for example, we might consider the posterior mean or median to identify where the pos- terior is concentrating or the posterior standard deviation or the posterior quartiles to quantify the breadth of the distribution. θ posterior expectations also deﬁne a unique decision making process in bayesian inference. first we deﬁne the expected loss for a given action by averaging a model-based loss function, l(a, θ), over the posterior distribution, l(a, ˜y) = dθ π(θ | ˜y)l(a, θ). (cid:90) (cid:90) θ we can then deﬁne a decision making process by taking the action with the smallest expected loss, a∗(˜y) = min a∈a l(a, ˜y). model-based calibration 15 for example, our decision might be to summarize the posterior with a single “best ﬁt” model conﬁguration, ˆθ. given the loss function l(ˆθ, θ) = (ˆθ − θ)2 the expected losses for each possible summary becomes l(ˆθ, ˜y) = (ˆθ − µ(˜y)) + σ2(˜y), where µ(˜y) is the posterior mean and σ(˜y) is the posterior standard deviation. following the bayesian decision making process, our optimal decision is to summarize our posterior by reporting the posterior mean, ˆθ∗(˜y) = µ(˜y). 3.4.2 the bayesian calibration criterion bayes’ theorem provides a unique procedure for constructing inferences and making subsequent decisions given an observation, but there are no guarantees that these decisions will achieve any desired performance for any possi- ble observation. consequently sensitivity analysis and calibration of this decision making process across possible is still important in bayesian inference. instead of having to consider each model conﬁguration equally, however, the prior dis- tribution allows us to diminish the eﬀect of unrealistic but not impossible model conﬁgura- tions. in particular sampling from the joint distribution generates an ensemble of reasonable data generating process and corresponding observations which we can use to quantify the performance of our decisions. for example, we can quantify the sensitivity of any inferential outcome by integrating the model conﬁgurations out of the bayesian joint to give the prior data generating process, (cid:90) (cid:90) π(y) = dθ π(y, θ) = dθ π(y | θ) π(θ). θ θ the prior data generating process probabilistically aggregates the behaviors of all of the possible data generating process in the model conﬁguration space into a single probability distribution over the measurement space. we can then analyze the sensitivity of any in- ferential outcome by running our analysis over an ensemble of observations sampled from this distribution. moreover, we can calibrate a decision making process by integrating a model-based loss function against the full bayesian joint distribution, ¯la = dy dθ lˆa(y, θ)π(θ, y). this calibration immediately quantiﬁes the expected loss as both the observations and data generating processes vary within the scope of our model. y,θ (cid:90) 16 betancourt 3.4.3 bayesian methods in practice the uniﬁed probabilistic treatment of bayesian in- ference ensures that all calculations take the form of expectation values with respect the bayesian joint distribution, its marginals, such as the prior distribution and the prior data generating process, or its conditionals, such as the posterior distribution. consequently calculating expectation values, or more realistically accurately estimating them, is the sole computational burden of bayesian inference. posterior expectations are challenging to compute, and indeed much of the eﬀort on the frontiers of statistical research concerns the development and understanding of ap- proximation methods. one of the most powerful and well-understood of these is markov chain monte carlo (robert and casella, 1999; brooks et al., 2011) and its state of the art implementations like hamiltonian monte carlo (betancourt, 2017). on the other hand, expectations with respect to the bayesian joint distribution are often amenable to much simpler monte carlo methods. in particular, if we can draw exact samples from the prior distribution and each of the data generating processes in the model conﬁguration space then we can generate joint samples with the sequential sampling scheme ˜θ ∼ π(θ) ˜y ∼ π(y | ˜θ). for each simulated observation, ˜y, we can construct a subsequent posterior distribution, make posterior-informed decisions, and then compare those decisions to the simulated truth, ˜θ. as we generate a larger sample from the bayesian joint distribution we can more accurately quantify our sensitivities and calibrations. we can also quantify how sensitivity a calibration is to a particular component of the parameter space, ϑ, by sampling the complementary parameters, σ, from the corresponding conditional prior distribution, ˜σ ∼ π(σ | ϑ) ˜y ∼ π(y | (ϑ, ˜σ)). this allows us, for example, to see how our decision making process behaves as for various phenomenlogical behaviors. interestingly, the application of monte carlo to the bayesian joint distribution is not at all dissimilar to many of the heuristic schemes common in the sciences. sampling ˜y ∼ π(y | ˜θ) just simulates the experiment conditioned on the model conﬁguration, ˜θ. the addition step θ ∼ π(θ) simply simulates model conﬁgurations consistent with the given prior information instead of selecting a few model conﬁguration by hand. one inferential outcome immediately amenable to calibration is the approximation of posterior expectations themselves. cook, gelman and rubin (2006), for example, introduce a natural way to calibrate the estimation of any posterior quantiles. this then immediately provides a procedure for quantifying the accuracy of any algorithm that yields deterministic approximations to posterior quantiles, for example as demonstrated in yao et al. (2018). model-based calibration 17 bayesian sensitivity analysis is particularly useful for identifying known pathologies in bayesian inference by carefully examining the simulated analyses. consider, for example, the posterior z-score for the parameter component, ˜θn, (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) µn(˜y) − ˜θn σn(˜y) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) , zn = where µn(˜y) denotes the posterior mean of ˜θn and σn(˜y) the corresponding posterior stan- dard deviation. the posterior z-score quantiﬁes how much the posterior distribution en- velops the presumed true data generating process along this direction in parameter space. at the same time consider the posterior shrinkage of that parameter component, sn = 1 − σ2 n(˜y) τ 2 n(˜y) , were τn(˜y) is the prior standard deviation of ˜θn. the posterior shrinkage quantiﬁes how much the posterior distribution contracts from the initial prior distribution. an ideal experiment is extremely informative, with large shrinkage for every observation, while also being accurate, with small z-scores for every observation. in this case the distri- bution of posteriors derived from prior predictive observations should concentrate towards small z-scores and large posterior shrinkages for each parameter component. on the other hand, small posterior shrinkage indicates an experiment that poorly identiﬁes the given parameter component, while large z-scores indicates inferences biased away from the true data generating process. we can readily visualize this behavior by plotting the posterior z- score verses the pos- terior shrinkage. concentration to the top right of this plot indicates overﬁtting, while concentration to the top left indicates a poorly-chosen prior that biases the model conﬁgu- ration space away from the presumed true data generating process (figure 6a). because the bayesian joint distribution considers only those true data generating consistent with the prior, however, this latter behavior should be impossible within the scope of a model-based sensitivity analysis. by investigating this simple summary we can quickly identify problems with our ex- perimental design (figure 6b, c). a scatter plot that combines the outcomes for all of the parameters components into one plot ﬁrst summarizes the aggregate performance of the entire model, and then individual plots for each parameter component can be used to isolate the source of any noted pathological behavior. 3.5 limitations of model-based calibration the ultimate limitation of model-based calibration is its dependence on the model con- ﬁguration space. any model-based sensitivities or guarantees claimed by model-based cal- ibrations rely on the model conﬁguration space being rich enough to capture the true data 18 betancourt (a) (b) (c) fig 6. (a) the distribution of posterior z-scores and posterior shrinkages with respect to the bayesian joint distribution identiﬁes well-known pathologies of bayesian inference. the sensitivity of the posterior to the observations within the scope of a given model can be summarized by sampling from the bayesian joint distribution, constructing posteriors, and then plotting the corresponding posterior z-scores and shrinkages. (b) this model demonstrates good behavior for most observations, with a small tail of outcomes that overﬁt to a few observations. (c) this model is weakly identiﬁed, with the information introduced by the data doing little to improve upon the information encoded in the prior distribution. model-based calibration 19 generating process, or at least contain model conﬁgurations that approximate it suﬃciently well. unfortunately it is diﬃcult to quantify how these guarantees might change as the as the model conﬁgurations become worse approximations to the true data generating process. consequently it is up to the user to verify the suﬃciency of the assumed model conﬁguration space with, for example, predictive validations such as residual analysis for frequentist point estimators or posterior predictive checks for bayesian analyses. another point of fragility of model-based sensitivity and calibrations is that they apply only for the exact models and decisions being considered. if those models or decisions are tweaked then the guarantees no longer need apply. the only way to ensure valid calibrations is to recompute them every time the experiment is modiﬁed. an consequence of this fragility is that any sensitivity or calibration is suspect whenever the construction of the model itself depends on the observed data! the only rigorous way to maintain the validity of these results is to consider a larger model that incorporates this implicit dependence of the model conﬁguration space on the observed data. because observations are often used to critique and ultimately tune the model, this vul- nerability is almost impossible to avoid in practice. consequently model-based calibration is perhaps best considered as a tool for identifying poorly inferential behaviors in a model rather than making absolute guarantees about its performance. 4. calibrating discovery claims one of the most common decisions made in the applied sciences is whether or not to claim that a phenomenon in the system being studied exists or doesn’t exist. whether such discovery claims are good scientiﬁc practice is debatable, but given their prevalence it is important to be able to calibrate these decisions regardless. many decision making processes have been developed within in both statistics and ap- plied ﬁelds, and many of these methods have come under recent scrutiny given their failure to replicate in subsequent experiments. the underlying issue in these failed replications is often poor calibration of the original discovery claim. in this section i review how discovery claims can be constructed from a statistical model both in the frequentist and bayesian paradigms and discuss some of the practical issues with their calibration. 4.1 partitioning the model conﬁguration space in order to decide on the presence of a phenomenon we need to partition the model conﬁguration space into those model conﬁgurations that are inﬂuenced by the phenomenon and those that are not. for example we might partition the model conﬁguration space into data generating processes containing only background sources and those containing both signal and background sources. alternatively we might consider a partition into model conﬁgurations where two phenomena manifest distinct behaviors and those where they 20 betancourt behave identically. discovery claims are then informed by which of the two partitions is more consistent with the observed data. let the phenomenon of interest be characterized with a subset of parameters, ϑ = (θ), where  projects the total parameter space onto the phenomenological parameters of interest. additionally assume that the parameters are structured such that ϑ = 0 identiﬁes those data generating processes not inﬂuenced by the phenomenon being considered. in this case the model conﬁguration space partitions into an absence model θ1 = {θ ∈ θ | (θ) = 0} , and a complementary presence model, θ2 = {θ ∈ θ | (θ) (cid:54)= 0} (figure 7). this includes the case where ϑ is constrained to be positive, in which case the presence model reduces to θ > 0, or the more general case where ϑ is unconstrained and the presence model includes all positive and negative, but non-zero, parameters. for example, consider a model where our observations are generated from overlapping signal and background sources, µs and µb respectively, with gaussian measurement vari- ability σ. this yields the model conﬁguration space π(y | µs, µb, σ) = n (y | µs + µb, σ). if we are interested in understanding the signal then we would consider the projection µs = ϑ = (µs, µb, σ), with the absence model deﬁned as µs = 0, regardless of the value of the nuisance parameters µb and σ, and the presence model as the complement with µs (cid:54)= 0. because we don’t know which of the two partitions contains the true data generating process we have to calibrate our decisions with respect to both. in particular we have to consider the probability of claiming a discovery when the true data generating process is in the presence model and when it is in the absence model. we can fully characterize the four possible outcomes with two probabilities: the false discovery rate, fdr and the true discovery rate, tdr (table 1). in classical statistics the false discovery rate is also known as the type i error while one minus the true discovery rate is also known as the type ii error. given these probabilities we can compute the expected loss once we have assigned losses to the possible decision outcomes. for example, let l1 be the loss associated with claiming a discovery when the true data generating process is in the absence model and l2 the possibly negative loss associated with claiming a discovery when the true data generating process is in the presence model. the expected loss for claiming a discovery is then given by lclaim discovery = (1 − fdr) l1 + tdr l2. model-based calibration 21 absence model ϑ = 0 presence model full model ϑ < 0 ϑ = 0 ϑ > 0 ϑ < 0 ϑ > 0 fig 7. when claiming a discovery we ﬁrst identify the phenomenological parameters, ϑ, which quantify the behavior of a phenomena of interest. the remaining nuisance parameters quantify the behavior of the environment in which the phenomenon interacts and the the measurement process itself. when ϑ = 0 identiﬁes a vanishing phenomenon we can partition the full model conﬁguration space into an absence model of those model conﬁgurations satisfying ϑ = 0 and a presence model of those model conﬁgurations satisfying ϑ (cid:54)= 0. discovery is claimed when the presence model is strongly preferred over the absence model by an observation, for various deﬁnitions of “preferred”. truth decision claim θ∗ ∈ θ1 claim θ∗ ∈ θ2 θ∗ ∈ θ1 (no phenomenon) θ∗ ∈ θ2 (phenomenon) fdr 1 − tdr 1 − fdr tdr when preparing to claim a discovery we have to consider the range of inferential outcomes both for when the true data generating process, θ∗, is an element of the absence model, θ∗ ∈ θ1, and when it is an element of the presence model, θ∗ ∈ θ2. given the false discovery rate, fdr, and true discovery rate, tdr, we can compute any expected loss with respect to the claiming a discovery or not. table 1 22 betancourt truth decision claim θ∗ ∈ θ1 claim θ∗ ∈ θ2 claim nothing θ∗ ∈ θ1 (no phenomenon) θ∗ ∈ θ2 (phenomenon) p1 p(cid:48) 1 p2 p(cid:48) 2 p3 p(cid:48) 3 a decision process where we can avoid making any claim about the phenomenon of interest by not publishing our analysis is characterized by six probabilities, four of which are independent because of the normalization constraints,(cid:80)3 n=1 pn = 1 and(cid:80)3 n=1 p(cid:48) n = 1. table 2 in practice we need not limit ourselves to dichotomous decisions. we could also consider a decision process that claims the phenomenon exists, claims the phenomenon doesn’t exist, or makes no claim at all. this process would be characterized by six probabilities, four of which are independent (table 2). importantly the expected loss of a discovery claim can be miscalculated if we ignore the possibility that an analysis may not be reported for some observations and the two additional degrees of freedom needed to quantify the expected loss in this more general circumstance. 4.2 frequentist null hypothesis signiﬁcance testing the conventional approach to claiming discoveries in a frequentist framework is the null hypothesis signiﬁcance testing framework. here the null hypothesis that the true data generating process falls into the absence model is treated as something of a strawman set up to be rejected by observation. in order to reject the null hypothesis we consider how extreme an observation is with respect to the model conﬁgurations in the absence model. the more extreme our rejection threshold the smaller the false discovery rate should be. naively, if the null hypothesis is rejected then we are left with only the alternative hypothesis that the true data generating process falls into the presence model. that said, we can’t simply reject the null hypothesis in isolation — a poor ﬁt to the null hypothesis does not imply that the alternative hypothesis is any more consistent with the observation! at the very least we have to consider also how likely we are to reject the null hypothesis when the alternative hypothesis is true. exactly how the null hypothesis signiﬁcance testing framework is implemented depends on the structure of the null and alternative hypotheses. the procedure is straightforward for simple hypotheses but quickly becomes diﬃcult to implement in practice as the hypotheses become more complex. 4.2.1 point hypotheses the simplest case of null hypothesis signiﬁcance testing is when both the null hypothesis and alternative hypothesis are point hypotheses consisting of a single model conﬁguration each. in this case we’ll denote the lone data generating processes in the absence model πn (y) and the lone data generating process in the precence model πa(y). model-based calibration 23 for a point null hypothesis we can quantify the extremity of an observation, ˜y, with a tail probability or p-value, p(˜y) = dy πn (y). (cid:90) ∞ ˜y the integral might be computed analytically or with numerical methods such as quadrature for low-dimensional measurement spaces and monte carlo for high-dimensional measure- ment spaces. if we reject the null hypothesis when p(˜y) < 1 − α for some signiﬁcance, α, then by construction the false discovery rate of our claim will be (cid:90) (cid:90) 1 y fdr = dy πn (y) i [ p(y) < 1 − α] d˜p i [ ˜p < 1 − α] = = 1 − α. 0 by tuning the signiﬁcance of the null hypothesis test we can immediately achieve whatever false discovery rate is desired in a given application. the true discovery rate, also known as the power of the null hypothesis test, is the average null p-value with respect to the alternative data generating process, (cid:90) tdr = β = 1 − dy πa(y) p(y). y provided that the power is suﬃciently high, observations for which the null hypothesis is rejected will be more consistent with the alternative hypothesis in expectation. there is no guarantee, however, that the alternative will actually be more consistent for every observation. unlike the false discovery rate, the true discovery rate is a consequence of the assumed model and cannot be tuned once a signiﬁcance has been set. consequently unsatisfactorily low true discovery rates can be remedied only by modifying the experimental circumstances, for example by increasing the number of observations included in each measurement. 4.2.2 point null hypotheses and complex alternative hypotheses null hypothesis sig- niﬁcance testing becomes more complicated when the model conﬁguration space no longer consists of just two data generating processes and both hypotheses cannot be point hy- potheses. consider next the situation where the null hypothesis is still given by a single data generating process, θ = 0, and the alternative hypothesis contains the remaining model conﬁgurations θ (cid:54)= 0. this might arise, for example, when our model contains only 24 betancourt one phenomenological parameter and θ = 0 uniquely deﬁnes the circumstance where the phenomenon is absent. as before we can deﬁne the p-value, p(˜y) = (cid:90) ∞ dy π(y | θ = 0), and then reject the null hypothesis when ˜y p(˜y) < 1 − α to ensure a given false discovery rate. unfortunately there is no longer a unique way of deﬁning a power that gives the true discovery rate because the true discovery rate will, in general, be diﬀerent for each of the model conﬁguration in the alternative hypothesis, (cid:90) βθ = 1 − dy π(y | θ) p(y), θ (cid:54)= 0. y if θ is one-dimensional then we can visualize the sensitivity of the p-values as a function of θ (figure 8). when θ is two-dimensional we can no longer visualize the full variation of the sensitivity distribution, but we can visualize the variation of a summary statistic such as the power (figure 9). visualizations allow us to identify regions in the alternative model conﬁguration space of high power, but they do not deﬁne a unique power or true discovery rate for the test. one immediate strategy is to deﬁne an overall power is to consider the minimum power over all of the model conﬁgurations in the alternative hypothesis, β ≡ min θ(cid:54)=0 βθ. provided that we could accurately compute the minimum, this deﬁnition would ensure that the overall power lower bounds the true discovery rate for all model conﬁgurations in the alternative hypothesis. unfortunately, when the null hypothesis is nested within the alternative hypothesis the power can become arbitrarily small for the alternative model conﬁgurations in the neighborhood around the lone null model conﬁguration. consequently in practice we will generally be able to claim calibrated discoveries only for a subset of the data generating processes in the alternative hypothesis. in order to visualize how the power varies with the phenomenological parameters, ϑ, we might also consider deﬁning conditional powers. if the parameters partition into (ϑ, σ), with ϑ ∈ (θ) and σ ∈ (θ)c, then we could deﬁne the conditional power as (cid:90) y βϑ = 1 − max σ∈(θ)c dy π(y | ϑ, σ) p(y). model-based calibration 25 (a) (b) fig 8. when the model conﬁguration space is one-dimensional we can visualize the sensitivity of the null p-values for each of the model conﬁgurations in the alternative hypothesis, θ (cid:54)= 0. this allows us to quickly communicate the distribution of (a) the null p-values, p(y) or the equivalent (b) standard scores z(y) = φ−1(1 − p(y)). here the distributions are summarized with their medians in dark red and with the quartiles in light red. in particular we can identify which θ in the alternative hypothesis achieve a given true discovery rate, here 0.999, by seeing where the central value of the distributions surpass a dashed line. fig 9. when the model conﬁguration space is two-dimensional we can visualize how the power, βθ, varies with the model conﬁgurations in the alternative hypothesis, here (θ1, θ2) (cid:54)= (0, 1). as in the one-dimensional case we can vary the decoration to identify those model conﬁgurations achieving a given true discovery rate, here 0.999, shown in red. 26 betancourt because the optimization of the nuisance parameters, σ, may be infeasible, proﬁle likelihood methods are often utilized to approximate the conditional powers for visualization. as with any application of asymptotics, the validity of this approximations must be carefully veriﬁed for the visualization to be useful. finally we might acknowledge the conceptual advantage of having dual point hypotheses and consider not one null hypothesis test but rather an inﬁnite number of tests, where each tests is deﬁned with respect to the point null hypothesis θ = 0, and one of the alternative model conﬁgurations, θ(cid:48) (cid:54)= 0. when we reject the null hypothesis we reject it for any of the alternatives. the preponderance of alternative hypotheses, however, signiﬁcantly increases the false discovery rate unless we apply a multiple comparison correction to the signiﬁcance threshold. this increase in the false discovery rate by separating the alternative hypothesis into many point hypothesis is also known as the “look elsewhere eﬀect” in particle physics. 4.2.3 complex hypotheses unsurprisingly, implementing null hypothesis signiﬁcance test- ing becomes all the more diﬃcult when neither the null hypothesis nor the alternative hy- pothesis are point hypotheses. given the inﬂuence of systematic and environmental factors present in any experiment we rarely if ever enjoy point hypotheses when using realistic models. when there are multiple model conﬁgurations in the null hypothesis we have to consider them all. for example we might deﬁne the p-value to be the smallest tail probability across all of the null model conﬁgurations, (cid:90) ∞ p(˜y) = min θ∈θ1 ˜y dy π(y | θ). if we then reject the null when this minimal p-value is less than 1−α then the false discovery rate will be at least α for every data generating process in the null hypothesis. power and true discover rate calculations proceed as above, with all of the potential complications. in this general case the computation of the optima needed to bound the false and true discovery rates becomes a particularly signiﬁcant computational burden that must be ad- dressed with a careful combination of principled assumptions and approximations. 4.2.4 the likelihood ratio test one of the diﬃculties with the null hypothesis signiﬁ- cance testing framework presented so far is the need to compute tail probabilities over the measurement space. when the measurement space is more than a few dimensions these tail probabilities are diﬃcult to accurately approximate even with substantial computa- tional resources available. a better strategy is to construct a lower-dimensional summary of the measurement space that captures the diﬀerences between the null and alternative hypotheses while admitting tests that are easier to implement. perhaps the most ubiquitous summary for testing is the likelihood ratio λ(˜y) = maxθ∈θ1 π(˜y | θ) maxθ∈θ π(˜y | θ) , model-based calibration 27 which admits the likelihood ratio test where we reject the null hypothesis if λ(˜y) < λ0 for some 0 < λ0 < 1. the false and true discovery rates of the likelihood ratio test intimately depend on the threshold, λ0, and the particular structure of the model conﬁguration space. consequently without further assumptions the likelihood ratio test has to be explicitly calibrated for every application. the assumption of asymptotics, however, admits an analytic calibration of the likelihood ratio test. wilk’s theorem demonstrates that, under the typical asymptotic conditions, the distribution of the logarithm of the likelihood ratio with respect to the model conﬁgurations in the null hypothesis asymptotically approaches a χ2 distribution, with degrees of freedom, −2 log λ ∼ χ2 k, k = dim(θ) − dim(θ1). consequently the false discovery rate for a given threshold, −2 log λ0, can be calculated by looking up the corresponding χ2 k tail probability. indeed theoretical analysis shows that the likelihood ratio test is the optimal test in this asymptotic regime. many popular tests that have been developed in applied ﬁelds, such as the feldman-cousins test (feldman and cousins, 1998), are actually instances of the likelihood ratio test for speciﬁc classes of models. 4.3 bayesian model comparison bayesian model comparison is an immediate consequence of extending a probabilistic treatment to the absence and presence partitions of the model conﬁguration space. given that it’s not conceptually more diﬃcult, however, let’s consider the more general case where we are interested in selecting between one of n models, {m1, . . . ,mn}. each model conﬁguration space can have diﬀerent dimensions, but integrating the pa- rameters out of the corresponding bayesian joint distribution gives a marginal likelihood over the common measurement space, π(y | mn) = (cid:90) (cid:90) = = eπ(θ|mn)[π(y | θ,mn)]. dθ π(y, θ | mn) dθ π(y | θ,mn) π(θ | mn) θ θ the marginal likelihood is also often known as the bayes factor or evidence in some ﬁelds. given the marginal likelihoods we can construct a joint distribution over the measure- ment and model spaces, π(y,mn) = π(y | mn) π(mn), 28 betancourt from which bayes’ theorem gives the model posteriors, π(mn | ˜y) = π(˜y,mn) π(˜y) = (cid:80)n π(˜y | mn) π(mn) n) π(m(cid:48) n(cid:48)=1 π(˜y | m(cid:48) n) . in particular, given only two models, m1 and m2, we are immediately guided to select the ﬁrst when it exhibits a higher model posterior density, π(m1 | ˜y) > π(m2 | ˜y) (cid:80)n π(˜y | m1) π(m1) n) π(m(cid:48) n(cid:48)=1 π(˜y | m(cid:48) n) (cid:80)n π(˜y | m2) π(m2) n) π(m(cid:48) n(cid:48)=1 π(˜y | m(cid:48) n) π(˜y | m1) π(m1) > π(˜y | m2) π(m2) > π(˜y | m1) π(˜y | m2) > π(m2) π(m1) . in words, we select m1 when the odds ratio, π(˜y | m1) / π(˜y | m2), surpasses a threshold deﬁned by the the relative prior probabilities of the two models, π(m2)/π(m1). interest- ingly this procedure resembles the likelihood ratio test where we use marginal likelihoods instead of maximum likelihoods and the testing threshold is constructed from our prior distributions. calibration of this bayesian model selection then proceeds as with the calibration of any bayesian inference or decision making processs. 1. we ﬁrst sample a true model from the model prior, m˜n ∼ π(mn). 2. then we sample a true model conﬁguration from the subsequent prior distribution, ˜θ ∼ π(θ | m˜n). 3. next we sample an observation from that model conﬁguration, ˜y ∼ π(y | ˜θ,m˜n), 4. finally we calculate the marginal likelihoods to inform model selection, π(˜y | mn), and estimate the corresponding discovery rates for each model, rmn|mn(cid:48) ≈ (cid:93)[ select mn given observation from mn(cid:48)] (cid:93)[ observations from mn(cid:48)] . model-based calibration 29 in particular, if we deﬁne ma as the absence model and mp as the presence model then the false discovery rate is estimated as fdr ≈ (cid:93)[ select mp given observation from ma] (cid:93)[ observations from ma] . with the true discovery rate estimated as tdr ≈ (cid:93)[ select mp given observation from mp ] (cid:93)[ observations from mp ] . in cases like this where there are only a few models being considered it may also be easier to condition on each model and compute the corresponding discovery rates one at a time instead of sampling a model at each iteration. marginal likelihoods and bayesian model selection arise immediately once we consider probabilities over the set of models. unfortunately the theoretical elegance of this approach does not always translate into practical utility. first and foremost the marginal likelihood is extremely challenging to estimate, even in relatively simple problems. the structure of the integral frustrates typical computational tools like markov chain monte carlo and necessitates more complex, and less well estab- lished, tools like nested sampling and simulated tempering. unfortunately these methods are poorly understood relative to the more established tools and consequently their imple- mentations are still limited by our modest understanding. in particular, quantiﬁcation of the accuracy of these methods is typically limited to only heuristics. beyond the computational issues, however, is a more subtle conceptual issue. the marginal likelihood evaluates a model by comparing a given observation to all of the model conﬁg- urations in the model conﬁguration space, each weighted by only the prior distribution. consequently even the smallest details of the prior distribution can signiﬁcantly aﬀect the marginal likelihood. this is in stark contrast to the eﬀect of the prior distribution on the posterior distri- bution. here the likelihood reduces the inﬂuence of model conﬁgurations inconsistent with an observation, obscuring much of the structure of the prior distribution. even seemingly irrelevant details of the prior distribution will still strongly aﬀect the marginal likelihoods, and the practice of constructing prior distributions to ensure only well-behaved posteriors is grossly insuﬃcient for ensuring meaningful marginal likelihoods (gelman, simpson and betancourt, 2017). in practice the sensitivity of the marginal likelihoods, and hence bayesian model selec- tion, to the intricate details of the prior distribution manifests in strong dependencies on the observation and a fragility in the corresponding model selection. small changes in the observation can cause signiﬁcant changes in the marginal likelihoods, with the decision making process rapidly vacillating amongst the possible models. fortunately this behavior will manifest in sensitivity analyses and poor false discovery rates and true discovery rates and so it can be quantiﬁed provided that the test is calibrated! 30 betancourt 4.4 posterior probability of the region of practical equivalence one of the implicit diﬃculties in informing discovery claims as presented so far is that the absence model is singular with respect to the full model conﬁguration space – the absence model conﬁguration space and the presence model conﬁguration spaces are of diﬀerent dimensionality. because of this the posterior probability for all of the model conﬁgurations in the absence will always be zero for a prior that is continuous across the full model conﬁguration space. the only way to admit non-zero posterior probabilities over both models is to assign inﬁnitely more prior probability to those model conﬁgurations in the absence model relative to those in the presence model. bayesian model comparison avoids this issue by comparing only marginal likelihoods and avoiding the individual model posteriors altogether. we can inform a discovery claim using only the posterior over the full model conﬁguration space, however, if we absorb some of the presence model into the absence model. in particular, those model conﬁgurations in the presence model close to those in the absence model will generate nearly identical observations and hence indistinguishable inferences; an inﬁnitesimally weak phenomenon will be impossible to diﬀerentiate from no phenomenon without an impractical amount of data. this suggests that we redeﬁne our absence model as θ1 = {θ ∈ θ | |ϑ(θ)| ≤ ϑ0} , with the presence model becoming θ2 = {θ ∈ θ | |ϑ(θ)| > ϑ0} , for some threshold ϑ0. the neighborhood around the absence model conﬁgurations, |ϑ(θ)| ≤ ϑ0, is known as the region of practical equivalence (kruschke, 2014). notice that separating model conﬁgurations close to ϑ = 0 from the presence model is not entirely dissimilar in what we had to do when considering the power of a complex alternative model in null hypothesis signiﬁcance testing. with this modiﬁcation of the absence model we can then claim a discovery when the posterior probability in the region of practical equivalence is below a given threshold, (cid:90) ϑ0 −ϑ0 dθ π(ϑ | ˜y) < 1 − α, where π(ϑ | ˜y) is the marginal posterior over the phenomenological parameters. i have deﬁned the formal decision making process here to superﬁcially resemble that used in null hypothesis signiﬁcance testing, but we could just as easily use the complementary situation where the posterior probability outside the region of practical equivalence is above the given threshold, (cid:90) −ϑ0 −∞ (cid:90) ∞ ϑ0 dθ π(ϑ | ˜y) + dθ π(ϑ | ˜y) > 1 − α. model-based calibration 31 calibration of this method, in particular the estimation of the false discovery rate and true discovery rate, immediately follows from the bayesian calibration paradigm. 1. we ﬁrst sample a true model conﬁguration from the prior distribution over the model conﬁguration space, ˜θ ∼ π(θ). 2. next we sample an observation from that model conﬁguration, ˜y ∼ π(y | ˜θ), 3. finally we reconstruct the posterior probability of the absence model, (cid:90) ϑ0 −ϑ0 dϑ π(ϑ | ˜y). p = the false discovery rate follows as fdr ≈ (cid:93)[p ≤ 1 − α and |(˜θ)| < ϑ0] (cid:93)[|(˜θ)| < ϑ0] with the true discovery rate, tdr ≈ (cid:93)[p ≤ 1 − α and |(˜θ)| > ϑ0 (cid:93)[|(˜θ)| > ϑ0] , . by sampling from various conditional priors we can also quantify how the false and true discovery rates, or even the distribution of p itself, varies with respect to various parameters. this allows us to visualize the sensitivity of the experiment similar to figure 8, only for arbitrarily complicated models (figure 10). 4.5 predictive scores lastly we can select a model and claim discovery or no discovery by comparing the predictive performance of the possible hypotheses. here we use our inferences to construct a predictive distribution for new data and then select the model whose predictive distribution is closest to the true data generating process. predictive distributions arise naturally in many forms of inference. for example, the model conﬁguration identiﬁed by a frequentist point estimator deﬁnes the predictive dis- tribution bayesian inference immediately yields two predictive distributions: the prior predictive distribution, πp = π(y | ˆθ(˜y)). (cid:90) θ πp = π(y) = dθ π(y | θ) π(θ), 32 betancourt ϑ fig 10. by sampling from a conditional joint distribution, π(σ | ϑ), we can visualize the sensitivity of the distribution of absence model posterior probabilities with respect to a phenomenological parameter, ϑ. the average of this distribution, shown here in dark red, gives the conditional false discovery rate. given a desired false discovery rate, here 10−3, we can then identify for which phenomenological parameters we expect a discovery on average, shown here in grey. and the posterior predictive distribuiton, πp = π(y | ˜y) = (cid:90) θ dθ π(y | θ) π(θ | ˜y). regardless of how a predictive distribution is constructed, it’s similarity to the true data generating process, π∗, is deﬁned by the kullback-leibler divergence, (cid:90) (cid:90) y kl(πp || π∗) = = dy π∗(y) log π∗(y) πp (y) dy π∗(y) log π∗(y) − (cid:90) dy π∗(y) log πp (y). y y because the ﬁrst term is the same for all models, the relative predictive performance be- tween models is quantiﬁed by the predictive score, (cid:90) s = − dy π∗(y) log πp (y). y this expectation with respect to the true data generating process cannot be calcu- lated without already knowing the true data generating process, but predictive scores can be approximated using observations which, by construction, are drawn from that distri- bution. diﬀerent approximation methods combined with various predictive distributions yield a host of predictive model comparison techniques, ranging from cross validation to the akaike information criterion, to the bayesian information criterion, to bayesian cross model-based calibration 33 validation, the widely applicable information criterion, and the deviance information criterion (betancourt, 2015). error in these approximations, however, can be quite large and diﬃcult to quantify in practice, leading to poorly calibrated selection between the absence model and the presence model. consequently it is critically important to estimate their expected false discovery rate and true discovery rate using the assumed model. in the frequentist settings these rates can be quantiﬁed as the minimal performance across the model conﬁgurations in the absence and presence models, where as in the bayesian setting these rates can be quantiﬁed by their expected performance over the bayesian joint distribution. 5. applications to limit setting limit setting is a complement to claiming discovery when the experiment is not expected to be suﬃciently sensitive to the relevant phenomenon. instead of claiming a discovery we consider how strongly we can constrain the magnitude of that phenomenon and calibrate the corresponding constraint with respect to the absence model. because limit setting is derived from standard inference methods, its implementation is signiﬁcantly more straight- forward than the implementation of discovery claims. 5.1 frequentist limit setting with anchored conﬁdence intervals in the frequentist setting we can constrain the magnitude of a phenomenon by construct- ing conﬁdence intervals than span from a vanishing phenomenon to some upper limit. more formally, if the magnitude of the phenomenon is positive, so that the absence model is de- ﬁned by ϑ = 0 and the absence model is deﬁned by ϑ > 0, then we construct an anchored conﬁdence interval of the form(cid:2)0, ˆϑ(y)(cid:1) that has a given coverage, α, with respect to the full model. given an observation, ˜y, we then claim that ϑ < ˆϑ(˜y) with conﬁdence α. the sensitivity of this claim is deﬁned with respect to the possible distribution of ˆϑ(y) with respect to the data generating process in the absence model. for one and two-dimensional absence models the sensitivity can be visualized using the same techniques in section 4.2.2. 5.2 bayesian limit setting with posterior quantiles the frequentist approach to limit setting has an immediate bayesian analogue where we use posterior quantiles to bound the magnitude of the phenomenon. for a given credibility, α, we deﬁne the upper limit, ϑα as ϑα(˜y) =(cid:8)ϑ | (cid:90) ϑ dϑ π(ϑ | ˜y) = 1 − α(cid:9). 0 by deﬁning the limit in terms of the marginal posterior for the phenomenological parame- ters, ϑ, we automatically incorporate the uncertainty in any nuisance parameters into the bound. 34 betancourt the corresponding sensitivity follows by considering the distribution ϑα with respect to the bayesian joint distribution for the absence model. 1. we ﬁrst sample a true model conﬁguration from the absence model by sampling the nuisance parameters from the conditional prior distribution, ˜σ ∼ π(σ | ϑ = 0) ˜θ = (0, ˜σ) . 2. next we sample an observation from that model conﬁguration, ˜y ∼ π(y | ˜θ), 3. finally we compute the inferred upper bound, ϑα(˜y) =(cid:8)ϑ | (cid:90) ϑ dϑ π(ϑ | ˜y) = 1 − α(cid:9). 6. conclusions and future directions 0 both the frequentist and bayesian perspectives admit procedures for analyzing sensitivi- ties and calibrating decision making processes. implementing these calibrations in practice, however, is far from trivial. frequentist calibration requires bounding the expectation of a given loss function over all of the data generating processes in a given model, or partitions thereof. the deriva- tion of analytic bounds from assumptions about the structure of the model conﬁguration space and the loss function, especially those derived from asymptotic analyses, is greatly facilitated with the presumption of simple model conﬁguration spaces. numerical methods for computing the bounds are also aided by simple models. the probabilistic computa- tions required of bayesian calibration are often more straightforward to approximate but suﬃciently complex models will eventually frustrate even the most advanced bayesian computational methods. in practice these computational challenges result in a dangerous tension between models that are simple enough to admit accurate calibrations and models that are complex enough for their resulting calibrations to be relevant to the experiment being analyzed. the continued improvement in computational resources and algorithms has gradually reduced, and promises to continue to reduce, this tension. monte carlo and markov chain monte carlo methods, for example, have revolutionized our ability to compute expected losses and bayesian posterior expectations over high-dimensional measurement and model conﬁguration spaces. unfortunately the applicability of this method critically depends on the desired calibration. in particular, the square root convergence of monte carlo estima- tors is often too slow to ensure accurate calibration of rare observations. this frustrates model-based calibration 35 the calculation, for example, of the o(1 − 10−7) signiﬁcance thresholds presumed in con- temporary particle physics. computational limitations restrain not only the complexity of our models but also the complexity of the loss function we consider. statistics is a constant battle between computational feasibility and compatibility with analysis goals. ultimately it is up to the practitioner to exploit their domain expertise to identify compromises that facilitate high-performance decision making. finally there is the issue of experimental design where we tune the design of an exper- iment to achieve a given performance. as diﬃcult as it is to compute this performance, “inverting” the calibration to identify the optimal experiment is even harder. for com- plex models that don’t admit analytic results, contemporary best practice often reduces to exploring the experiment design space heuristically, guided by computed calibrations and domain expertise. an interesting future direction is the use of automatic diﬀerentiation methods to au- tomatically estimate not only the expected losses but also their gradients with respect to the experimental design. although an imposing implementation challenge, these gradients have the potential to drastically improve the exploration and optimization of experimental designs. 7. acknowledgements i thank lindley winslow, charles margossian, joe formaggio, and dan simpsons for helpful comments and discussions. references bernardo, j.-m. and smith, a. f. m. (2009). bayesian theory. wiley series in probability and mathe- matical statistics: probability and mathematical statistics. john wiley &amp; sons, ltd., chichester. betancourt, m. (2015). a uniﬁed treatment of predictive model comparison. "
1803.08394," iris recognition is used in many applications around the world, with enrollment sizes as large as over one billion persons in india’s aadhaar program. large enrollment sizes can require special optimizations in order to achieve fast database searches. one such optimiza- tion that has been used in some operational scenarios is 1:first search. in this approach, instead of scanning the entire database, the search is terminated when the ﬁrst suﬃciently good match is found. this saves time, but ignores potentially better matches that may exist in the unexamined portion of the enrollments. at least one prominent and successful border-crossing program used this approach for nearly a decade, in order to allow users a fast “token-free” search. our work investigates the search accuracy of 1:first and compares it to the traditional 1:n search. several diﬀerent scenarios are considered trying to emulate real environments as best as possible: a range of enrollment sizes, closed- and open-set conﬁgu- rations, two iris matchers, and diﬀerent permutations of the galleries. results conﬁrm the expected accuracy degradation using 1:first search, and also allow us to identify acceptable working parameters where signiﬁcant search time reduction is achieved, while maintaining accuracy similar to 1:n search. keywords: biometrics, iris recognition, error rates, identiﬁcation, accuracy, search, 1:first, 1:n, open-set "
1803.08395,". research on science ﬁction (sci-ﬁ) in scientiﬁc publications has indicated the usage of sci-ﬁ stories, movies or shows to inspire novel human-computer interaction (hci) research. yet no studies have anal- ysed sci-ﬁ in a top-ranked computer science conference at present. for that reason, we examine the chi main track for the presence and nature of sci-ﬁ referrals in relationship to hci research. we search for six sci-ﬁ terms in a dataset of 5812 chi main proceedings and code the context of 175 sci-ﬁ referrals in 83 papers indexed in the chi main track. in our results, we categorize these papers into ﬁve contemporary hci research themes wherein sci-ﬁ and hci interconnect: 1) theoretical design re- search; 2) new interactions; 3) human-body modiﬁcation or extension; 4) human-robot interaction and artiﬁcial intelligence; and 5) visions of computing and hci. in conclusion, we discuss results and implications located in the promising arena of sci-ﬁ and hci research. keywords: design ﬁction, future visions, hci inspiraton, popular cul- ture in science, science ﬁction 1 "
1803.08396," we propose a new end-to-end single image dehazing method, called densely connected pyramid dehazing net- work (dcpdn), which can jointly learn the transmission map, atmospheric light and dehazing all together. the end- to-end learning is achieved by directly embedding the atmo- spheric scattering model into the network, thereby ensur- ing that the proposed method strictly follows the physics- driven scattering model for dehazing. inspired by the dense network that can maximize the information ﬂow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder struc- ture with multi-level pyramid pooling module for estimat- ing the transmission map. this network is optimized us- ing a newly introduced edge-preserving loss function. to further incorporate the mutual structural information be- tween the estimated transmission map and the dehazed re- sult, we propose a joint-discriminator based on genera- tive adversarial network framework to decide whether the corresponding dehazed image and the estimated transmis- sion map are real or fake. an ablation study is con- ducted to demonstrate the effectiveness of each module evaluated at both estimated transmission map and dehazed result. extensive experiments demonstrate that the pro- posed method achieves signiﬁcant improvements over the state-of-the-art methods. code will be made available at: https://github.com/hezhangsprinter "
1803.08397," let ω ⊂ rn be a bounded domain and δ(x) be the distance of a point x ∈ ω to the boundary. we study the positive solutions δ(x)2 u = up in ω, where p > 0, p (cid:54)= 1 and of the problem ∆u + µ µ ∈ r, µ (cid:54)= 0 is smaller then the hardy constant. the interplay between the singular potential and the nonlinearity leads to interesting structures of the solution sets. in this paper we ﬁrst give the complete picture of the radial solutions in balls. in particular we establish for p−1 at p > 1 the existence of a unique large solution behaving like δ the boundary. in general domains we extend results of [4] and show that there exists a unique singular solutions u such that u/δβ− → c on the boundary for an arbitrary positive function c ∈ c2+γ(∂ω) (γ ∈ (0, 1)), c ≥ 0. here β− is the smaller root of β(β − 1) + µ = 0. − 2 ams subject classiﬁcation: 35j75, 35b09, 35b51, 34b16. key words: elliptic problems, hardy potential, power nonlinearities, dead core and blowup solutions, singular boundary data. 1 "
1803.08399," surface growth, by association or dissociation of material on the boundaries of a body, is ubiquitous in both natural and engineering systems. it is the fundamental mechanism by which biological materials grow, starting from the level of a single cell, and is increasingly applied in engineering processes for fabrication and self-assembly. a signiﬁcant complexity in describing the kinetics of such processes arises due to their inherent coupled interaction with the diﬀusing constituents that are needed to sustain the growth, and the inﬂuence of local stresses on the growth rates. moreover, changes in concentration of solvent within the bulk of the body, generated by diﬀusion, can aﬀect volumetric changes, thus leading to an additional interacting growth mechanism. in this paper we present a general theoretical framework that captures these complexities to describe the kinetics of surface growth while accounting for coupled diﬀusion. then, by combination of analytical and numerical tools, applied to a simple growth geometry, we show that the evolution of such growth processes rapidly tends towards a universal path that is independent of initial conditions. this path, on which surface growth mechanisms and diﬀusion act harmoniously, can be extended to analytically portray the evolution of a body from inception up to a treadmilling state, in which addition and removal of material are balanced. keywords: surface growth, driving force, universal path "
1803.08400," in order to study the performance of interatomic potentials and their reliability at higher pressures, the phase diagram of four different embedded-atom type potential models of iron is compared. the calculations were done by the nested sampling technique in the pressure range 0.1 gpa–100 gpa. the low pressure stable structure is found to be the body- centred cubic in all cases, but the higher pressure phases show a great variation, being face-centred cubic, hexagonal close-packed and – at very low temperatures – different body-centred tetragonal phases are observed as well. the melting line is overestimated considerably for three of the models, but for the one where liquid properties had been taken into account during the potential ﬁtting process, the agreement with experimental results is good, even at very high pressures. keywords: nested sampling, phase diagrams, phase transition prediction, eam models for iron "
1803.08401," the cauchy problem for the complete euler system is in general ill posed in the class of admissible (entropy producing) weak solutions. this suggests there might be sequences of approximate solutions that develop ﬁne scale oscillations. accordingly, the concept of measure–valued solution that capture possible oscillations is more suitable for analysis. we study the convergence of a class of entropy stable ﬁnite volume schemes for the barotropic and complete compressible euler equations in the multidimensional case. we establish suitable stability and consistency estimates and show that the young measure generated by numerical solutions represents a dissipative measure–valued solution of the euler system. here dissi- pative means that a suitable form of the second law of thermodynamics is incorporated in the deﬁnition of the measure–valued solutions. in particular, using the recently established weak-strong uniqueness principle, we show that the numerical solutions converge pointwise to the regular solution of the limit systems at least on the lifespan of the latter. ∗the research of e.f. and h.m. leading to these results has received funding from the european research council under the european union’s seventh framework programme (fp7/2007-2013)/ erc grant agreement 320078. the institute of mathematics of the academy of sciences of the czech republic is supported by rvo:67985840. †the research of m.l. was supported by the german science foundation under the collaborative research centers trr 146 and trr 165. 1 keywords: compressible euler equations, entropy stable ﬁnite volume scheme, entropy sta- bility, convergence, dissipative measure–valued solution ams subject classiﬁcations: 65m08, 76n10, 35l65, 35r06 contents "
1803.08404,. the pursley-sarwate criterion of a pair of ﬁnite complex- valued sequences measures the collective smallness of the aperiodic auto- correlations and the aperiodic crosscorrelations of the two sequences. it is known that this quantity is always at least 1 with equality if and only if the sequence pair is a golay pair. we exhibit pairs of complex-valued sequences whose entries have unit magnitude for which the pursley- sarwate criterion tends to 1 as the sequence length tends to inﬁnity. our constructions use diﬀerent carefully chosen chu sequences. 
1803.08405," detection of power-law behavior and studies of scaling exponents uncover the characteristics of complexity in many real world phenomena. the complexity of ﬁnancial markets has always presented challenging issues and provided interesting ﬁndings, such as the inverse cubic law in the tails of stock price ﬂuctuation distributions. motivated by the rise of novel digital assets based on blockchain technology, we study the distributions of cryptocurrency price ﬂuctuations. we consider bitcoin returns over various time intervals and from multiple digital exchanges, in order to investigate the existence of universal scaling behavior in the tails, and ascertain whether the scaling exponent supports the presence of a ﬁnite second moment. we provide empirical evidence on slowly decaying tails in the distributions of returns over multiple time intervals and diﬀerent exchanges, corresponding to a power-law. we estimate the scaling exponent and ﬁnd an asymptotic power-law behavior with 2 < α < 2.5 suggesting that bitcoin returns, in addition to being more volatile, also exhibit heavier tails than stocks, which are known to be around 3. our results also imply the existence of a ﬁnite second moment, thus providing a fundamental basis for the usage of standard ﬁnancial theories and covariance-based techniques in risk management and portfolio optimization scenarios. keywords: ﬁnancial markets, cryptocurrencies, bitcoin, power-law, scaling "
1803.08407,". we introduce a novel rgb-d patch descriptor designed for detect- ing coplanar surfaces in slam reconstruction. the core of our method is a deep convolutional neural net that takes in rgb, depth, and normal information of a planar patch in an image and outputs a descriptor that can be used to ﬁnd coplanar patches from other images. we train the network on 10 million triplets of coplanar and non-coplanar patches, and evaluate on a new coplanarity benchmark created from commodity rgb-d scans. experiments show that our learned descriptor outperforms alternatives extended for this new task by a signiﬁcant margin. in addition, we demonstrate the beneﬁts of coplanarity matching in a robust rgbd reconstruction formulation. we ﬁnd that coplanarity constraints detected with our method are sufﬁcient to get reconstruction results comparable to state-of-the-art frameworks on most scenes, but outperform other methods on standard bench- marks when combined with a simple keypoint method. keywords: rgb-d registration, co-planarity, loop closure "
1803.08408," 8 1 0 2   r a   m 2 2     ] . o c h t a m [     1 v 8 0 4 8 0 . 3 0 8 1 : v i x r a let g be a graph and t a certain connected subgraph of g. the t -structure con- nectivity κ(g; t ) (or resp., t -substructure connectivity κs(g; t )) of g is the minimum m}) number of a set of subgraphs f = {t1, t2, . . . , tm} (or resp., f = {t (cid:48) such that ti is isomorphic to t (or resp., t i is a connected subgraph of t ) for every 1 ≤ i ≤ m, and f’s removal will disconnect g. the twisted hypercube hn is a new variant of hypercubes with asymptotically optimal diameter introduced by x.d. zhu. in this paper, we will determine both κ(hn; t ) and κs(hn; t ) for t ∈ {k1,r, pk}, respectively, where 3 ≤ r ≤ 4 and 1 ≤ k ≤ n. (cid:48) 2, . . . , t (cid:48) 1, t (cid:48) keywords: twisted hypercube; t -structure connectivity; t -substructure connec- tivity 1 "
1803.08410," in laparoscopic surgery, image quality can be severely de- graded by surgical smoke, which not only introduces error for the image processing (used in image guided surgery), but also reduces the visibility of the surgeons. in this paper, we propose to enhance the laparoscopic images by decomposing them into unwanted smoke part and enhanced part using a variational approach. the proposed method relies on the ob- servation that smoke has low contrast and low inter-channel differences. a cost function is deﬁned based on this prior knowledge and is solved using an augmented lagrangian method. the obtained unwanted smoke component is then subtracted from the original degraded image, resulting in the enhanced image. the obtained quantitative scores in terms of fade, jnbm and re metrics show that our proposed method performs rather well. furthermore, the qualitative visual inspection of the results show that it removes smoke effectively from the laparoscopic images. index terms— laparoscopic images, smoke removal, dehazing, variational, quality. "
1803.08412," inspired by group-based sparse coding, recently proposed group sparsity residual (gsr) scheme has demonstrated superior perfor- mance in image processing. however, one challenge in gsr is to estimate the residual by using a proper reference of the group-based sparse coding (gsc), which is desired to be as close to the truth as possible. previous researches utilized the estimations from other algorithms (i.e., gmm or bm3d), which are either not accurate or too slow. in this paper, we propose to use the non-local sam- ples (nls) as reference in the gsr regime for image denoising, thus termed gsr-nls. more speciﬁcally, we ﬁrst obtain a good estimation of the group sparse coefﬁcients by the image nonlocal self-similarity, and then solve the gsr model by an effective iter- ative shrinkage algorithm. experimental results demonstrate that the proposed gsr-nls not only outperforms many state-of-the-art methods, but also delivers the competitive advantage of speed. index terms— image denoising, group-based sparse coding, group sparsity residual, nonlocal self-similarity, iterative shrinkage algorithm. "
1803.08414," in this paper, we adapt the faster-rcnn framework for the detection of underground buried objects (i.e. hyperbola re- ﬂections) in b-scan ground penetrating radar (gpr) images. due to the lack of real data for training, we propose to in- corporate more simulated radargrams generated from differ- ent conﬁgurations using the gprmax toolbox. our designed cnn is ﬁrst pre-trained on the grayscale cifar-10 database. then, the faster-rcnn framework based on the pre-trained cnn is trained and ﬁne-tuned on both real and simulated gpr data. preliminary detection results show that the pro- posed technique can provide signiﬁcant improvements com- pared to classical computer vision methods and hence be- comes quite promising to deal with this kind of speciﬁc gpr data even with few training samples. index terms— ground penetrating radar (gpr), object detection, deep learning, faster-rcnn "
1803.08415," vehicle-to-infrastructure (v2i) communications are increasingly supporting high- way operations such as electronic toll collection, carpooling, and vehicle platooning. in this paper we study the incentives of strategic misbehavior by individual vehicles who can exploit the security vulnerabilities in v2i communications and impact the highway operations. we consider a v2i-enabled highway segment facing two classes of vehicles (agent populations), each with an authorized access to one server (subset of lanes). vehicles are strategic in that they can misreport their class (type) to the system operator and get unauthorized access to the server dedicated to the other class. this misbehavior causes a congestion externality on the compliant vehicles, and thus, needs to be deterred. we focus on an environment where the operator is able to inspect the vehicles for misbehavior based on their reported types. the inspection is costly and successful detection incurs a ﬁne on the misbehaving vehi- cle. we formulate a signaling game to study the strategic interaction between the vehicle classes and the operator. our equilibrium analysis provides conditions on the cost parameters that govern the vehicles’ incentive to misbehave, and determine the operator’s optimal inspection strategy. index terms: cyber-physical systems security, asymmetric information games, smart highway systems, crime deterrence. 1 "
1803.08416," parametric approaches to learning, such as deep learning (dl), are highly popular in nonlinear regression, in spite of their extremely difﬁcult training with their increasing complexity (e.g. number of layers in dl). in this paper, we present an alternative semi-parametric framework which foregoes the ordinarily required feedback, by introducing the novel idea of geometric regularization. we show that certain deep learning techniques such as residual network (resnet) architecture are closely related to our approach. hence, our technique can be used to analyze these types of deep learn- ing. moreover, we present preliminary results which conﬁrm that our approach can be easily trained to obtain complex structures. index terms— supervised learning, back propagation, geometric approaches "
1803.08417," this thesis addresses questions in representation and invariant theory of ﬁnite groups. the ﬁrst concerns singularities of quotient spaces under actions of ﬁnite groups. we introduce a class of ﬁnite groups such that the quotients have at worst abelian quotient singularities. we prove that supersolvable groups belong to this class and show that nonabelian ﬁnite simple groups do not belong to it. the sec- ond question concerns the cohen-macaulayness of the invariant ring z[x1, . . . , xn]g, where g is a permutation group. we prove that this ring is cohen-macaulay if g is generated by transpositions, double transpositions, and 3-cycles, and conjecture that the converse is true as well. vi contents dedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . list of figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . list of tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . list of symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . "
1803.08419," conversational agents have become ubiquitous, ranging from goal-oriented systems for helping with reserva- tions to chit-chat models found in modern virtual assistants. in this survey paper, we explore this fascinating ﬁeld. we look at some of the pioneering work that deﬁned the ﬁeld and gradually move to the current state-of-the-art models. we look at statistical, neural, generative adversarial network based and reinforcement learning based approaches and how they evolved. along the way we discuss various challenges that the ﬁeld faces, lack of context in utterances, not having a good quantitative metric to compare models, lack of trust in agents because they do not have a consistent persona etc. we structure this paper in a way that answers these pertinent questions and discusses competing approaches to solve them. "
1803.08421," using brst-bv approach, massless and massive continuous-spin ﬁelds propagating in the ﬂat space are studied. for such ﬁelds, brst-bv gauge invariant lagrangian is obtained. the lagrangian and gauge transformations are constructed out of traceless gauge ﬁelds and traceless gauge transformation parameters. interrelation between the brst-bv lagrangian and the la- grangian for the continuous-spin ﬁelds in metric-like approach is demonstrated. considering the brst-bv lagrangian in the siegel gauge, we get gauge-ﬁxed lagrangian which is invariant under global brst and antibrst transformations. keywords: continuous-spin ﬁeld; brst-bv approach; higher-spin ﬁeld. 8 1 0 2   n u j   4     ] h t - p e h [     2 v 1 2 4 8 0 . 3 0 8 1 : v i x r a ∗ e-mail: metsaev@lpi.ru 1 1 "
1803.08422," one important problem in the theory of hardy space is to ﬁnd the best rational approxi- mation of a given order to a function in the hardy space h 2 on the unit disk. it is equivalent to ﬁnding the best blaschke form with free poles. the cyclic adaptive fourier decomposition method is based on the grid search technique. its approximative precision is limited by the grid spacing. this paper propose an enhancement of the cyclic adaptive fourier decomposition. the new method ﬁrst changes the rectangular grids to the polar ones for cyclic adaptive fourier decomposition, so that the decomposition enables us to employ fast fourier transformation to accelerate the search speed. furthermore, the proposed algorithm utilizes the gradient descent optimization to tune the best pole-tuple on the mesh grids, reaching higher precision with less computation cost. its validity and eﬀectiveness are conﬁrmed by several examples. key words. best rational approximation, blaschke products, hardy space, gradient descent method, fast fourier transform 2010 ams subject classiﬁcations. 41a20, 30h10, 30j10, 65t50 "
1803.08428, we show that there are no smooth warped ads3 solutions in 10- and 11-dimensional supergravities which preserve strictly more than 16 supersymmetries and have in- ternal space a compact without boundary manifold. 1 1 
1803.08429," the nele group, formerly known as the iannini family, is one of the youngest asteroid families in the main belt. previously, it has been noted that the pericenter longi- tudes ̟ and nodal longitudes ω of its largest member asteroids are clustered at the present time, therefore suggesting that the collisional breakup of parent body must have happened recently. here we verify this conclusion by detailed orbit-propagation of a synthetic nele family and show that the current level of clustering of secular an- gles of the largest nele family members requires an approximate age limit of 4.5 myr. additionally, we make use of an updated and largely extended nele membership to obtain, for the ﬁrst time, an age estimate of this family using the backward integration method (bim). convergence of the secular angles in a purely gravitational model and in a model including the non-gravitational forces caused by the yarkovsky eﬀect are both compatible with an age younger than 7 myr. more accurate determination of the nele family age would require additional data about the spin state of its members. key words: minor planets, asteroids: general – minor planets, asteroids: individual: nele–celestial mechanics. 1 "
1803.08430,". in this article, we focus on the left translation actions on non- commutative compact connected lie groups whose topological dimensions are 3 or 4, consisting of su(2), u(2), so(3), so(3)×s 1 and spinc(3). (the relevant conclusions of the right translation actions on these lie groups are the same as left translation actions.) we deﬁne the rotation vectors of the left actions in- duced by the elements in the maximal tori on these ﬁve non-commutative com- pact connected lie groups, and utilize the rotation vectors deﬁned to give the topologically conjugate classiﬁcations of these left actions. as a by-product, we show that for any homeomorphism f : l(p, 1) × s 1 → l(p, 1) × s 1, the induced isomorphism (π ◦ f ◦ i)∗ maps each element in the fundamental group of l(p, 1) to itself or its inverse, where i : l(p, 1) → l(p, 1) × s 1 is the natural inclusion and π : l(p, 1) × s 1 → l(p, 1) is the projection. furthermore, for the left actions on these ﬁve lie groups, we study the relationships between their topologically conjugate classiﬁcations and their algebraically conjugate classiﬁcations as well as their smooth conjugate classiﬁcations. "
1803.08432,". we introduce several parallel algorithms operating on a distributed forest of adaptive quadtrees/octrees. they are targeted at large-scale applications relying on data layouts that are more complex than required for standard ﬁnite elements. such applications appear in various contexts, examples being the hp-adaptive discontinuous galerkin method, element-based particle tracking, and in-situ post-processing and visualization. speciﬁcally, we design algorithms to derive an adapted worker forest based on sparse data, to identify owner processes in a top-down search of remote objects, and to allow for variable process counts and per-element data sizes in partitioning and parallel ﬁle i/o. we demonstrate the algorithms’ usability and performance in the context of a particle tracking example that we scale to 21e9 particles and 64ki mpi processes on the juqueen supercomputer. key words. parallel algorithms, adaptive mesh reﬁnement, forest of octrees, particle tracking ams subject classiﬁcations. 65d18, 65m50, 65y05, 68w10 "
1803.08433," currently, accurate localization system based on com- modity wifi devices is not broadly available yet. in the literature, the solutions are based on either network in- frastructure like wifi router, which have at least three antennas, or sacriﬁce accuracy with coarse-grained in- formation like rssi. in this work, we design a new lo- calizing system - dyloc which is accurate based on aoa estimation and instantly deployable on users’ devices. dyloc is designed to be dynamically constructed with user’s devices as network nodes without any network infrastructure. on the platform of laptops, our system achieve comparable localization accuracy with state-of- the-art work despite of the limitation of less number and large separation of antennas. we design multi-stage signal processing to resolve the ambiguity issue arisen in this scenario. to enable dynamic and collaborative construction, our system can accurately conduct self- localization and also eliminate the need of infrastruc- ture anchors, which is due to the dedicated two-layer algorithm design. 1. "
1803.08434," deep insights into the possible infrared dynamics of strongly-coupled nonabelian gauge theories such as qcd come from the analyses of n = 1 or n = 2 super- symmetric gauge theories. central in the whole discussion will be the topological soliton monopoles and vortices and their quantum dynamics. we review the argu- ments that nonabelian monopoles, free from the classic ”diﬃculties”, can be deﬁned semi-classically via the topology and stability connection to the better understood nonabelian vortices. recent results on cp n−1 models on 2d worldsheet of ﬁnite width, establish the quantum mechanical nature of such nonabelian monopoles. an interesting class of rg ﬂows and emergence of conﬁning vacua ”nearby” strongly- coupled infrared-ﬁxed point (irfp) conformal theories are discussed in the context of most singular vacua in n = 2 supersymmetric qcd. certain analogy with the real-world qcd is drawn. in many systems, color-ﬂavor locking emerges as a crucial mechanism for the gauge system to avoid dynamical abelianization. ∗cp 3-origins distinguished lecture, university of southern denmark, odense, 12 march 2018 1 contents "
1803.08435,". deep generative models have shown success in automatically synthesizing missing image regions using surrounding context. however, users cannot directly decide what content to synthesize with such ap- proaches. we propose an end-to-end network for image inpainting that uses a diﬀerent image to guide the synthesis of new content to ﬁll the hole. a key challenge addressed by our approach is synthesizing new con- tent in regions where the guidance image and the context of the original image are inconsistent. we conduct four studies that demonstrate our results yield more realistic image inpainting results over seven baselines. "
1803.08436," in this paper we prove the liouville type theorem for stable at inﬁnity solutions of the following equation for 1 < m − 1 < θ < θs,m := n(m−1)+3m n−3m . here θs,m is a the classic critical exponent for m− bi-harmonic equation. keywords: liouville type theorem, stable solutions, m- tri-harmonic equations, critical exponents. ∆3 mu = |u|θ−1u in rn ,     ] p a h t a m . [     1 v 6 3 4 8 0 . 3 0 8 1 : v i x r a "
1803.08437,". we compute the ´etale cohomology ring h ∗(spec ok , z/pz) for p a prime number, where ok is the ring of integers of a number ﬁeld k. as an application, we give a non-vanishing formula for an invariant deﬁned by minhyong kim. contents "
1803.08438,". let md,n(q) denote the number of monic irreducible polynomials in fq[x1, x2, . . . , xn] of degree d. we show that for a ﬁxed degree d, the se- quence md,n(q) converges q-adically to an explicitly determined rational func- tion md,∞(q). furthermore we show that the limit md,∞(q) is related to the classic necklace polynomial md,1(q) by an involutive functional equation, lead- ing to a phenomenon we call liminal reciprocity. the limiting ﬁrst moments of factorization statistics for squarefree polynomials are expressed in terms of a family of symmetric group representations as a consequence of liminal reci- procity. "
1803.08441,". it has been shown that self-assembled chains of active colloidal particles can present sustained oscillations. these oscillations are possible because of the eﬀective diﬀusiophoretic forces that mediate the interactions of colloids do not respect the action–reaction principle and hence, a hopf bifurcation is possible even for overdamped dynamics. anchoring the particles in one extreme breaks the head-tail symmetry and the oscillation is transformed into a traveling wave pattern and thus the chain behaves like a beating cilium. the net force on the anchor, estimated using the resistive force theory, vanishes before the bifurcation and thereafter grows linearly with the bifurcation parameter. if the mobilities of the particles on one extreme are reduced to mimic an elongated cargo, the traveling wave generates a net velocity on the chain that now behaves like a moving ﬂagellum. the average velocity again grows linearly with the bifurcation parameter. our results demonstrate that simpliﬁed systems, consisting only of a few particles with non-reciprocal interaction and head- tail asymmetry show beating motion and self-propulsion. both properties are present in many non-equilibrium models thus making our results a general feature of active matter. submitted to: new j. phys. 8 1 0 2   r a   m 2 2     ] t f o s . t a m - d n o c [     1 v 1 4 4 8 0 . 3 0 8 1 : v i x r a active colloidal chains with cilia- and ﬂagella-like motion 2 "
1803.08444," statistical techniques play a large role in the structural health monitoring of instrumented infras- tructure, such as a railway bridge constructed with an integrated network of ﬁbre optic sensors. one possible way to reason about the structural health of such a railway bridge, is to model the time it takes to recover to a no-load (baseline) state after a train passes over. inherently, this recovery time is random and should be modelled statistically. this paper uses a non-parametric model, based on empirical quantile approximations, to construct a space-memory eﬃcient baseline distribution for the streaming data from these sensors. a fast statistical test is implemented to detect deviations away from, and recovery back to, this distribution when trains pass over the bridge, yielding a recovery time. our method assumes that there are no temporal variations in the data. a median-based detrending scheme is used to remove the temporal variations likely due to temperature changes. this allows for the continuous recording of sensor data with a space-memory constraint. 1 "
1803.08446," context. observations of molecular gas have played a key role in developing the current understanding of the late stages of stellar evolution. aims. the survey planetary nebulae and their co reservoir with apex (pandora) was designed to study the circumstellar shells of evolved stars with the aim to estimate their physical parameters. methods. millimetre carbon monoxide (co) emission is the most useful probe of the warm molecular component ejected by low- to intermediate-mass stars. co is the second-most abundant molecule in the universe, and the millimetre transitions are easily excited, thus making it particularly useful to study the mass, structure, and kinematics of the molecular gas. we present a large survey of the co (j = 3 − 2) line using the atacama pathﬁnder experiment (apex) telescope in a sample of 93 proto-planetary nebulae and planetary nebulae. results. co (j = 3 − 2) was detected in 21 of the 93 objects. only two objects (irc+10216 and pn m 2-9) had previous co (j = 3 − 2) detections, therefore we present the ﬁrst detection of co (j = 3 − 2) in the following 19 objects: frosty leo, hd 101584, iras 19475+3119, pn m1-11, v* v852 cen, ic 4406, hen 2-113, hen 2-133, pn fg 3, pn cn 3-1, pn m 2-43, pn m1-63, pn m1-65, bd +30 3639, hen 2-447, hen 2-459, pn m3-35, ngc 3132, and ngc 6326. conclusions. co (j = 3− 2) was detected in all 4 observed ppne (100%), 15 of the 75 pne (20%), one of the 4 wide binaries (25%), and in 1 of the 10 close binaries (10%). using the co (j = 3 − 2) line, we estimated the column density and mass of each source. the h2 column density ranges from 1.7x1018 to 4.2x1021cm−2 and the molecular mass ranges from 2.7x10−4 to 1.7x10−1m(cid:12). key words. line: identiﬁcation – molecular data – catalogs – (ism:) planetary nebulae: general "
1803.08447," we use gaussian processes in combination with mcmc method to place constraints on cosmological parameters of three dark energy models including ﬂat and curved frw and bianchi type i spacetimes. recently compiled 36 measurements of the hubble parameter h(z) in the redshifts intermediate 0.07 6 z 6 2.36 have been used in our nonparametric reconstruction of the cosmological parameters. moreover, we use these models to estimate the redshift of the deceleration-acceleration transition. we consider two gaussian priors for current value of the hubble constant i.e h0 = 73 ± 1.74(68 ± 2.8) km/s/mpc to to investigate the eﬀect of the assumed h0 on our parameters estimations. for statistical analysis we use nuts sampler which is an extension of hamiltonian monte carlo algorithm to generate mcmc chains for parameters of dark energy models. in general, when we compared our results with 9 years wmap as well as planck 2016 collaboration, we found that bianchi type i model is slightly ﬁts better to the observational hubble data with respect to the ﬂat frw model. key words: bianchi type i – dark energy – hubble rate "
1803.08448," edge computing caters to a wide range of use cases from la- tency sensitive to bandwidth constrained applications. how- ever, the exact specifications of the edge that give the most benefit for each type of application are still unclear. we inves- tigate the concrete conditions when the edge is feasible, i.e., when users observe performance gains from the edge while costs remain low for the providers, for an application that re- quires both low latency and high bandwidth: video analytics. keywords edge computing, distributed systems "
1803.08449,"— the indirect approach to continuous-time system identiﬁcation consists in estimating continuous-time models by ﬁrst determining an appropriate discrete-time model. for a zero-order hold sampling mechanism, this approach usually leads to a transfer function estimate with relative degree 1, independent of the relative degree of the strictly proper real sys- tem. in this paper, a reﬁnement of these methods is developed. inspired by indirect pem, we propose a method that enforces a ﬁxed relative degree in the continuous-time transfer function estimate, and show that the resulting estimator is consistent and asymptotically efﬁcient. extensive numerical simulations are put forward to show the performance of this estimator when contrasted with other indirect and direct methods for continuous-time system identiﬁcation. index terms— system identiﬁcation; continuous-time sys- tems; parameter estimation; sampled data. "
1803.08450,"—deep learning revolutionized data science, and recently, its popularity has grown exponentially, as did the amount of papers employing deep networks. vision tasks such as human pose estimation did not escape this methodological change. the large number of deep architectures lead to a plethora of methods that are evaluated under different experimental protocols. moreover, small changes in the architecture of the network, or in the data pre-processing procedure, together with the stochastic nature of the optimization methods, lead to notably different results, making extremely difﬁcult to sift methods that signiﬁcantly outperform others. therefore, when proposing regression algorithms, practitioners proceed by trial-and-error. this situation motivated the current study, in which we perform a systematic evaluation and a statistical analysis of the performance of vanilla deep regression – short for convolutional neural networks with a linear regression top layer –. up to our knowledge this is the ﬁrst comprehensive analysis of deep regression techniques. we perform experiments on three vision problems and report conﬁdence intervals for the median performance as well as the statistical signiﬁcance of the results, if any. surprisingly, the variability due to different data pre-processing procedures generally eclipses the variability due to modiﬁcations in the network architecture. index terms—deep learning, regression, computer vision, convolutional neural networks, statistical signiﬁcance, empirical and systematic evaluation, head-pose estimation, full-body pose estimation, facial landmark detection. ! "
1803.08452," we demonstrate a method of associating the principal symbol at a k-point with a linear diﬀerential operator acting between modules over a commutative algebra, and we use it to deﬁne the ellipticity of a linear diﬀerential operator in a purely algebraic way. we prove that the ellipticity is preserved by a surjective homomorphism of algebras. as an example, we show that for every real aﬃne variety there is an elliptic linear diﬀerential operator acting on the algebra of regular functions on this variety. keywords: elliptic operators, k-points, principal symbol, universal derivations mathematics subject classiﬁcation (2010): 13n05, 13n15, 58j05 0 "
1803.08455," it has been recently argued that an embedding of the sm into a consistent theory of quantum gravity may imply important constraints on the mass of the lightest neutrino and the cosmological constant λ4. the constraints come from imposing the absence of any non-susy ads stable vacua obtained from any consistent compactiﬁcation of the sm to 3 or 2 dimensions. this condition comes as a corollary of a recent extension of the weak gravity conjecture (wgc) by ooguri and vafa. in this paper we study t 2/zn compactiﬁcations of the sm to two dimensions in which sm wilson lines are projected out, leading to a considerable simpliﬁcation. we analyze in detail a t 2/z4 compactiﬁcation of the sm in which both complex structure and wilson line scalars are ﬁxed and the potential is only a function of the area of the torus a2. we ﬁnd that the sm is not robust against the appearance of ads vacua in 2d and hence would be by itself inconsistent with quantum gravity. on the contrary, if the sm is embedded at some scale mss into a susy version like the mssm, the ads vacua present in the non-susy case disappear or become unstable. this means that wgc arguments favor a susy version of the sm, independently of the usual in a t 2/z4 compactiﬁcation in which the orbifold hierarchy problem arguments. action is embedded into the b − l symmetry the bounds on neutrino masses and the cosmological constant are recovered. this suggests that the mssm should be extended with a u (1)b−l gauge group. in other families of vacua the spectrum of susy particles is further constrained in order to avoid the appearance of new ads vacua or instabilities. we discuss a possible understanding of the little hierarchy problem in this context. contents "
1803.08456," deep reinforcement learning has been successfully applied to several visual-input tasks using model-free methods. in this paper, we propose a model-based approach that combines learning a dnn-based transition model with monte carlo tree search to solve a block-placing task in minecraft. our learned transition model predicts the next frame and the rewards one step ahead given the last four frames of the agent’s ﬁrst-person-view image and the current action. then a monte carlo tree search algorithm uses this model to plan the best sequence of actions for the agent to perform. on the proposed task in minecraft, our model-based approach reaches the performance comparable to the deep q-network’s, but learns faster and, thus, is more training sample efﬁcient. keywords: reinforcement learning, model-based reinforcement learning, deep learning, model learning, monte carlo tree search acknowledgements i would like to express my sincere gratitude to my supervisor dr. stefan uhlich for his continuous support, patience, and immense knowledge that helped me a lot during this study. my thanks and appreciation also go to my colleague anna konobelkina for insightful comments on the paper as well as to sony europe limited for providing the resources for this project. 8 1 0 2   r a   m 2 2     ] i a . s c [     1 v 6 5 4 8 0 . 3 0 8 1 : v i x r a s0s0 (cid:98)s1(cid:98)s1 t = 3 (cid:98)s2(cid:98)s2 (cid:98)s3(cid:98)s3 (cid:98)s1(cid:98)s1 (cid:98)s2(cid:98)s2 t = 4 (cid:98)s3(cid:98)s3 (cid:98)s4(cid:98)s4 a3 = “turn right” a4 = “place block” dnn model (cid:98)r5 (cid:98)s4(cid:98)s4 dnn model (cid:98)r6 (cid:98)s5(cid:98)s5 spectator view at time step t = 3 spectator view at time step t = 5 figure 1: transition model for the block-placing task in minecraft "
1803.08457,". recently, there has been increasing interest to leverage the compe- tence of neural networks to analyze data. in particular, new clustering methods that employ deep embeddings have been presented. in this paper, we depart from centroid-based models and suggest a new framework, called clustering-driven deep embedding with pairwise constraints (cpac), for non-parametric cluster- ing using a neural network. we present a clustering-driven embedding based on a siamese network that encourages pairs of data points to output similar represen- tations in the latent space. our pair-based model allows augmenting the informa- tion with labeled pairs to constitute a semi-supervised framework. our approach is based on analyzing the losses associated with each pair to reﬁne the set of con- straints. we show that clustering performance increases when using this scheme, even with a limited amount of user queries. we present state-of-the-art results on different types of datasets and compare our performance to parametric and non-parametric techniques. "
1803.08460," unseen action recognition (uar) aims to recognise novel action categories without training examples. while previous methods focus on inner-dataset seen/unseen splits, this paper proposes a pipeline using a large-scale train- ing source to achieve a universal representation (ur) that can generalise to a more realistic cross-dataset uar (cd- uar) scenario. we ﬁrst address uar as a generalised multiple-instance learning (gmil) problem and discover ‘building-blocks’ from the large-scale activitynet dataset using distribution kernels. essential visual and semantic components are preserved in a shared space to achieve the ur that can efﬁciently generalise to new datasets. pre- dicted ur exemplars can be improved by a simple se- mantic adaptation, and then an unseen action can be di- rectly recognised using ur during the test. without fur- ther training, extensive experiments manifest signiﬁcant im- provements over the ucf101 and hmdb51 benchmarks. "
1803.08461,". weak lensing peak counts are a powerful statistical tool for constraining cos- mological parameters. so far, this method has been applied only to surveys with relatively small areas, up to several hundred square degrees. as future surveys will provide weak lensing datasets with size of thousands of square degrees, the demand on the theoretical prediction of the peak statistics will become heightened. in particular, large simulations of increased cosmological volume are required. in this work, we investigate the possibility of using simu- lations generated with the fast comoving-lagrangian acceleration (cola) method, coupled to the convergence map generator ufalcon, for predicting the peak counts. we examine the systematics introduced by the cola method by comparing it with a full treepm code. we ﬁnd that for a 2000 deg2 survey, the systematic error is much smaller than the statistical error. this suggests that the cola method is able to generate promising theoretical predictions for weak lensing peaks. we also examine the constraining power of various conﬁgurations of data vectors, exploring the inﬂuence of splitting the sample into tomographic bins and combining diﬀerent smoothing scales. we ﬁnd the combination of smoothing scales to have the most constraining power, improving the constraints on the s8 amplitude parameter by at least 40% compared to a single smoothing scale, with tomography brining only limited increase in measurement precision. 8 1 0 2   r a   m 2 2     ] . o c h p - o r t s a [     1 v 1 6 4 8 0 . 3 0 8 1 : v i x r a 1corresponding author. contents "
1803.08462," √ an r-cut of a k-uniform hypergraph h is a partition of the vertex set of h into r parts and the size of the cut is the number of edges which have a vertex in each part. a classical m), and this result of edwards says that every m-edge graph has a 2-cut of size m/2 + ω( is best possible. that is, there exist cuts which exceed the expected size of a random cut by some multiple of the standard deviation. we study analogues of this and related results in hypergraphs. first, we observe that similarly to graphs, every m-edge k-uniform hypergraph m) larger than the expected size of a random r-cut. moreover, has an r-cut whose size is ω( in the case where k = 3 and r = 2 this bound is best possible and is attained by steiner triple systems. surprisingly, for all other cases (that is, if k ≥ 4 or r ≥ 3), we show that every m- edge k-uniform hypergraph has an r-cut whose size is ω(m5/9) larger than the expected size of a random r-cut. this is a signiﬁcant diﬀerence in behaviour, since the amount by which the size of the largest cut exceeds the expected size of a random cut is now considerably larger than the standard deviation. √ 1 "
1803.08463," in this report, we describe our par- ticipant named-entity recognition sys- tem at vlsp 2018 evaluation campaign. we formalized the task as a sequence labeling problem using bio encoding scheme. we applied a feature-based model which combines word, word-shape fea- tures, brown-cluster-based features, and word-embedding-based features. we com- pare several methods to deal with nested entities in the dataset. we showed that combining tags of entities at all levels for training a sequence labeling model (joint-tag model) improved the accuracy of nested named-entity recognition. "
1803.08466," 8 1 0 2   r a   m 2 2     ] . a f h t a m [     1 v 6 6 4 8 0 . 3 0 8 1 : v i x r a the purpose of this paper is to study frames for a hilbert space h, having the form {t nϕ}∞ n=0 for some ϕ ∈ h and an operator t : h → h. we characterize the frames that have such a representation for a bounded operator t, and discuss the properties of this operator. in particular, we prove that the image chain of t has ﬁnite length n in n=0 has the very particular n=n +ℓ is a frame for h for all ℓ ∈ n0. n=0 are sensitive to the ordering of the elements and to norm-perturbations of the generator ϕ and the operator t. on the other hand positive stability results are obtained by considering perturbations of the generator ϕ belonging to an invariant subspace on which t is a contraction. the overcomplete case; furthermore {t nϕ}∞ property that {t nϕ}n −1 we also prove that frames of the form {t nϕ}∞ n=0 ∪{t nϕ}∞ 1 "
1803.08467,". we introduce branchgan, a novel training method that enables unconditioned generative adversarial networks (gans) to learn image manifolds at multiple scales. what is unique about branchgan is that it is trained in multiple branches, progressively covering both the breadth and depth of the network, as resolutions of the training images increase to reveal ﬁner-scale features. speciﬁcally, each noise vector, as input to the generator network, is explicitly split into several sub-vectors, each corresponding to and trained to learn image representations at a particular scale. during training, we progressively “de-freeze” the sub- vectors, one at a time, as a new set of higher-resolution images is em- ployed for training and more network layers are added. a consequence of such an explicit sub-vector designation is that we can directly manipu- late and even combine latent (sub-vector) codes that are associated with speciﬁc feature scales. experiments demonstrate the eﬀectiveness of our training method in multi-scale, disentangled learning of image manifolds and synthesis, without any extra labels and without compromising qual- ity of the synthesized high-resolution images. we further demonstrate two new applications enabled by branchgan. keywords: generative adversarial network; image synthesis; image representation learning; multi-scale; disentanglement 1 "
1803.08471," as more aspects of social interaction are digi- tally recorded, there is a growing need to develop privacy-preserving data analysis methods. so- cial scientists will be more likely to adopt these methods if doing so entails minimal change to their current methodology. toward that end, we present a general and modular method for privatiz- ing bayesian inference for poisson factorization, a broad class of models that contains some of the most widely used models in the social sciences. our method satisﬁes local differential privacy, which ensures that no single centralized server need ever store the non-privatized data. to for- mulate our local-privacy guarantees, we introduce and focus on limited-precision local privacy—the local privacy analog of limited-precision differen- tial privacy (flood et al., 2013). we present two case studies, one involving social networks and one involving text corpora, that test our method’s ability to form the posterior distribution over la- tent variables under different levels of noise, and demonstrate our method’s utility over a na¨ıve ap- proach, wherein inference proceeds as usual, treat- ing the privatized data as if it were not privatized. "
1803.08472,". in earlier work in collaboration with pavel galashin and thomas mc- conville we introduced a version of chip-ﬁring for root systems. our investigation of root system chip-ﬁring led us to deﬁne certain polynomials analogous to ehrhart poly- nomials of lattice polytopes, which we termed the symmetric and truncated ehrhart- like polynomials. we conjectured that these polynomials have nonnegative integer coeﬃcients. here we aﬃrm “half” of this positivity conjecture by providing a positive, combinatorial formula for the coeﬃcients of the symmetric ehrhart-like polynomials. this formula depends on a subtle integrality property of slices of permutohedra which may be of independent interest. we also discuss how our formula very naturally sug- gests a conjecture for the coeﬃcients of the truncated ehrhart-like polynomials that turns out to be false in general, but which may hold in some cases. "
1803.08061," we have explored the evolution of gas distributions from cosmological simulations carried out using the ramses adaptive mesh reﬁnement (amr) code, to explore the eﬀects of resolution on cosmological hydrodynamical simulations. it is vital to understand the eﬀect of both the resolution of initial conditions and the ﬁnal resolution of the simulation. lower initial resolution simulations tend to produce smaller numbers of low mass structures. this will strongly aﬀect the assembly history of objects, and has the same eﬀect of simulating diﬀerent cosmologies. the resolution of initial conditions is an important factor in simulations, even with a ﬁxed maximum spatial resolution. the power spectrum of gas in simulations using amr diverges strongly from the ﬁxed grid approach - with more power on small scales in the amr simulations - even at ﬁxed physical resolution and also produces oﬀsets in the star formation at speciﬁc epochs. this is because before certain times the upper grid levels are held back to maintain approximately ﬁxed physical resolution, and to mimic the natural evolution of dark matter only simulations. although the impact of hold back falls with increasing spatial and initial-condition resolutions, the oﬀsets in the star formation remain down to a spatial resolution of 1 kpc. these oﬀsets are of order of 10-20%, which is below the uncertainty in the implemented physics but are expected to aﬀect the detailed properties of galaxies. we have implemented a new grid-hold-back approach to minimize the impact of hold back on the star formation rate. key words: galaxies: evolution — methods: numerical "
1803.08062," cannibals are dark matter particles with a scattering process that allows three particles to annihilate to two. this exothermic process keeps the gas of the remaining particles warm long after they become non-relativistic. a cannibalizing dark sector which is decoupled from the standard model naturally arises from a pure-glue conﬁning hidden sector. it has an eﬀective ﬁeld theory description with a single massive interacting real scalar ﬁeld, the lightest glueball. since warm dark matter strongly suppresses growth of structure cannibals cannot be all of the dark matter. thus we propose a scenario where most dark matter is non-interacting and cold but about 1 percent is cannibalistic. we review the cannibals’ unusual scaling of the temperature and energy and number densities with redshift and generalize the equations for the growth of matter density perturbations to the case of cannibals. we solve the equations numerically to predict the scaling of the hubble parameter and the characteristic shape of the linear matter power spectrum as a function of model parameters. our results may have implications for the σ8 and h0 problems. 8 1 0 2   r a   m 1 2     ] h p - p e h [     1 v 2 6 0 8 0 . 3 0 8 1 : v i x r a ∗electronic address: buenabad@bu.edu †electronic address: iasraziehm@ust.hk ‡electronic address: schmaltz@bu.edu 1 i. "
1803.08063," the search for habitable exoplanets inspires the question - how do habitable planets form? planet habitability models traditionally focus on abiotic processes and neglect a biotic response to changing conditions on an inhabited planet. the gaia hypothesis postulates that life inﬂuences the earth’s feedback mechanisms to form a self-regulating system, and hence that life can maintain habitable conditions on its host planet. if life has a strong inﬂuence, it will have a role in determining a planet’s habitability over time. we present the exogaia model - a model of simple ‘planets’ host to evolving microbial biospheres. microbes interact with their host planet via consumption and excretion of atmospheric chemicals. model planets orbit a ‘star’ which provides incoming radiation, and atmospheric chemicals have either an albedo, or a heat-trapping property. planetary temperatures can therefore be altered by microbes via their metabolisms. we seed multiple model planets with life while their atmospheres are still forming and ﬁnd that the microbial biospheres are, under suitable conditions, generally able to prevent the host planets from reaching inhospitable temperatures, as would happen on a lifeless planet. we ﬁnd that the underlying geochemistry plays a strong role in determining long-term habitability prospects of a planet. we ﬁnd ﬁve distinct classes of model planets, including clear examples of ‘gaian bottlenecks’ - a phenomenon whereby life either rapidly goes extinct leaving an inhospitable planet, or survives indeﬁnitely maintaining planetary habitability. these results suggest that life might play a crucial role in determining the long-term habitability of planets. ∗ arwen.e.nicholson@gmail.com "
1803.08064,". we study production of self-interacting dark matter (dm) during an early matter- dominated phase. as a benchmark scenario, we consider a model where the dm consists of singlet scalar particles coupled to the visible standard model (sm) sector via the higgs portal. we consider scenarios where the initial dm abundance is set by either the usual thermal freeze-out or an alternative freeze-in mechanism, where dm was never in thermal equilibrium with the sm sector. in both cases, the number density of dm may change considerably compared to the standard radiation-dominated case, having signiﬁcant observational and experimental ramiﬁcations. keywords: dark matter, wimp, fimp, simp, freeze-out, freeze-in, dark matter self- interactions, matter-dominated universe 8 1 0 2   r a   m 1 2     ] h p - p e h [     1 v 4 6 0 8 0 . 3 0 8 1 : v i x r a contents "
1803.08065," in an unconventional presentation of the data on the transverse momentum spectra of baryons produced in heavy-ion collisions, regularities are found that make possible the discovery of a uni- versal formula valid for p, λ, ξ and ω. the formula describes the baryon distributions over wide ranges of pt (0.5 < ∼ pt ∼ 5 gev/c) for 0.06 < < ∼ √sn n < ∼ 3 tev, except for very peripheral collisions. some aspects of their empirical properties are derived in the recombination model, resulting in a revelation of some features of the light and strange quark distributions before hadronization. in- terpretation of the inverse slopes of their exponential behavior leads to an implication that cannot accommodate the conventional description of ﬂuid ﬂow. this is mainly a study of phenomenology without detailed model input. 8 1 0 2   r a   m 1 2     ] x e - l c u n [     1 v 5 6 0 8 0 . 3 0 8 1 : v i x r a 1 i. "
1803.08066," modern machine learning techniques, such as convolutional, recurrent and recursive neural networks, have shown promise for jet substructure at the large hadron collider. for example, they have demonstrated eﬀectiveness at boosted top or w boson identi- ﬁcation or for quark/gluon discrimination. we explore these methods for the purpose of classifying jets according to their electric charge. we ﬁnd that neural networks that incorporate distance within the jet as an input can provide signiﬁcant improvement in jet charge extraction over traditional methods. we ﬁnd that both convolutional and recurrent networks are eﬀective and both train faster than recursive networks. the advantages of using a ﬁxed-size input representation (as with the cnn) or a smaller input representation (as with the rnn) suggest that both convolutional and recurrent networks will be essential to the future of modern machine learning at colliders. 8 1 0 2   r a   m 1 2     ] h p - p e h [     1 v 6 6 0 8 0 . 3 0 8 1 : v i x r a 1 1 "
1803.08068,". we give an algorithm to compute the periods of smooth projective hypersurfaces of any dimension. this is an improvement over existing algorithms which could only compute the periods of plane curves. our algorithm reduces the evaluation of period integrals to an initial value problem for ordinary diﬀerential equations of picard–fuchs type. in this way, the periods can be computed to extreme-precision in order to study their arithmetic properties. the initial conditions are obtained by an exact determination of the cohomology pairing on fermat hypersurfaces with respect to a natural basis. "
1803.08069,"— this paper presents an automated method for creating spatial maps of soil condition with an outdoor mobile robot. effective soil mapping on farms can enhance yields, reduce inputs and help protect the environment. traditionally, data are collected manually at an arbitrary set of locations, then soil maps are constructed ofﬂine using kriging, a form of gaussian process regression. this process is laborious and costly, limiting the quality and resolution of the resulting information. instead, we propose to use an outdoor mobile robot for automatic collection of soil condition data, building soil maps online and also adapting the robot’s exploration strategy on-the-ﬂy based on the current quality of the map. we show how using kriging variance as a reward function for robotic exploration allows for both more efﬁcient data collection and better soil models. this work presents the theoretical foundations for our proposal and an experimental comparison of exploration strategies using soil compaction data from a ﬁeld generated with a mobile robot. "
1803.08070," we study the borel-reducibility of isomorphism relations in the generalized baire space κκ . in the main result we show for inaccessible κ, that if t is a classiﬁable theory and t′ is superstable with s-dop, then the isomorphism of models of t is borel reducible to the isomorphism of models of t′. in fact we show the consistency of the following: if t is a superstable theory with s-dop, then the isomorphism of models of t is σ1 1-complete. acknowledgement this work was made under the supervision of tapani hyttinen. i want to express my gratitude to him for introducing me to the topic, his valuable advices and support during this work. this research was supported by the doctoral programme in mathematics and statistics (domast). "
1803.08071,". many classical computer vision problems, such as essential matrix computation and pose estimation from 3d to 2d correspondences, can be solved by ﬁnding the eigenvector corresponding to the smallest, or zero, eigenvalue of a matrix representing a linear system. incorporat- ing this in deep learning frameworks would allow us to explicitly encode known notions of geometry, instead of having the network implicitly learn them from data. however, performing eigendecomposition within a net- work requires the ability to diﬀerentiate this operation. unfortunately, while theoretically doable, this introduces numerical instability in the optimization process in practice. in this paper, we introduce an eigendecomposition-free approach to train- ing a deep network whose loss depends on the eigenvector corresponding to a zero eigenvalue of a matrix predicted by the network. we demon- strate on several tasks, including keypoint matching and 3d pose esti- mation, that our approach is much more robust than explicit diﬀerentia- tion of the eigendecomposition, it has better convergence properties and yields state-of-the-art results on both tasks. keywords: end-to-end learning, eigendecomposition, singular value de- composition, geometric vision. "
1803.08072,". considering a compact riemann surface of genus greater than two, a higgs bundle is a pair composed of a holomorphic bundle over the riemann surface, joint with an auxiliar vector ﬁeld, so-called higgs ﬁeld. this theory started around thirty years ago, with hitchin’s work, when he reduced the self-duality equations from dimension four to dimension two, and so, studied those equations over riemann surfaces. hitchin baptized those ﬁelds as “higgs ﬁelds” beacuse in the context of physics and gauge theory, they describe similar particles to those described by the higgs bosson. later, simpson used the name “higgs bundle” for a holomorphic bundle together with a higgs ﬁeld. today, higgs bundles are the subject of research in several areas such as non-abelian hodge theory, langlands, mirror symmetry, integrable systems, quantum ﬁeld theory (qft), among others. the main purposes here are to introduce these objects, and to present a brief construction of the moduli space of higgs bundles. keywords: hodge bundles, moduli of higgs bundles, stratiﬁcations, variations of hodge structures, vector bundles. ams 2010 msc classes: primary 14h60; secondaries 14d07, 55q52. 1 "
1803.08073," automatic interpretation of the relation between the constituents of a noun com- pound, e.g. olive oil (source) and baby oil (purpose) is an important task for many nlp applications. recent approaches are typically based on either noun-compound representations or paraphrases. while the former has initially shown promising re- sults, recent work suggests that the suc- cess stems from memorizing single pro- totypical words for each relation. we ex- plore a neural paraphrasing approach that demonstrates superior performance when such memorization is not possible. "
1803.08075," dark matter, one of the important portion of the universe, could aﬀect the visible matter in neutron stars. an important physical feature of dark matter is due to the spin of dark matter particles. here, applying the piecewise polytropic equation of state for the neutron star matter and the equation of state of spin polarized self-interacting dark matter, we investigate the structure of neutron stars which are inﬂuenced by the spin polarized self-interacting dark matter. the behavior of the neutron star matter and dark matter portions for the stars with diﬀerent values of the interaction between dark matter particles and spin polarization of dark matter is considered. in addition, we present the value of the gravitational redshift of these stars in diﬀerent cases of spin polarized and self-interacting dark matter. pacs: 97.60.jd 95.35.+d key words: neutron stars; dark matter; spin polarization 1 "
1803.08076," as multi-agent networks grow in size and scale, they become increasingly difﬁcult to synchronize, though agents must work together even when generating and sharing different information at different times. targeting such cases, this paper presents an asynchronous optimization framework in which the time between successive communications and computations is unknown and unspeciﬁed for each agent. agents’ updates are carried out in blocks, with each agent updating only a small subset of all decision variables. each agent uses an independently chosen tikhonov regularization, and convergence is measured with respect to a weighted block-maximum norm in which agents’ blocks can be measured in different p-norms and weighted differently to normalize problems. asymptotic convergence is shown and convergence rates are derived explicitly in terms of a problem’s parameters, with only mild restrictions on what they can be. simulation results are included to verify the theoretical developments made. "
1803.08079,". eﬃcient representations of convex sets are of crucial importance for many algorithms that work with them. it is well-known that sometimes, a complicated convex set can be expressed as the projection of a much simpler set in higher dimensions called a lift of the original set. this is a brief survey of recent developments in the topic of lifts of convex sets. our focus will be on lifts that arise from aﬃne slices of real positive semideﬁnite cones known as psd or spectrahedral lifts. the main result is that projection representations of a convex set are controlled by factorizations, through closed convex cones, of an operator that comes from the convex set. this leads to several research direc- tions and results that lie at the intersection of convex geometry, combinatorics, real algebraic geometry, optimization, computer science and more. "
1803.08080,": we explore the phenomenology of a qcd-like dark sector which conﬁnes around the gev scale. the dark sector inherits a ﬂavour structure from a coupling between dark quarks and sm quarks via a heavy mediator, which leads to exciting new phenomena. while stable baryonic bound states are the dark matter candidates, the phenomenology is dominated by the lightest composite mesons, the dark pions, which can have decay lengths ranging from millimetres to hundreds of meters. for masses below 1.5 gev, their exclusive decays to sm mesons are calculated for the ﬁrst time by matching both dark and visible sectors to a chiral lagrangian. constraints from big bang nucleosynthesis, dark matter direct detection and ﬂavour single out a small region of allowed parameter space for dark pion masses below 5 gev. it is best probed by the ﬁxed target experiments na62 and ship, where dark pions can be produced copiously in rare decays like b → kπd. the dominant πd → k±π∓ and πd → 3π decay modes are a smoking gun for a cp-odd, ﬂavour violating new resonance. heavier dark pions are best searched for at the lhc, where they decay after hadronisation to produce jets which emerge into sm states within the detector. here the ﬂavour structure ensures diﬀerent ﬂavours emerge on diﬀerent length scales, leading to a striking new feature in the emerging jets signature. 8 1 0 2   r p a 6       ] h p - p e h [     2 v 0 8 0 8 0 . 3 0 8 1 : v i x r a contents "
1803.08081," based on a simple object, an i.i.d. sequence of positive integer- valued random variables, {an}n∈z, we introduce and study two ran- dom structures and their connections. first, a population dynamics, in which each individual is born at time n and dies at time n + an. this dynamics is that of a d/gi/∞ queue, with arrivals at integer times and service times given by {an}n∈z. second, the directed ran- dom graph t f on z generated by the random map f (n) = n + an. only assuming e[a0] < ∞ and p[a0 = 1] > 0, we show that, in steady state, the population dynamics is regenerative, with one individual alive at each regenerative epochs. we identify a unimodular structure in this dynamics. more precisely, t f is a unimodular directed tree, in which f (n) is the parent of n. this tree has a unique bi-inﬁnite path. moreover, t f splits the integers into two categories: ephemeral inte- gers, with a ﬁnite number of descendants of all degrees, and successful integers, with an inﬁnite number. each regenerative epoch is a suc- cessful individual such that all integers less than it are its descendants of some order. ephemeral, successful, and regenerative integers form stationary and mixing point processes on z. key words: population dynamics, queuing system, unimodular random graph, eternal family tree, branching process. msc 2010 subject classiﬁcation: primary: 92d25, 60k25, 05c80. ∗baccelli@math.utexas.edu †asodre@math.utexas.edu 1 figure 1: the population process. here, a−6 = 10, a−5 = 3, a−4 = 8, a−3 = 3, a−2 = 3, and a−1 = 6. assuming a−i < i for all i > 6, there are four edges crossing 0. individuals -6,-4,-2, and -1 are still alive at time 0. hence, n0 = 5, as it includes the individual born at 0. "
1803.08082,". we derive the 3d energy critical quintic nls from quantum many-body dy- namics with 3-body interaction in the t3 (periodic) setting. due to the known complexity of the energy critical setting, previous progress was limited in comparison to the 2-body interaction case yielding energy subcritical cubic nls. previously, the only result for the 3d energy critical case was [42], which proved the uniqueness part of the argument in the case of small solutions. in the main part of this paper, we develop methods to prove the convergence of the bbgky hierarchy to the inﬁnite gross-pitaevskii (gp) hierarchy, and separately, the uniqueness of large gp solutions. since the trace estimate used in the previous proofs of convergence is the false sharp trace estimate in our setting, we instead introduce a new frequency interaction analysis and apply the ﬁnite dimensional quantum de finetti theorem. for the large solution uniqueness argument, we discover the new hufl (hierarchical uni- form frequency localization) property for the gp hierarchy and use it to prove a new type of uniqueness theorem. the hufl property reduces to a new statement even for nls. with the help of [24, 43] which proved the global well-posedness for the quintic nls, this new uniqueness theorem establishes global uniqueness. 8 1 0 2   r a   m 1 2     ] p a h t a m . [     1 v 2 8 0 8 0 . 3 0 8 1 : v i x r a contents "
1803.08084," we examine some inﬂationary models based on modiﬁ- cations of gravity in the light of planck 2015 data, such as the generalised chaplygin inspired inﬂation, models based in n = 1 supergravity and braneworld scenar- ios. we also show that, conversely, potentials with a very ﬂat plateau yield a primordial spectrum similar to that of the starobinsky model with no need to modify general relativity. "
1803.08087,". let ℓ be a commutative ring with unit. to every pair of ℓ-algebras a and b one can associate a simplicial set hom(a, b∆) so that π0hom(a, b∆) equals the set of polynomial homotopy classes of morphisms from a to b. we prove that πnhom(a, b∆) is the set of homotopy classes of morphisms from a to bsn is the ind-algebra of polynomials on the n-dimensional cube with coeﬃcients in b vanishing at the boundary of the cube. this is a generalization to arbitrary dimensions of a theorem of corti˜nas- thom, which addresses the cases n ≤ 1. as an application we give a simpliﬁed proof of a theorem of garkusha that computes the homotopy groups of his matrix-unstable algebraic kk-theory space in terms of polynomial homotopy classes of morphisms. • , where bsn • "
1803.08088," we study holographic conformal anomalies and the corresponding a-theorem for einstein gravity extended with horndeski terms that involve up to and including linear curvature tensors. we focus on our discussion in d = 5 bulk dimensions. for the generic horndeski coupling, the a-charge is the same as that in einstein gravity, but the inclusion of the horn- deski term violates the a-theorem. however, there exists a critical point of the horndeski coupling, for which the theory admits nearly ads spacetimes with non-vanishing horndeski scalar. the full ads isometry is broken down by the logarithmic scalar hair to the poincar´e group plus the scale invariance. we ﬁnd that in this case the a-charge depends on the ads radius ℓ and the integration constant χs of the horndeski scalar. in addition, we ﬁnd that two new central charges emerge, that are absent in gravities with minimally-coupled matter. we call them b-charges. these b-charges also depend on ℓ and χs. we construct an a-function for ﬁxed ℓ but with the running horndeski scalar χ replacing the constant χs, and establish the holographic a-theorem using the null energy condition in the bulk. furthermore, we ﬁnd that there exist analogous monotonous b-functions as well. we also obtain the a-charge and the a-theorem in general odd bulk dimensions. 8 1 0 2   y a m 1 2       ] h t - p e h [     3 v 8 8 0 8 0 . 3 0 8 1 : v i x r a †liyuezhou@tju.edu.cn ∗mrhonglu@gmail.com contents "
1803.08089," in learning-to-learn the goal is to infer a learning algorithm that works well on a class of tasks sampled from an unknown meta distribution. in contrast to previous work on batch learning-to- learn, we consider a scenario where tasks are presented sequentially and the algorithm needs to adapt incrementally to improve its performance on future tasks. key to this setting is for the algorithm to rapidly incorporate new observations into the model as they arrive, without keeping them in memory. we focus on the case where the underlying algorithm is ridge regression parameterized by a positive semideﬁnite matrix. we propose to learn this matrix by applying a stochastic strategy to minimize the empirical error incurred by ridge regression on future tasks sampled from the meta distribution. we study the statistical properties of the proposed algorithm and prove non-asymptotic bounds on its excess transfer risk, that is, the generalization performance on new tasks from the same meta distribution. we compare our online learning-to-learn approach with a state of the art batch method, both theoretically and empirically. "
1803.08091,". the direct product n × n of two free monogenic semigroups contains uncountably many pairwise non-isomorphic subdirect products. furthermore, the fol- lowing hold for n × s, where s is a ﬁnite semigroup. it contains only countably many pairwise non-isomorphic subsemigroups if and only if s is a union of groups. and it contains only countably many pairwise non-isomorphic subdirect products if and only if every element of s has a relative left- or right identity element. "
1803.08092,"— hybrid dynamical systems have proven to be a powerful modeling ion, yet fundamental questions regarding their dynamical properties remain. in this paper, we develop a novel solution concept for a class of hybrid systems, which is a generalization of filippov’s solution concept. in the mathematical theory, these hybrid filippov solutions eliminate the notion of zeno executions. building on previous techniques for relaxing hybrid systems, we then introduce a family of smooth control systems that are used to approximate this solution concept. the trajectories of these relaxations vary differentiably with respect to initial conditions and inputs, may be numerically approximated using existing techniques, and are shown to converge to the hybrid filippov solution in the limit. finally, we outline how the results of this paper provide a foundation for future work to control hybrid systems using well-established techniques from control theory. "
1803.08093,". we develop a theory of grassmann triples via hasse-schmidt derivations, which formally generalizes results such as the cayley-hamilton theorem in linear algebra, thereby providing a uniﬁed approach to classical linear algebra and tropical algebra. y a m 4 1     ]   . a r h t a m [     2 v 3 9 0 8 0 . 3 0 8 1 : v i x r a "
1803.08094,". an action should remain identiﬁable when modifying its speed: consider the contrast between an expert chef and a novice chef each chopping an onion. here, we expect the novice chef to have a rela- tively measured and slow approach to chopping when compared to the expert. in general, the speed at which actions are performed, whether slower or faster than average, should not dictate how they are recog- nized. we explore the erratic behavior caused by this phenomena on state-of-the-art deep network-based methods for action recognition in terms of maximum performance and stability in recognition accuracy across a range of input video speeds. by observing the trends in these metrics and summarizing them based on expected temporal behaviour w.r.t. variations in input video speeds, we ﬁnd two distinct types of net- work architectures. in this paper, we propose a preprocessing method named t-recs, as a way to extend deep-network-based methods for ac- tion recognition to explicitly account for speed variability in the data. we do so by adaptively resampling the inputs to a given model. t-recs is agnostic to the speciﬁc deep-network model; we apply it to four state- of-the-art action recognition architectures, c3d, i3d, tsn, and con- vnet+lstm. on hmdb51 and ucf101, t-recs-based i3d models show a peak improvement of at least 2.9% in performance over the base- line while t-recs-based c3d models achieve a maximum improvement in stability by 59% over the baseline, on the hmdb51 dataset. keywords: preprocessing, temporal modeling, action recognition "
1803.08095,". let b be an inﬁnite subset of n. when we consider parti- tions of natural numbers into elements of b, a partition number without a restriction of the number of equal parts can be expressed by partition numbers with a restriction α of the number of equal parts. although there are many way of the expression, we prove that there exists a ex- pression form such that this expression form is true for all possible set b. this identities comes from the partition numbers of natural numbers into {1, α, α2, α3, · · · }. furthermore, we prove that there exist inverse forms of the expression forms. and we prove other similar identities. the proofs in this paper are constructive. let p(n) be the number of partitions of n into natural numbers, and d(n) be the number of partitions of n into distinct natural numbers. if one calculates p(5) and from d(1) to d(5) except d(4), p(5) = 7, d(1) = 1, d(2) = 1, d(3) = 2, d(5) = 3. however, p(5) can de expressed by the following product and sum of d(1), d(2), d(3), d(5). p(5) = 7 = 3 + 1 × 1 + 1 × 1 + 2 × 1 = d(5) + d(1)d(1) + d(2)d(1) + d(3)d(1). next, let pp(n) be the number of partitions of n into primes, and pp number of partitions of n into distinct primes. then 1 (n) be the pp(5) = 2, pp 1 (1) = 0, pp 1 (2) = 1, pp 1 (3) = 1, pp 1 (5) = 2 and pp(5) can de expressed by the following product and sum of pp pp 1 (3), pp 1 (1), pp 1 (2), 1 (5). pp(5) = 2 = 2 + 0 × 0 + 1 × 0 + 1 × 0 1 (1) + pp 1 (5) + pp 1 (1)pp = pp 1 (2)pp 1 (1) + pp 1 (3)pp 1 (1). finally, let p o(n) be the number of partitions of n into odd numbers, and po 1 (n) be the number of partitions of n into distinct odd numbers. then po(5) = 3, po 1 (1) = 1, po 1 (2) = 0, po 1 (3) = 1, po 1 (5) = 1 1the theorems in this paper stated and proved in 2012 winter. 2010 mathematics subject classiﬁcation. 11p81, 11p84, 05a17 key words and phrases. partition number identities, change of set of parts. 1 8 1 0 2   r a   m 9 1     ] . o c h t a m [     1 v 5 9 0 8 0 . 3 0 8 1 : v i x r a 2 and kim, bongju po(5) = 3 = 1 + 1 × 1 + 0 × 1 + 1 × 1 1 (1) + po 1 (5) + po 1 (1)po = po 1 (2)po 1 (1) + po 1 (3)po 1 (1). if one compares above three expressions, one can see that these three expres- sion forms are same though the set of parts was changed. the numbers in the previous identity are not random but come from the all possible binary expressions of 5 i.e. 5 = 5 × 20 = 1 × 20 + 1 × 22 = 1 × 20 + 2 × 21 = 3 × 20 + 1 × 21. deﬁnition. let n be the set of natural numbers and ψ be an one to one function such that ψ : n → n. we denote the set {ψ(n) ∈ n} by a. pa α (n) is the number of partitions of n into elements of a such that the number of equal parts is less than or equals to α ∈ n \ {0}. pa(n) is the number of partitions of n into elements of ψ(n) without a restriction of the number of equal parts. we deﬁne pa α (0) = pa(0) := 1 for all α ∈ n \ {0}. 1. identities between pa and pa α before considering the identities between pa and pa α , we prove the identities between pa and pa 1 . deﬁnition. consider all non-negative integer solutions of the indeterminate equation n = n0+2n1+4n2+· · · = pi≥0 2ini and denote by (an 13 · · · ), 21, an (an 23 · · · ), · · · . in other word, 22, an 12, an 11, an 11 + 2an 21 + 2an 12 + 4an 22 + 4an 13 + · · · 23 + · · · n = an = an · · · then the solution matrix of this equation is proposition 1.1. let an = (an let ψ be an one to one function such that ψ : n → n. then ij). an := (an ij) be the solution matrix of n = pi≥0 2ini and pa(n) = x i≥1 y j≥1 1 (an pa ij ) for all n ∈ n. partition number identities which are true for all set of parts 3 proof. it is well known fact that for |q| < 1, x n≥0 pa(n)qn = y n≥1 1 1 − qψ(n) and x n≥0 1 (n)qn = y pa n≥1 (1 + qψ(n)) (see [1] or [2]). on the other hand, y (1 − q2ψ(n)) = y (1 − qψ(n)) y (1 + qψ(n)) n≥1 y n≥1 (1 − q4ψ(n)) = y (1 − qψ(n)) y (1 + qψ(n)) y (1 + q2ψ(n)) n≥1 n≥1 n≥1 n≥1 n≥1 ... y n≥1 (1 − q2i+1ψ(n)) = y (1 − qψ(n)) n≥1 i y i=0 (1 + q2iψ(n)) y n≥1 for all i ∈ n. so, 1 = lim i→∞ y n≥1 (1 − q2i+1ψ(n)) (1 − qψ(n)) = y n≥1 ∞ y i=0 (1 + q2iψ(n)). y n≥1 therefore, and y n≥1 1 1 − qψ(n) = y y i≥0 n≥1 (1 + q2iψ(n)) x n≥0 pa(n)qn = y (x i≥0 n≥0 1 (n)q2in). pa if one expands the above inﬁnite product to an inﬁnite series, one can express pa(n) by pa 1 (1) and one can see that the coeﬃcient of qn is related with non-negative integer solutions of n = pi≥0 2ini. if one expands 1 (n − 1), · · · , pa 1 (n), pa 4 kim, bongju some terms, 1 (n)q2in) pa y (x n≥0 i≥0 = 1 + pa + [pa + [pa + [pa · · · 1 (1)]q2 1 (1)q + [pa 1 (1)pa 1 (2)pa 1 (1)pa 1 (3) + pa 1 (4) + pa 1 (5) + pa 1 (2) + pa 1 (1)]q3 1 (1) + pa 1 (2) + pa 1 (2) + pa 1 (1)pa 1 (1)]q4 1 (1) + pa 1 (3)pa 1 (1)]q5 now, we prove more general identities between pa and pa α . (cid:3) deﬁnition. let n ∈ n and {(an,α 23 · · · ), · · · } be the set of all non-negative integer solutions of the indeterminate equation n = n0 + (α + 1)n1 + (α + 1)2n2 + · · · = pi≥0(α + 1)ini. in other words, 13 · · · ), (an,α 11 , an,α 21 , an,α 22 , an,α 12 , an,α 11 + (α + 1)an,α 21 + (α + 1)an,α 12 + (α + 1)2an,α 22 + (α + 1)2an,α 13 + · · · 23 + · · · n = an,α = an,α · · · where an,α ij ∈ n. then the solution matrix of this equation is an,α := (an,α ij ). theorem 1.2. let ψ be an one to one function such that ψ : n → n. and let n ∈ n and an,α = (an,α ij ) be the solution matrix of n = pi≥0(α + 1)ini. then pa(n) = x i≥1 y j≥1 α (an,α pa ij ) for all α ∈ n \ {0}. proof. it is well known fact that for |q| < 1, x n≥0 α (n)qn = y pa n≥1 1 − q(α+1)ψ(n) 1 − qψ(n) (1 + qψ(n) + q2ψ(n) + · · · + qαψ(n)) = y n≥1 (see [2]). on the other hand, partition number identities which are true for all set of parts 5 y n≥1 y n≥1 (1 − q(α+1)ψ(n)) = y (1 − qψ(n)) y (1 + qψ(n) + q2ψ(n) + · · · + qαψ(n)) (1 − q(α+1)2ψ(n)) = y (1 − qψ(n)) y (1 + qψ(n) + q2ψ(n) + · · · + qαψ(n)) n≥1 n≥1 n≥1 n≥1 (1 + q(α+1)ψ(n) + q(α+1)2ψ(n) + · · · + q(α+1)αψ(n)) × y n≥1 ... y n≥1 (1 − q(α+1)i+1ψ(n)) = y (1 − qψ(n)) n≥1 × i y i=0 for all i ∈ n. so, (1 + q(α+1)iψ(n) + · · · + q(α+1)iαψ(n)) y n≥1 1 = lim i→∞ y n≥1 (1 − q(α+1)i+1ψ(n)) (1 − qψ(n)) = y n≥1 ∞ y i=0 (1 + q(α+1)iψ(n) + · · · + q(α+1)iαψ(n)). y n≥1 therefore, y n≥1 and 1 1 − qψ(n) = y y i≥0 n≥1 (1 + q(α+1)iψ(n) + · · · + q(α+1)iαψ(n)) x n≥0 pa(n)qn = y (x i≥0 n≥0 α (n)q(α+1)in). pa if one expands the above inﬁnite product to an inﬁnite series, one can express pa(n) by pa α (1) and one can see that the coeﬃcient of qn is related with non-negative integer solutions of n = pi≥0(α + 1)ini (cid:3) α (n − 1), · · · , pa α (n), pa 2. inverse identities and some other similar identities in section 1, we found the identities which express pa(n) by pa α (n), pa α (n−1), · · · , pa α (1). now, we will ﬁnd the inverse identities. deﬁnition. let ea(n) be the number of even partitions of n without a restric- tion of the number of equal parts and oa(n) be the number of odd partitions of n without a restriction of the number of equal parts. then we deﬁne and ¯pa(0) := 1. ¯pa(n) := ea(n) − oa(n) 6 kim, bongju deﬁnition. let n = (α+1)pi≥0 2ini be the indeterminate equation for n, α ∈ n\{0}. if this equation has solutions (bn,α where bn,α 22 , bn,α ij ∈ n, then we deﬁne the solution matrix of this equation by 13 , · · · ), (bn,α 12 , bn,α 11 , bn,α 21 , bn,α 23 , · · · ), · · · bn,α := (bn,α ij ). deﬁnition. let n, α ∈ n \ {0}. if n = (α + 1)pi≥0 2ini has non-negative integer solutions, we deﬁne α(n) := x γψ i≥1 y j≥1 ¯pa(bn,α ij ) and if n = (α + 1)pi≥0 2ini does not have a non-negative integer solution, we deﬁne γψ α(n) := 0. for n = 0, we deﬁne γψ α(0) := 1. theorem 2.1. let ψ be an one to one function such that ψ : n → n. then pa α (n) = n x i=0 pa(n − i)γψ α(i) for all n, α ∈ n. proof. for |q| < 1, (1 − q2(α+1)ψ(n)) = y (1 − q(α+1)ψ(n)) y (1 + q(α+1)ψ(n)) (1 − q4(α+1)ψ(n)) = y (1 − q(α+1)ψ(n)) y n≥1 y n≥1 × y (1 + q(α+1)ψ(n)) y (1 + q2(α+1)ψ(n)) n≥1 n≥1 (1 − q2i+1(α+1)ψ(n)) = y (1 − q(α+1)ψ(n)) y n≥1 i y i=0 (1 + q2i(α+1)ψ(n)) y n≥1 for all i ∈ n. so, 1 = lim i→∞ y n≥1 (1 − q2i+1(α+1)ψ(n)) (1 − q(α+1)ψ(n)) = y n≥1 ∞ y i=0 (1 + q2i(α+1)ψ(n)) y n≥1 and y n≥1 1 − q(α+1)ψ(n) 1 − qψ(n) = y n≥1 1 1 − qψ(n) y i≥0 1 1 + q2i(α+1)ψ(n) . y n≥1 n≥1 n≥1 n≥1 n≥1 ... partition number identities which are true for all set of parts 7 on the other hand, y n≥1 1 1 + q2i(α+1)ψ(n) ¯pa(n)q2i(α+1)n. = x n≥0 therefore, if we deﬁne y i≥0 y n≥1 then 1 1 + q2i(α+1)ψ(n) := x γψ α(n)qn, n≥0 α(n) = x γψ i≥1 y j≥1 ¯pa(bn,α ij ) when n = (α + 1)pi≥0 2ini has non-negative integer solutions and γψ when n = (α + 1)pi≥0 2ini does not have a non-negative integer solution. finally, α(n) = 0 x n≥0 α (n)qn = y pa n≥1 1 y n≥1 1 − qψ(n) y pa(n)qn)(x i≥0 n≥0 = (x n≥0 γψ α(n)qn) 1 1 + q2i(α+1)ψ(n) and pa α (n) = n x i=0 pa(n − i)γψ α(i). next, we prove two similar theorems. deﬁnition. let ea α (n) be the number of even partitions of n such that the number of equal parts is less than or equals to α ∈ n and oa α (n) be the number of odd partitions of n such that the number of equal parts is less than or equals to α ∈ n \ {0}. then we deﬁne (cid:3) ¯pa α (n) := ea α (n) − oa α (n) and ¯pa α (0) := 1. theorem 2.2. let an = (an let ψ be an one to one function such that ψ : n → n. then ij ) be the solution matrix of n = pi≥0 2ini and 1 (n) = x ¯pa i≥1 y j≥1 ¯pa(an ij ) for all n ∈ n. proof. we proved that for |q| < 1, 1 = y n≥1 (1 − qψ(n)) ∞ y i=0 (1 + q2iψ(n)) y n≥1 8 kim, bongju in the proof of theorem 2.1. so, (1 − qψ(n)) = y n≥1 ∞ y i=0 y n≥1 1 (1 + q2iψ(n)) and x n≥0 1 (n)qn = y ¯pa (x i≥0 n≥0 ¯pa(n)q2in). this proves the theorem. (cid:3) theorem 2.3. let an,α = (an,α and let ψ be an one to one function such that ψ : n → n. then ij ) be the solution matrix of n = pi≥0(α+1)ini ¯pa(n) = x i≥1 y j≥1 α (an,α ¯pa ij ) for all n ∈ n and for all even natural number α. proof. let α be an even natural number. if |q| < 1, (1 + q(α+1)i+1ψ(n)) y n≥1 (1 + qψ(n)) = y n≥1 i y i=0 (1 − q(α+1)iψ(n) + q(α+1)i2ψ(n) − · · · + q(α+1)iαψ(n)) y n≥1 for all i ∈ n. so, 1 = lim i→∞ y n≥1 (1 + q(α+1)i+1ψ(n)) (1 + qψ(n)) = y n≥1 ∞ y i=0 (1 − q(α+1)iψ(n) + q(α+1)i2ψ(n) − · · · + q(α+1)iαψ(n)). y n≥1 therefore, 1 (1 + qψ(n)) = ∞ y i=0 y n≥1 and (1 − q(α+1)iψ(n) + q(α+1)i2ψ(n) − · · · + q(α+1)iαψ(n)) y n≥1 x n≥0 ¯pa(n) = y i≥0 (x n≥0 α (n)q(α+1)in). ¯pa this proves the theorem. (cid:3) partition number identities which are true for all set of parts 9 3. appendix: examples in this appendix, we will consider two identities for pa(10) and pa i) let us consider the expression of pa(10) by pa 1 (10). 1 s. the indeterminate equation for this identity is the solution matrix of this equation and the identity for pa(10) are 10 = n0 + 2n1 + 4n2 + · · · .   1 0 1 0 3 0 5 0 2 2 4 2 0 2 0 2 1 4 3 4 2 6 0 6 8 1 10 0 0 1 2 0 1 0 0 0 1 0 0 0 0 1 2 0 1 0 0 0 0 0 1 0 0 0 0 0 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·   , pa(10) = pa +pa +pa +pa +pa +pa 1 (1)pa 1 (5) + pa 1 (2)pa 1 (2)pa 1 (4)pa 1 (8)pa 1 (1) + pa 1 (2)pa 1 (4) + pa 1 (2) + pa 1 (3) + pa 1 (1) + pa 1 (1)pa 1 (2)pa 1 (2)pa 1 (4)pa 1 (6)pa 1 (10). 1 (2) + pa 1 (1) 1 (1) 1 (1)pa 1 (1) 1 (2) + pa 1 (1)pa 1 (3) 1 (6)pa 1 (1) now, we calculate pa(10) for three set of parts. 1) ψ(n) = {p | p is a prime} let us calculate partition numbers. 10 = 5 + 5 3 + 2 + 5 3 + 3 + 2 + 2 7 + 3 2 + 2 + 2 + 2 + 2 , pa 1 (1) = 0 pa 1 (2) = 1 pa pa 1 (3) = 1 pa pa pa 1 (4) = 0 pa 1 (5) = 2 1 (6) = 0 1 (8) = 1 1 (10) = 2. so, pa(10) = 5 and since pa pa(10) = pa 1 (5) + pa 1 (1) = pa 1 (2)pa 1 (4) = pa 1 (2) + pa 1 (6) = 0, 1 (10) = 2 + 1 + 2 = 5. 2) ψ(n) = {n2 | n ∈ n}. let us calculate the partition numbers. 10 = 1 + 9 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 4 + 1 + 1 + 1 + 1 + 1 + 1 4 + 4 + 1 + 1 , so, pa(10) = 4 and since pa pa(10) = pa 1 (2) = pa 1 (1) + pa = 1 + 1 + 1 + 1 = 4. 1 (1)pa 1 (3) = pa 1 (5) + pa 1 (6) = pa 1 (4)pa 1 (1)pa pa 1 (1) = 1 pa 1 (2) = 0 pa pa 1 (3) = 0 pa pa 1 (4) = 1 pa pa 1 (8) = 0, 1 (5) = 1 1 (6) = 0 1 (8) = 0 1 (10) = 1. 1 (1) + pa 1 (10) 10 kim, bongju 3) ψ(n) = {n | n is an odd number}. let us calculate the partition numbers. 10 = 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 3 + 1 + 1 + 1 + 1 + 1 + 1 + 1 5 + 1 + 1 + 1 + 1 + 1 7 + 1 + 1 + 1 9 + 1 3 + 3 + 1 + 1 + 1 + 1 3 + 3 + 3 + 1 5 + 5 7 + 3 3 + 5 + 1 + 1 so, pa(10) = 10 and since pa 1 (2) = 0, , pa 1 (1) = 1 pa pa 1 (2) = 0 pa pa 1 (3) = 1 pa pa 1 (4) = 1 pa 1 (5) = 1 1 (6) = 1 1 (8) = 2 1 (10) = 2. pa(10) = pa 1 (3) + pa 1 (4)pa + pa 1 (3) + pa 1 (1) + pa = 1 + 1 + 1 + 1 + 1 + 1 + 2 + 2 1 (1) + pa 1 (1)pa 1 (1) + pa 1 (1)pa 1 (1) + pa 1 (8)pa 1 (1)pa 1 (4)pa 1 (6)pa 1 (5) 1 (10) = 10. ii) let us consider the identity for pa 1 (10). the indeterminate equation for this identity is and solutions of this equation are n = 2n0 + 4n1 + 8n2 + · · · solutions n (1, 0, 0, · · · ) 2 (2, 0, 0, · · · ), (0, 1, 0, · · · ) 4 (3, 0, 0, · · · ), (1, 1, 0, · · · ) 6 (4, 0, 0, · · · ), (2, 1, 0, · · · ), (0, 0, 1, · · · ), (0, 2, 0, · · · ) 8 10 (5, 0, 0, · · · ), (1, 2, 0, · · · ), (1, 0, 1, · · · ), (3, 1, 0, · · · ) and ∅ for n = odd number. so, n 0 2 4 6 8 10 1 (n) γψ 1 ¯pa(1) ¯pa(2) + ¯pa(1) ¯pa(3) + ¯pa(1)¯pa(1) ¯pa(4) + ¯pa(2)¯pa(1) + ¯pa(2) + ¯pa(1) ¯pa(5) + ¯pa(2)¯pa(1) + ¯pa(1)¯pa(1) + ¯pa(3)¯pa(1) partition number identities which are true for all set of parts 11 and γψ therefore, if we denote pa(n) by pa(n) and ¯pa(n) by ¯pa(n), 1 (n) = 0 for n = odd number. pa 1 (10) = pa(10) + pa(8)¯pa(1) + pa(6)[¯pa(2) + ¯pa(1)] +pa(4)[¯pa(3) + ¯pa(1)¯pa(1)] +pa(2)[¯pa(4) + ¯pa(2)¯pa(1) + ¯pa(2) + ¯pa(1)] +¯pa(5) + ¯pa(2)¯pa(1) + ¯pa(1)¯pa(1) + ¯pa(3)¯pa(1). 1 (10) = 2 and now, we claulate for two set of parts {p | p is a prime} and {n2 | n ∈ n}. 1) for {p | p is a prime}, pa pa(2) = 1 pa(4) = 1 pa(6) = 2 pa(8) = 3 pa(10) = 5 if we calculate pa ¯pa(1) = 0 ¯pa(2) = −1 ¯pa(3) = −1 ¯pa(4) = 1 ¯pa(5) = 0. 1 (10), pa 1 (10) = 2 = 5 + 3 × 0 + 2 × [(−1) + 0] + 1 × [(−1) + 0 × 0] + 1 × [1 + (−1) × 0 + (−1) + 0] + 0 + 0 × (−1) + 0 × 0 + 0 × (−1). 1 (10) = 1 and 2) for {n2 | n ∈ n}, pa ¯pa(1) = −1 ¯pa(2) = 1 ¯pa(3) = −1 ¯pa(4) = 0 ¯pa(5) = 0. pa(2) = 1 pa(4) = 2 pa(6) = 2 pa(8) = 3 pa(10) = 4 if we calculate pa 1 (10), pa 1 (10) = 1 = 4 + 3 × (−1) + 2 × [1 + (−1)] + 2 × [(−1) + (−1) × (−1)] + 1 × [0 + 1 × (−1) + 1 + (−1)] + 0 + (−1) × 1 + (−1) × (−1) + (−1) × (−1). references "
1803.08096, we extend the caﬀarelli–kohn–nirenberg type partial regularity theory for the steady 5- dimensional fractional navier–stokes equations with external force to the hyperdissipative set- ting. in our argument we use the methods of colombo–de lellis–massaccesi to apply a blowup procedure adapted from work of ladyzhenskaya–seregin. 1 
1803.08097," safety-critical software systems are those whose failure or malfunc- tion could result in casualty and/or serious financial loss. in such systems, safety assurance cases (sacs) are an emerging approach that adopts a proactive strategy to produce structuralized safety jus- tifications and arguments. while sacs are recommended in many software-intensive safety-critical domains, the lack of knowledge regarding the practitioners’ perspectives on using sacs hinders effective adoption of this approach. to gain such knowledge, we interviewed nine practitioners and safety experts who focused on safety-critical software systems. in general, our participants found the sac approach beneficial for communication of safety arguments and management of safety issues in a multidisciplinary setting. the challenges they faced when using sacs were primarily associated with (1) a lack of tool support, (2) insufficient process integration, and (3) scarcity of experienced personnel. to overcome those chal- lenges, our participants suggested tactics that focused on creating direct safety arguments. process and organizational adjustments are also needed to streamline sac analysis and creation. finally, our participants emphasized the importance of knowledge sharing about sacs across software-intensive safety-critical domains. ccs concepts • general and reference → empirical studies; • software and its engineering → software safety; keywords safety-critical systems, safety assurance case, practitioner perspec- tives, empirical study. acm reference format: jinghui cheng, micayla goodrum, ronald metoyer, and jane cleland-huang. 2018. how do practitioners perceive assurance cases in safety-critical software systems?. in chase’18: ieee/acm 11th international workshop on cooperative and human aspects of software, may 27, 2018, gothenburg, permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. ing with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. chase’18, may 27, 2018, gothenburg, sweden © 2018 copyright held by the owner/author(s). publication rights licensed to the association for computing machinery. acm isbn 978-1-4503-5725-8/18/05...$15.00 https://doi.org/10.1145/3195836.3195838 sweden. acm, new york, ny, usa, 4 pages. https://doi.org/10.1145/3195836. 3195838 "
1803.08099," we have studied the p → γ(cid:63)γ(cid:63) transition form factors (p = π0, η, η(cid:48)) within a chiral invariant framework that allows us to relate the three form-factors and evaluate the corresponding contributions to the muon anomalous magnetic moment aµ = (gµ − 2)/2, through pseudoscalar pole contributions. we use a chiral invariant lagrangian to describe the inter- actions between the pseudo-goldstones from the spontaneous chiral sym- metry breaking and the massive meson resonances. we will consider just the lightest vector and pseudoscalar resonance multiplets. photon inter- actions and u (3) ﬂavor breaking eﬀects are accounted for in this covariant framework. this article studies the most general corrections of order m2 p within this setting. requiring short-distance constraints ﬁxes most of the parameters entering the form-factors, consistent with previous determi- nations. the remaining ones are obtained from a ﬁt of these form-factors to experimental measurements in the space-like (q2 ≤ 0) region of photon momenta. no time-like observable is included in our ﬁts. the combination of data, chiral symmetry relations between form-factors and high-energy constraints allows us to determine with improved precision the on-shell p - pole contribution to the hadronic light-by-light scattering of the muon = (8.47 ± 0.16) · 10−10 anomalous magnetic moment: we obtain ap,hlbl for our best ﬁt. this result was obtained excluding babar π0 data, which our analysis ﬁnds in conﬂict with the remaining experimental inputs. this study also allows us to determine the parameters describing the η−η(cid:48) sys- tem in the two-mixing angle scheme and their correlations. µ 1 contents "
1803.08100,"— uncertainty in modeling real world physics makes transferring traditional open-loop motion planning techniques from simulation to the real world particularly challenging. available closed-loop policy learning approaches, for physics- based manipulation tasks, typically either focus on single object manipulation, or rely on imitation learning, which inherently constrains task generalization and performance to the available demonstrations. in this work, we propose an approach to learn a policy for physics-based manipulation in clutter, which enables the robot to react to the uncertain dynamics of the real world. we start with presenting an imitation learning technique which compiles demonstrations from a sampling-based planner into an action-value function encoded as a deep neural network. we then use the learned action-value function to guide a look-ahead planner, giving us a control policy. lastly, we propose to reﬁne the deep action-value function through reinforcement learning, taking advantage of the look-ahead planner. we evaluate our approach in a physics-enabled simulation environment with artiﬁcially injected uncertainty, as well as in a real world task of manipulation in clutter. "
1803.08102,". mixing by cutting-and-shuﬄing can be understood and predicted using dynamical systems based tools and techniques. in existing studies, mixing is generated by maps that repeat the same cut-and-shuﬄe process at every iteration, in a “ﬁxed” manner. however, mixing can be greatly improved by varying the cut-and-shuﬄe parameters at each step, using a “variable” approach. to demonstrate this approach, we show how to optimize mixing by cutting-and-shuﬄing on the one-dimensional line interval, known as an interval exchange transformation (iet). mixing can be signiﬁcantly improved by optimizing variable protocols, especially for initial conditions more complex than just a simple two-color line interval. while we show that optimal variable iets can be found analytically for arbitrary numbers of iterations, for more complex cutting-and-shuﬄing systems, computationally expensive numerical optimization methods would be required. furthermore, the number of control parameters grows linearly with the number of iterations in variable systems. therefore, optimizing over large numbers of iterations is generally computationally prohibitive. we demonstrate an ad hoc approach to cutting-and-shuﬄing that is computationally inexpensive and guarantees the mixing metric is within a constant factor of the optimum. this ad hoc approach yields signiﬁcantly better mixing than ﬁxed iets which are known to produce weak-mixing, because cut pieces never reconnect. the heuristic principles of this method can be applied to more general cutting-and-shuﬄing systems. key words. mixing optimization, cutting-and-shuﬄing, interval exchange transformation ams subject classiﬁcations. 37a25, 49j21, 49k21, 49n90 "
1803.08104,"—in the future, sensor nodes or internet of things (iots) will be tasked with sampling the environment. these nodes/devices are likely to be powered by a hybrid access point (hap) wirelessly, and may be programmed by the hap with a sampling time to collect sensory data, carry out computation, and transmit sensed data to the hap. a key challenge, however, is random channel gains, which cause sensor nodes to receive varying amounts of radio frequency (rf) energy. to this end, we formulate a stochastic program to determine the charging time of the hap and sampling time of sensor nodes. our objective is to minimize the expected penalty incurred when sensor nodes experience an energy shortfall. we consider two cases: single and multi time slots. in the former, we determine a suitable hap charging time and nodes sampling time on a slot-by-slot basis whilst the latter considers the best charging and sampling time for use in the next t slots. we conduct experiments over channel gains drawn from the gaussian, rayleigh or rician distribution. numerical results conﬁrm our stochastic program can be used to compute good charging and sampling times that incur the minimum penalty over the said distributions. index terms—wireless sensor networks, wireless charging, stochastic programs, sample average approximation (saa). "
1803.08105," we present a 45 ks chandra observation of the quasar ulas j1342+0928 at z = 7.54. we detect 14.0+4.8 −3.7 counts from the quasar in the observed-frame energy range 0.5–7.0 kev (6σ detection), representing the most distant non-transient astronomical source identiﬁed in x-rays to date. the present data are suﬃcient only to infer rough constraints on the spectral parameters. we ﬁnd an x-ray hardness ratio of hr = −0.51+0.26 −0.28 between the 0.5–2.0 kev and 2.0–7.0 kev ranges and derive a power-law photon index of γ = 1.95+0.55 −0.53. assuming a typical value for high-redshift quasars of γ = 1.9, ulas j1342+0928 has a 2–10 kev rest-frame x-ray luminosity of l2−10 = 11.6+4.3 −3.5 × 1044 erg s−1. its x-ray-to-optical power-law slope is αox = −1.67+0.16 −0.10, consistent with the general trend indicating that the x-ray emission in the most bolometrically powerful quasars is weaker relative to their optical emission. keywords: cosmology: observations — cosmology: early universe — quasars: individual (ulas j134208.10+092838.61) 8 1 0 2   r a   m 1 2     ] . a g h p - o r t s a [     1 v 5 0 1 8 0 . 3 0 8 1 : v i x r a corresponding author: eduardo ba˜nados ebanados@carnegiescience.edu ∗ carnegie-princeton fellow 2 ba˜nados et al. "
1803.08106,". we consider general symmetric systems of ﬁrst order linear partial dif- ferential operators on domains ω ⊂ rd, and we seek suﬃcient conditions on the coeﬃcients which ensure essential self-adjointness. the coeﬃcients of the ﬁrst order terms are only required to belong to c 1(ω) and there is no ellipticity condition. our criterion writes as the completeness of an associated riemannian structure which en- codes the propagation velocities of the system. as an application we obtain suﬃcient conditions for conﬁnement of energy for some wave propagation problems of classical physics. "
1803.08107,". in this article, we consider compact surfaces σ having constant mean curvature h (h- surfaces) whose boundary γ = ∂σ ⊂ m0 = m × f {0} is transversal to the slice m0 of the warped product m × f r, here m denotes a hadamard surface. we obtain height estimate for a such surface σ having positive constant mean curvature involving the area of a part of σ above of m0 and the volume it bounds. also we give general conditions for the existence of rotationally-invariant topological spheres having positive constant mean curvature h in the warped product h × f r, where h denotes the hyperbolic disc. finally we present a non-trivial example of such spheres. "
1803.08111,"— in the standard mechanism design framework (hurwicz-reiter [1]), there is a central authority that gathers agents’ messages and subsequently determines the allocation and tax for each agent. we consider a scenario where, due to communication overhead and other constraints, such broad- casting of messages to a central authority cannot take place. instead, only local message exchange is allowed between agents. as a result, each agent should be able to determine her own allo- cation and tax based on messages in the local neighborhood, as deﬁned by a given message graph describing the communication constraints. this scenario gives rise to a novel research direction that we call “distributed mechanism design”. in this paper, we propose such a distributed mechanism for the problem of rate allocation in a multicast transmission network. the proposed mechanism fully implements the optimal allocation in nash equilibria and its message space dimension is linear with respect to the number of agents in the network. index terms— mechanism design, rate allocation, decentral- ized optimization, strategic users, nash equilibrium "
1803.08112," we age-date the stellar populations associated with 12 historic nearby core-collapse supernovae (ccsne) and 2 supernova impostors, and from these ages, we infer their initial masses and associated uncertainties. to do this, we have obtained new hst imaging covering these ccsne. using these images, we measure resolved stellar photometry for the stars surrounding the locations of the sne. we then ﬁt the color-magnitude distributions of this photometry with stellar evolution models to determine the ages of any young existing populations present. from these age distributions, we infer the most likely progenitor mass for all of the sne in our sample. we ﬁnd ages between 4 and 50 myr, corresponding to masses from 7.5 to 59 solar masses. there were no sne that lacked a young population within 50 pc. our sample contains 4 type ib/c sne; their masses have a wide range of values, suggesting that the progenitors of stripped-envelope sne are binary systems. both impostors have masses constrained to be <∼ 7.5 solar masses. in cases with precursor imaging measurements, we ﬁnd that age-dating and precursor imaging give consistent progenitor masses. this consistency implies that, although the uncertainties for each technique are signiﬁcantly diﬀerent, the results of both are reliable to the measured uncertainties. we combine these new measurements with those from our previous work and ﬁnd that the distribution of 25 core-collapse sne progenitor masses is consistent with a standard salpeter power-law mass function, no upper mass cutoﬀ, and an assumed minimum mass for core-collapse of 7.5 m(cid:12). the distribution is consistent with a minimum mass <9.5 m(cid:12). subject headings: supernovae — 1. "
1803.08114,". in applications like medical imaging, error correction, and sensor networks, one needs to solve large-scale linear systems that may be corrupted by a small number of, but arbitrarily large, corruptions. we consider solving such large-scale systems of linear equations ax = b that are inconsistent due to corruptions in the measurement vector b. with this as our motivating ex- ample, we develop an approach for this setting that allows detection of the corrupted entries and thus convergence to the “true” solution of the original system. we provide analytical justiﬁcation for our approaches as well as experimental evidence on real and synthetic systems. 8 1 0 2   r a   m 1 2     ] . a n h t a m [     1 v 4 1 1 8 0 . 3 0 8 1 : v i x r a "
1803.08115," the physics of superconducting ﬁlms, and especially the role of remanent magne- tization has a deﬁning inﬂuence on the magnetic ﬁelds used to hold and manipu- late atoms on superconducting atomchips. we magnetically trap ultracold 87rb atoms on a 200 µm wide and 500 nm thick cryogenically cooled niobium z-wire structure. by measuring the distance of the atomcloud to the trapping wire for diﬀerent transport currents and bias ﬁelds, we probe the trapping characteristics of the niobium superconducting structure. at distances closer than the trapping wire width, we observe a diﬀerent behaviour than that of normal conducting wire traps. furthermore, we measure a stable magnetic trap at zero transport cur- rent. these observations point to the presence of a remanent magnetization in our niobium ﬁlm which is induced by a transport current. this current-induced magnetization deﬁnes the trap close to the chip surface. our measurements agree very well with an analytic prediction based on the critical state model (csm). our results provide a new tool to control atom trapping on superconducting atomchips by designing the current distribution through its current history. contents "
1803.08118," seglearn is an open-source python package for machine learning time series or sequences using a sliding window segmentation approach. the implementation provides a ﬂexible pipeline for tackling classiﬁcation, regression, and forecasting problems with multivariate sequence and contextual data. this package is compatible with scikit-learn and is listed under scikit-learn ”related projects”. the package depends on numpy, scipy, and scikit-learn. seglearn is distributed under the bsd 3-clause license. documenta- tion includes a detailed api description, user guide, and examples. unit tests provide a high degree of code coverage. source code and documentation can be download from https://github.com/dmbee/seglearn. keywords: machine-learning, time-series, sequences, python "
1803.08119," a z2 symmetry that extends the weak interaction, su (2)l → su (2)l × su (2)(cid:48), and the higgs sector, h(2) → h(2, 1) + h(cid:48)(1, 2), yields a standard model quartic coupling that vanishes at scale v(cid:48) = (cid:104)h(cid:48) (cid:105) (cid:29) (cid:104)h(cid:105). near v(cid:48), theories either have a “prime” sector, or possess “left-right” (lr) symmetry with su (2)(cid:48) = su (2)r. if the z2 symmetry incorporates spacetime parity, these theories can solve the strong cp problem. the lr theories have all quark and lepton masses arising from operators of dimension 5 or more, requiring froggatt-nielsen structures. two-loop contributions to ¯θ are estimated and typically lead to a neutron electric dipole moment of order 10−27e cm that can be observed in future experiments. minimal models, with gauge group su (3) × su (2)l × su (2)l × u (1)b−l, have precise gauge coupling uniﬁcation for v(cid:48) = 1010±1 gev, successfully correlating gauge uniﬁcation with the observed higgs mass of 125 gev. with su (3) × u (1)b−l embedded in su (4), the central value of the uniﬁcation scale is reduced from 1016−17 gev to below 1016 gev, improving the likelihood of proton decay discovery. uniﬁed theories based on so(10) × cp are constructed that have h + h(cid:48) in a 16 or 144 and generate higher-dimensional ﬂavor operators, while maintaining perturbative gauge couplings. 8 1 0 2   r a   m 1 2     ] h p - p e h [     1 v 9 1 1 8 0 . 3 0 8 1 : v i x r a 1 contents "
1803.08120,". we study the stochastic six vertex model and prove that under weak asymmetry scaling (i.e., when the parameter ∆ → 1+ so as to zoom into the ferroelectric/disordered phase critical point) its height function ﬂuctuations converge to the solution to the kardar–parisi–zhang (kpz) equation. we also prove that the one-dimensional family of stochastic gibbs states for the symmetric six vertex model converge under the same scaling to the stationary solution to the stochastic burgers equation. we achieve this through a new markov duality method which shows how duality can be leveraged to prove previously inaccessible results in hydrodynamics and spde limits. our starting point is an exact microscopic hopf–cole transform for the stochastic six vertex model which follows from the model’s known one-particle markov self-duality. given this transform, the crucial step is to establish self-averaging for speciﬁc quadratic function of the transformed height function. we use the model’s two-particle self-duality to produce explicit expressions (as bethe ansatz contour integrals) for conditional expectations from which we extract time-decorrelation and hence self-averaging in time. the crux of the markov duality method is that the entire convergence result reduces to precise estimates on the one-particle and two-particle transition probabilities. 8 1 0 2   r p a 3       ] . r p h t a m [     2 v 0 2 1 8 0 . 3 0 8 1 : v i x r a contents "
1803.08121," in high performance computing, scheduling of tasks and allocation to machines is very critical especially when we are dealing with heterogeneous execution costs. simulations can be performed with a large variety of environments and application models. however, this technique is sensitive to bias when it relies on random instances with an uncontrolled distribution. we use methods from the literature to provide formal guarantee on the distribution of the instance. in particular, it is desirable to ensure a uniform distribution among the instances with a given task and machine heterogeneity. in this article, we propose a method that generates instances (cost matrices) with a known distribution where tasks are scheduled on machines with heterogeneous execution costs. 1 "
1803.08125," typical tsallis’ statistical mechanics’ quantiﬁers like the partition function and the mean energy exhibit poles. we are speaking of the partition function z and the mean energy < u >. the poles ap- pear for distinctive values of tsallis’ characteristic real parameter q, at a numerable set of rational numbers of the q−line. these poles are dealt with dimensional regularization resources. the physical eﬀects of these poles on the speciﬁc heats are studied here for the two-body classical gravitation potential. keywords: tsallis entropy, divergences, dimensional regular- ization, speciﬁc heat. 1 1 "
1803.08127,". recently burkhardt et. al. introduced the k-checkerboard random matrix ensembles, which have a split limiting behavior of the eigenvalues (in the limit all but k of the eigenvalues are on the order of √n and converge to semi-circular behavior, with the remaining k of size n and converging to hollow gaussian ensembles). we generalize their work to consider non-hermitian ensembles with complex eigenvalues; instead of a blip new behavior is seen, ranging from multiple satellites to annular rings. these results are based on moment method techniques adapted to the complex plane as well as analysis of singular values. contents "
1803.08128,": in this paper we propose a ﬂexible cure rate model with frailty term in latent risk, which is obtained by incorporating a frailty term in risk function of latent competing causes. the number of competing causes of the event of interest follows negative binomial distribution and the frailty variable follows power variance function distribution, in which includes other frailty models such as gamma, positive stable and inverse gaussian frailty models as special cases. the proposed model takes into account the presence of covariates and right-censored survival data suitable for populations with a cure rate. besides, it allows to quantify the degree of unobserved heterogeneity induced by unobservable risk factors, in which is important to explain the survival time. once the posterior distribution has not close form, markov chain monte carlo simulations are consider for estimation procedure. we performed several simulation studies and the practical relevance of the proposed model is demonstrated in a real data set. keywords: bayesian model; competing causes; cure rate models; frailty models; power vari- ance function. "
1803.08134,". although deep nets have resulted in high accuracies for various vi- sual tasks, their computational and space requirements are prohibitively high for inclusion on devices without high-end gpus. in this paper, we introduce a neu- ron/ﬁlter level pruning framework based on fisher’s lda which leads to high accuracies for a wide array of facial trait classiﬁcation tasks, while signiﬁcantly reducing space/computational complexities. the approach is general and can be applied to convolutional, fully-connected, and module-based deep structures, in all cases leveraging the high decorrelation of neuron activations found in the pre-decision layer and cross-layer deconv dependency. experimental results on binary and multi-category facial traits from the lfwa and adience datasets il- lustrate the framework’s comparable/better performance to state-of-the-art prun- ing approaches and compact structures (e.g. squeezenet, mobilenet). ours suc- cessfully maintains comparable accuracies even after discarding most parameters (98%-99% for vgg-16, 82% for googlenet) and with signiﬁcant flop reduc- tions (83% for vgg-16, 64% for googlenet). keywords: deep nets pruning; facial trait classiﬁcation; fisher lda 8 1 0 2   r a   m 1 2     ] v c . s c [     1 v 4 3 1 8 0 . 3 0 8 1 : v i x r a "
1803.08135,". we discuss the pistone-sempi exponential manifold on the ﬁnite-dimensional gaussian space. we consider the role of the entropy, the continuity of translations, poincar´e-type inequalities, the generalized diﬀerentiability of probability densities of the gaussian space. keywords: information geometry, pistone-sempi exponential mani- fold, gaussian orlicz space, gaussian orlicz-sobolev space 1 "
1803.08136," various types of equilibrium processes involve electric ﬁelds. in some cases, the electrical energy appears to be negative (e.g. if the voltage is ﬁxed by an external source). this pa- per explains how to derive the correct thermo-dynamic potential for electro-static phenomena, whether the voltage is ﬁxed, or the charge is ﬁxed, or some combination is ﬁxed. in particular, we explain, in complete detail, why ﬁxing the voltage introduces “a minus sign” in the electrical energy. two explanations are given. the ﬁrst explanation is based on a lumped-parameter argument (i.e. a lumped-capacitor model). the second explanation uses a distributed parameter model (i.e. a partial diﬀerential equation (pde) model) of a dielectric medium; in this case, we allow for non-linearity and external polarization eﬀects. connections with legendre (duality) transforms are also discussed. 1 "
1803.08137," we revisit the blind deconvolution problem with a focus on understanding its robustness and con- vergence properties. provable robustness to noise and other perturbations is receiving recent interest in vision, from obtaining immunity to adversarial attacks to assessing and describing failure modes of algorithms in mission critical applications. further, many blind deconvolution methods based on deep architectures internally make use of or optimize the basic formulation, so a clearer under- standing of how this sub-module behaves, when it can be solved, and what noise injection it can tolerate is a ﬁrst order requirement. we derive new insights into the theoretical underpinnings of blind deconvolution. the algorithm that emerges has nice convergence guarantees and is provably robust in a sense we formalize in the paper. interestingly, these technical results play out very well in practice, where on standard datasets our algorithm yields results competitive with or superior to the state of the art. keywords: blind deconvolution, robust continuous optimization "
1803.08139,"—in the past few decades, ﬁbre bragg grating (fbg) sensors have gained a lot of attention in the ﬁeld of distributed point strain measurement. one of the most interesting properties of these sensors is the presumed linear relationship between the strain and the peak wavelength shift of the fbg reﬂected spectra. however, subjecting sensors to a non-uniform stress ﬁeld will in general result in a strain estimation error when using this linear relationship. in this paper we propose a new strain estimation algorithm that accurately estimates the mean strain value in the case of non-uniform strain distributions. to do so, we ﬁrst introduce an approximation of the classical transfer matrix model, which we will refer to as the approximated transfer matrix model (atmm). this model facilitates the analysis of fbg reﬂected spectra under arbitrary strain distributions. based on this new formulation, we derive a maximum likelihood estimator of the mean strain value. the algorithm is validated using both computer simulations and experimental fbg measurements. compared to state-of-the-art methods, which typically introduce errors of tens of microstrains, the proposed method is able to compensate for this error. in the typical examples that were analysed in this study, mean strain errors of around 30µε were compensated. index terms—ﬁber bragg grating, fbg, ﬁber optic sensing, reﬂected spectra, strain distribution, transfer matrix model. "
1803.08140,". the frequency of occurrence of “locally repeated” values of arithmetic functions is a common theme in analytic number theory, for instance in the erd˝os-mirsky problem on coincidences of the divisor function at consecutive integers, the analogous problem for the euler totient function, and the quantitative conjectures of erd˝os, pomerance and sark˝ozy and of graham, holt and pomerance on the frequency of occurrences. in this paper we introduce the corresponding problems in the setting of polynomials over a ﬁnite ﬁeld, and completely solve them in the large ﬁnite ﬁeld limit. "
1803.08141," one of the phenomena that inﬂuences signiﬁcantly the performance of low-density parity-check codes is known as trapping sets. an (a, b) elementary trapping set, or simply an ets where a is the size and b is the number of degree-one check nodes and b a < 1, causes high decoding failure rate and exert a strong inﬂuence on the error ﬂoor. in this paper, we provide sufﬁcient conditions for exponent matrices to have fully connected (3, n)-regular qc-ldpc codes with girths 6 and 8 whose tanner graphs are free of small etss. applying sufﬁcient conditions on the exponent matrix to remove some 8-cycles results in removing all 4-cycles, 6-cycles as well as some small elementary trapping sets. for each girth we obtain a lower bound on the lifting degree and present exponent matrices with column weight three whose corresponding tanner graph is free of certain etss. index terms ldpc codes, girth, tanner graph, trapping set. "
1803.08145," a word is called β-free if it has no factors of exponent greater than or equal to β. the repetition threshold rt(k) is the inﬁmum of the set of all β such that there are arbitrarily long k-ary β-free words (or equivalently, there are k-ary β-free words of every suﬃciently large length, or even every length). these three equivalent deﬁnitions of the repetition threshold give rise to three natural deﬁnitions of a repetition threshold for circular words. the inﬁmum of the set of all β such that (a) there are arbitrarily long k-ary β-free circular words is called the weak circular repetition threshold, denoted crtw(k); (b) there are k-ary β-free circular words of every suﬃciently large length is called the intermediate circular repetition threshold, denoted crti(k); (c) there are k-ary β-free circular words of every length is called the strong circular repetition threshold, denoted crts(k). 2 and crts(5) = 4 we prove that crts(4) = 3 3 , conﬁrming a conjecture of gorbunova and providing the last unknown values of the strong circular repetition threshold. we also prove that crti(3) = crtw(3) = rt(3) = 7 4 . msc 2010: 68r15 keywords: circular words; repetition threshold; circular repetition threshold 1 "
1803.08151,"—in this paper, we enunciate the theorem of secrecy in tagged protocols using the theory of witness-functions and we run a formal analysis on a new tagged version of the needham-schroeder public-key protocol using this theorem. we discuss the signiﬁcance of tagging in securing cryptographic protocols as well. index terms—cryptographic protocols, intruder, secrecy, secu- rity, tag, uniﬁcation, witness-function. notice 1 c(cid:13) 2018 ieee. personal use of this material is permitted. permission from ieee must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. "
1803.08152,"—this paper presents a distributed position synchro- nization strategy that also preserves the initial communication links for single-integrator multi-agent systems with time-varying delays. the strategy employs a coordinating proportional control derived from a speciﬁc type of potential energy, augmented with damping injected through a dynamic ﬁlter. the injected damping maintains all agents within the communication distances of their neighbours, and asymptotically stabilizes the multi-agent system, in the presence of time delays. regarding the closed-loop single- integrator multi-agent system as a double-integrator system suggests an extension of the proposed strategy to connectivity- preserving coordination of euler-lagrange networks with time- varying delays. lyapunov stability analysis and simulation results validate the two designs. index terms—connectivity preservation, distributed coordina- tion, multi-agent systems, time-varying delays. "
1803.08153," considered as a data-driven approach, fingerprinting localization solutions (fpss) en- joy huge popularity due to their good performance and minimal environment information requirement. this papers addresses applications of artiﬁcial intelligence to solve two prob- lems in received signal strength indicator (rssi) based fps, ﬁrst the cumbersome training database construction and second the extrapolation of ﬁngerprinting algorithm for similar buildings with slight environmental changes. after a concise overview of deep learning design techniques, two main techniques widely used in deep learning are exploited for the above mentioned issues namely data augmentation and transfer learning. we train a multi-layer neural network that learns the mapping from the observations to the locations. a data aug- mentation method is proposed to increase the training database size based on the structure of rssi measurements and hence reducing eﬀectively the amount of training data. then it is shown experimentally how a model trained for a particular building can be transfered to a similar one by ﬁne tuning with signiﬁcantly smaller training numbers. the paper implicitly discusses the new guidelines to consider about deep learning designs when they are employed in a new application context. "
1803.08154,". this paper provides a method to construct simultaneous conﬁdence bands for quantile functions and quantile eﬀects in nonlinear network and panel models with unobserved two-way eﬀects, strictly exogenous covariates, and possibly discrete outcome variables. the method is based upon projection of simultaneous conﬁdence bands for distribution functions constructed from ﬁxed eﬀects distribution regression estimators. these ﬁxed eﬀects estimators are bias corrected to deal with the incidental parameter problem. under asymptotic sequences where both dimensions of the data set grow at the same rate, the conﬁdence bands for the quantile functions and eﬀects have correct joint coverage in large samples. an empirical application to gravity models of trade illustrates the applicability of the methods to network data. "
1803.08155,". despite major methodological developments, bayesian inference for gaussian graphical models remains challenging in high dimension due to the tremendous size of the model space. this article proposes a method to infer the marginal and conditional independence structures between variables by multiple testing of hypotheses. speciﬁcally, we introduce closed-form bayes factors under the gaussian conjugate model to evaluate the null hypotheses of marginal and conditional independence between variables. their computation for all pairs of variables is shown to be extremely eﬃcient, thereby allowing us to address large problems with thousands of nodes. moreover, we derive exact tail probabilities from the null distributions of the bayes factors. these allow the use of any multiplicity correction procedure to control error rates for incorrect edge inclusion. we demonstrate the proposed approach to graphical model selection on various simulated examples as well as on a large gene expression data set from the cancer genome atlas. 1 "
1803.08157,"— we consider the problem of computing certain (mvoe) parameterized minimum volume outer ellipsoidal approximation of the minkowski sum of a ﬁnite number of ellipsoids. we clarify connections among several param- eterizations available in the literature, obtain novel analysis results regarding the conditions of optimality, and based on the same, propose two new algorithms for computing the pa- rameterized mvoe. numerical results reveal faster runtime for the proposed algorithms than the state-of-the-art semideﬁnite programming approach of computing the same. "
1803.08159,"—this paper presents a globally stable teleopera- tion control strategy for systems with time-varying delays that eliminates the need for velocity measurements through novel augmented immersion and invariance velocity observers. the new observers simplify a recent constructive immersion and in- variance velocity observer to achieve globally convergent velocity estimation with only n+2 states, where n is the number of degrees of freedom of the master and slave robots. they introduce dy- namic scaling factors to accelerate the speed of convergence of the velocity estimates and, thus, to limit the energy generated by the velocity estimation errors and to guarantee sufﬁcient estimate- based damping injection to dissipate the energy generated by the time-varying delays. the paper shows that proportional plus damping control with the simpliﬁed and augmented immersion and invariance-based velocity observers can synchronize the free master and slave motions in the presence of time-varying delays without using velocity measurements. numerical results illustrate the estimation performance of the new observers and the stability of a simulated two degrees-of-freedom nonlinear teleoperation system with time-varying delays under the proposed output feedback proportional plus damping control. "
1803.08160," we introduce a new diﬀusion process {xt}t≥0 to describe asset prices within an economic bubble cycle. the main feature of the process, which diﬀers from existing models, is the drift term where a mean-reversion is taken based on an exponential decay of the scaled price. our study shows the scaling factor on {xt}t≥0 is crucial for modelling economic bubbles as it mitigates the dependence structure between the price and parameters in the model. we prove both the process and its ﬁrst passage time are well-deﬁned. an eﬃcient calibration scheme, together with the probability density function for the process are given. moreover, by employing the perturbation technique, we deduce the closed-form density for the downward ﬁrst passage time, which therefore can be used in estimating the burst time of an economic bubble. the object of this study is to understand the asset price dynamics when a ﬁnancial bubble is believed to form, and correspondingly provide estimates to the bubble’s crash time. calibration examples on the us dot-com bubble and the 2007 chinese stock market crash verify the eﬀectiveness of the model itself. the example on bitcoin prediction conﬁrms that we can provide meaningful estimate on the downward probability for asset prices. keywords: economic bubbles, diﬀusion process, first passage time, perturbation, cryptocurrency 1 "
1803.08163," we conﬁrm the trend in the radial velocity data for epsilon indi a suggesting a long- period planetary companion and ﬁnd signiﬁcant curvature is present, suﬃcient to quantify epsilon indi ab as a cold jupiter with a minimum mass of 2.71+2.19 −0.44 mjup on a nearly circular orbit with a semi-major axis of 12.82+4.18 −0.71 au and an orbital period of 52.62+27.70 −4.12 yr. we also identify other signiﬁcant signals in the radial velocity data. we investigate a variety of spectral diagnostics and interpret these signals as arising from activity-induced radial velocity variations. in particular, the 2500 and 278 d signals are caused by magnetic cycles. while a planetary signal might be present in the 17.8 d signal, the origin of 17.8 and 11 d signals are most easily interpreted as arising in the rotation of the star with a period of about 35 d. we ﬁnd that traditional activity indicators have a variety of sensitivities. in particular, the sodium lines and cahk index are sensitive to all activity-induced signals. the line bisector measurement is sensitive to stellar rotation signal while hα is sensitive to the secondary magnetic cycle. in general, because of their diﬀerent sensitivities these activity indicators introduce extra noise if included in the noise model whereas diﬀerential rvs provide a robust proxy to remove wavelength-dependent noise eﬃciently. based on these analyses, we propose an activity diagnostics procedure for the detection of low amplitude signals in high precision radial velocity data. thus the epsilon indi system comprises of at least epsilon indi a, ab as well as a long period brown dwarf binary ba and bb; so it provides a benchmark case for our understanding of the formation of gas giants and brown dwarfs. key words: methods: statistical – methods: data analysis – techniques: radial ve- locities – stars: individual: epsilon indi a "
1803.08166," we consider a retailer who buys energy in the wholesale market and resells it to ﬁnal consumers. the retailer has to decide when to intervene to change the price he asks to his customers, in order to maximize his income. we model the problem as an inﬁnite-horizon stochastic impulse control problem. we characterize an optimal price strategy and provide analytical existence results for the equations involved. we then investigate the dependence on the intervention cost. in particular, we prove that the measure of the continuation region is asymptotic to the fourth root of the cost. finally, we provide some numerical results and consider a suitable extension of the model. msc classiﬁcation: 93e20, 91b70, 91b24. key words: price management, energy markets. impulse controls, quasi-variational inequality, asymptotic estimates, 1 "
1803.08167," electroactive polymers such as dielectric elastomers (des) have attracted signiﬁcant attention in recent years. computational techniques to solve the coupled electromechanical system of equations for this class of mate- rials have universally centered around fully coupled monolithic formulations, which while generating good accuracy requires signiﬁcant computational ex- pense. however, this has signiﬁcantly hindered the ability to solve large scale, fully three-dimensional problems involving complex deformations and electromechanical instabilities of des. in this work, we provide theoretical basis for the eﬀectiveness and accuracy of staggered explicit-implicit ﬁnite element formulations for this class of electromechanically coupled materials, and elicit the simplicity of the resulting staggered formulation. we demon- strate the stability and accuracy of the staggered approach by solving com- plex electromechanically coupled problems involving electroactive polymers, where we focus on problems involving electromechanical instabilities such as creasing, wrinkling, and bursting drops. in all examples, essentially identical results to the fully monolithic solution are obtained, showing the accuracy of the staggered approach at a signiﬁcantly reduced computational cost. keywords: staggered, explicit-implicit, creasing, dielectric elastomer, wrinkling, surface tension ∗corresponding author email address: parkhs@bu.edu (harold s. park) preprint submitted to cmame march 23, 2018 "
1803.08169," we extend analytic large network results on default contagion in random graphs to capture a pronounced block model structure. this includes as a special case the core-periphery network structure, which plays a prominent role in recent research on systemic risk. further, in the existing literature on systemic risk using random graph methods the problematic assumption that the distribution of liabilities solely depends on the creditor type seems to persist. under this assumption a straightforward application of the law of large numbers allows to turn edge related random elements into deterministic vertex properties. here we study a general setting in which the liabilities may depend on both the creditor and the debtor where this argument breaks down and a direct asymptotic analysis of the edge weighted random graph becomes necessary. among several other applications our results allow us to obtain resilience conditions for the entire network (for example the global ﬁnancial network) based only on subnetwork conditions. contrasting earlier research we also give an example that demonstrates how reshuﬄing edge weights to form blocks can in fact impact resilience even for otherwise very homogeneous networks. keywords: systemic risk, ﬁnancial contagion, inhomogeneous random graphs, weighted random graphs, directed random graphs, stochastic block model, core-periphery, assortative random graphs, counterparty dependent exposures 1 "
1803.08170," in the context of a sequential search problem, i explore large-generations learning dynamics for agents who suﬀer from the “gambler’s fallacy” — the statistical bias of anticipating too much regression to the mean for realizations of independent random events. searchers are uncertain about search pool qualities of diﬀerent periods but infer these fundamentals from search outcomes of the previous generation. searchers’ stopping decisions impose a censoring eﬀect on the data of their successors, as the val- ues they would have found in later periods had they kept searching remain unobserved. while innocuous for rational agents, this censoring eﬀect interacts with the gambler’s fallacy and creates a feedback loop between distorted stopping rules and pessimistic beliefs about search pool qualities of later periods. in general settings, the stopping rules used by diﬀerent generations monotonically converge to a steady-state rule that stops searching earlier than optimal. in settings where true pool qualities increase over time — so there is option value in rejecting above-average early draws — learning is monotonically harmful and welfare strictly decreases across generations. 1 "
1803.08171," in organizational and commercial settings, people often have clear roles and workﬂows against which functional and non-functional requirements can be extracted. however, in more social settings, such as platforms for enhancing social interaction, successful applica- tions are driven more by user emotional engagement than functionality, the drivers of user engagement are diﬃcult to identify. a key challenge is to understand people’s emotional goals so that they can be incorporated into the design. this paper proposes a novel frame- work called the emotional attachment framework that is based on existing models and theories of emotional attachment. its aim is to facilitate the process of capturing emotional goals in software engineering. to demonstrate the framework in use, emotional goals are elicited for a software application that aims to provide help for homeless people. the out- comes are evaluated by domain experts and compared with an alternative approach. the results indicate that the emotional attachment framework has the potential to help system analysts uncover additional emotional goals as well as provide valuable insights into these emotional goals. keywords: people-oriented software, requirements engineering, emotional goals, emotional attachment framework "
1803.08173," the james webb space telescope (jwst) will measure exoplanet transmission and eclipse spectroscopy at un- precedented precisions to better understand planet structure, dynamics, chemistry and formation. these are essential tools on the march toward biosignature searches on potentially habitable planets. we explore a range of exoplanet atmospheric conditions and forecast the expected results with jwst. we take realistic chimera models that match existing spitzer and hst results and simulate the spectra achievable with the jwst miri + nircam guaranteed time observations (gto) survey, which includes observations of hd 189733 b, wasp-80 b, hat-p-19 b, wasp- 107 b, gj 436 b and hat-p-26 b. we then retrieve atmospheric parameters from these spectra to estimate the precision to which the planets’ atmospheric compositions can be measured. we ﬁnd that emission spectra have well- constrained unimodal solutions but transmission spectra near 10× solar abundance and solar c/o ratios can suﬀer from bimodal solutions. broad wavelength coverage as well as higher precision data can resolve bimodal solutions and provide dramatically better atmospheric parameter constraints. we ﬁnd that metallicities can be measured to within 20% to 170%, which approaches the precisions on solar system planets, and c/o ratios can be constrained to ∼10% to 60%, assuming that observers can leverage short wavelength data to select the correct solution from the bimodal posteriors. these compositional precisions are suﬃcient to validate or refute predictions from disk formation models on ﬁnal atmospheric abundances as long as their history is not erased by planet evolution processes. we also show the extent to which eclipse mapping with jwst is possible on our brightest system hd 189733 b. keywords: planets and satellites: atmospheres — planets and satellites: composition 8 1 0 2   y a m   1 2 .     ] p e h p - o r t s a [     2 v 3 7 1 8 0 . 3 0 8 1 : v i x r a corresponding author: everett schlawin eas342 at email dot arizona .edu 2 schlawin et al. "
1803.08175," we place constraints on the propagation velocity of bulk turbulence within the intr- acluster medium of three clusters and an elliptical galaxy. using reﬂection grating spectrometer measurements of turbulent line broadening, we show that for these clus- ters, the 90% upper limit on turbulent velocities when accounting for instrumental broadening is too low to propagate energy radially to the cooling radius of the clus- ters within the required cooling time. in this way, we extend previous hitomi-based analysis on the perseus cluster to more clusters, with the intention of applying these results to a future, more extensive catalog. these results constrain models of turbu- lent heating in agn feedback by requiring a mechanism which can not only provide suﬃcient energy to oﬀset radiative cooling, but resupply that energy rapidly enough to balance cooling at each cluster radius. key words: intergalactic medium – intracluster medium – cooling ﬂows – turbulence. "
1803.08176," recent work on four dimensional eﬀective descriptions of the heterotic string has identiﬁed the moduli of such systems as being given by kernels of maps between ordinary dolbeault cohomology groups. the maps involved are deﬁned by the supergravity data of the background solutions. such structure is seen both in the case of calabi-yau compactiﬁcations with non-trivial constraints on moduli arising from the gauge bundle and in the case of some non-k¨ahler compactiﬁcations of the theory. this description of the moduli has allowed the explicit computation of the moduli stabilization eﬀects of a wide range of non-trivial gauge bundles on calabi-yau three-folds. in this paper we examine to what extent the ideas and techniques used in this work can be extended to the case of type iib string theory. certain simpliﬁcations arise in the type iib case in comparison to the heterotic situation. however, complications also arise due to the richer supergravity data of the theory inducing a more involved map structure. we illustrate our discussion with several concrete examples of compactiﬁcation of type iib string theory on conformal cicy three-folds with ﬂux. 8 1 0 2   r a   m 1 2     ] h t - p e h [     1 v 6 7 1 8 0 . 3 0 8 1 : v i x r a jamesgray@vt.edu varzi61@vt.edu contents "
1803.08177,"—the growing popularity of virtual and augmented reality communications and 360◦ video streaming is moving video communication systems into much more dynamic and resource-limited operating settings. the enormous data volume of 360◦ videos requires an efﬁcient use of network bandwidth to maintain the desired quality of experience for the end user. to this end, we propose a framework for viewport-driven rate-distortion optimized 360◦ video streaming that integrates the user view navigation pattern and the spatiotemporal rate-distortion charac- teristics of the 360◦ video content to maximize the delivered user quality of experience for the given network/system resources. the framework comprises a methodology for constructing dynamic heat maps that capture the likelihood of navigating different spatial segments of a 360◦ video over time by the user, an analysis and characterization of its spatiotemporal rate-distortion characteristics that leverage preprocessed spatial tilling of the 360◦ view sphere, and an optimization problem formulation that characterizes the delivered user quality of experience given the user navigation patterns, 360◦ video encoding decisions, and the available system/network resources. our experimental results demonstrate the advantages of our framework over the conventional approach of streaming a monolithic uniformly- encoded 360◦ video and a state-of-the-art reference method. considerable video quality gains of 4 - 5 db are demonstrated in the case of two popular 4k 360◦ videos. "
1803.08178," there has recently been a steadily increase in the iterative ap- proaches to boosted density estimation and sampling, usually pro- ceeding by adding candidate “iterate” densities to a model that gets more accurate with iterations. the relative accompanying burst of formal convergence results has not yet changed a striking picture: all results essentially pay the price of heavy assumptions on iterates, often unrealistic or hard to check, and oﬀer a blatant contrast with the original boosting theory where such assumptions would be the weakest possible. in this paper, we show that all that suﬃces to achieve boost- ing for density estimation is a weak learner in the original boosting theory sense, that is, an oracle that supplies classiﬁers. we provide converge rates that comply with boosting requirements, being better and / or relying on substantially weaker assumptions than the state of the art. one of our rates is to our knowledge the ﬁrst to rely on not just weak but also empirically testable assumptions. we show that the model ﬁt belongs to exponential families, and obtain in the course of our results a variational characterization of f -divergences better than f -gan’s. experimental results on several simulated problems display signiﬁcantly better results than adagan during early boosting rounds, in particular for mode capture, and using architectures less than the ﬁfth’s of adagan’s size. ∗data61 & the australian national university †data61, the australian national university & the university of sydney 1 1 "
1803.08181,"— 3d lidars and 2d cameras are increasingly being used alongside each other in sensor rigs for perception tasks. before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. a vast majority of existing calibration techniques require signiﬁcant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. we address this gap with calibnet: a self-supervised deep network capable of automatically estimating the 6-dof rigid body transformation between a 3d lidar and a 2d camera in real-time. calibnet alleviates the need for calibration targets, thereby resulting in signiﬁcant savings in calibration efforts. during training, the network only takes as input a lidar point cloud, the corresponding monocular image, and the camera calibration matrix k. at train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration ganesh iyer, karnik ram r., and k. madhava krishna are with the robotics research center at the international institute of information tech- nology, hyderabad, india. j. krishna murthy is with the montreal institute for learning algorithms, universit´e de montr´eal, quebec, canada. authors’ email: giyer2309@gmail.com, karnikram@gmail.com, mkrishna@iiit.ac.in parameters, for example). instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. calibnet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration param- eters for a wide range of mis-calibrations, without requiring retraining or domain adaptation. the project page is hosted at https://epiception.github.io/calibnet "
1803.08182," generative adversarial networks (gans) are becoming popular choices for unsu- pervised learning. at the same time there is a concerted eﬀort in the machine learning community to expand the range of tasks in which learning can be applied as well as to utilize methods from other disciplines to accelerate learning. with this in mind, in the current work we suggest ways to enforce given constraints in the output of a gan both for interpolation and extrapolation. the two cases need to be treated diﬀerently. for the case of interpolation, the incorporation of constraints is built into the train- ing of the gan. the incorporation of the constraints respects the primary game- theoretic setup of a gan so it can be combined with existing algorithms. however, it can exacerbate the problem of instability during training that is well-known for gans. we suggest adding small noise to the constraints as a simple remedy that has performed well in our numerical experiments. the case of extrapolation (prediction) is more involved. first, we employ a modiﬁed interpolation training process that uses noisy data but does not necessarily enforce the constraints during training. second, the resulting modiﬁed interpolator is used for extrapolation where the constraints are enforced after each step through projection on the space of constraints. 1 "
1803.08184,"—one of the greatest challenges in applying compres- sive sensing (cs) signal processing techniques to electromagnetic imaging applications is designing a sensing matrix that has good reconstruction capabilities. compressive reﬂector antennas (cra) are a class of antennas that have been shown to provide enhanced image reconstruction performance over traditional reﬂector antennas (tra) when cs techniques are employed. in this paper, we present a uniﬁed cra design method, which considers both the sensing capacity and efﬁciency of the antenna, and can be used for both compressive imaging and multiple- input multiple-output (mimo) communication applications. the uniﬁed design method is assessed for a cra conﬁguration in which dielectric scatterers are added to the surface of a tra. the design results demonstrate the ability of the uniﬁed design method to enhance the cs reconstruction capabilities of the cra. index terms—compressive sensing, antenna design, coded apertures "
1803.08185," the contribution of line-of-sight peculiar velocities to the observed redshift of objects breaks the translational symmetry of the underlying theory, modifying the predicted 2-point functions. these ‘wide angle eﬀects’ have mostly been studied using linear perturbation theory in the context of the multipoles of the correlation function and power spectrum . in this work we present the ﬁrst calculation of wide angle terms in the zeldovich approximation, which is known to be more accurate than linear theory on scales probed by the next generation of galaxy surveys. we present the exact result for dark matter and perturbatively biased tracers as well as the small angle expansion of the conﬁguration- and fourier-space two-point functions and the connection to the multi-frequency angular power spectrum. we compare diﬀerent deﬁnitions of the line- of-sight direction and discuss how to translate between them. we show that wide angle terms can reach tens of percent of the total signal in a measurement at low redshift in some approximations, and that a generic feature of wide angle eﬀects is to slightly shift the baryon acoustic oscillation scale. "
1803.08186,"—it is well established in the compressive sensing (cs) literature that sensing matrices whose elements are drawn from independent random distributions exhibit enhanced recon- struction capabilities. in many cs applications, such as elec- tromagnetic imaging, practical limitations on the measurement system prevent one from generating sensing matrices in this fashion. although one can usually randomized the measurements to some degree, these sensing matrices do not achieve the same reconstruction performance as the truly randomized sensing matrices. in this paper, we present a novel method, based upon capacity maximization, for designing sensing matrices with enhanced block-sparse signal reconstruction capabilities. through several numerical examples, we demonstrate how our method signiﬁcantly enhances reconstruction performance. index terms—compressive sensing, block compressive sensing, sensing matrix design, nonconvex optimization "
1803.08188,"—the fact that millimeter wave (mmwave) com- is usually perceived as a munication needs to be directional challenge; it enables efﬁcient secret key sharing that are unconditionally secure from passive eavesdroppers, by building on packet erasures. we showcase the potential of our approach in two setups: beam sweeping to align receiver/transmitter pairs, and vehicle platooning. we show that in the ﬁrst case, we can establish a few hundred secret bits at virtually no additional transmission cost; and in the second case, we can potentially establish keys in the order of tenths of mbps. these ﬁrst results are based on some simplifying assumptions, yet we believe they give incentives to further explore such techniques. "
1803.08189,"—in an internet-of-things system where status data are collected from sensors and actuators for time-critical appli- cations, the freshness of data is vital and can be quantiﬁed by the recently proposed age-of-information (aoi) metric. in this paper, we ﬁrst consider a general scenario where multiple terminals share a common channel to transmit or receive randomly gener- ated status packets. the optimal scheduling problem to minimize aoi is formulated as a restless multi-armed bandit problem. to solve the problem efﬁciently, we derive the whittle’s index in closed-form and establish the indexability thereof. compared with existing work, we extend the index policy for aoi opti- mization to incorporate stochastic packet arrivals and optimal packet management (buffering the latest packet). inspired by the index policy which has near-optimal performance but is centralized by nature, a decentralized status update scheme, i.e., the index-prioritized random access policy (ipra), is further proposed, achieving universally near-optimal aoi performance and outperforming state-of-the-arts in the literature. index terms—internet-of-things, age-of-information, markov decision process, restless multi-armed bandit, whittle’s index, random access "
1803.08190,". in this paper, we address the problem of estimating a 3d human pose from a single image, which is important but difﬁcult to solve due to many rea- sons, such as self-occlusions, wild appearance changes, and inherent ambiguities of 3d estimation from a 2d cue. these difﬁculties make the problem ill-posed, which have become requiring increasingly complex estimators to enhance the performance. on the other hand, most existing methods try to handle this prob- lem based on a single complex estimator, which might not be good solutions. in this paper, to resolve this issue, we propose a multiple-partial-hypothesis-based framework for the problem of estimating 3d human pose from a single image, which can be ﬁne-tuned in an end-to-end fashion. we ﬁrst select several joint groups from a human joint model using the proposed sampling scheme, and esti- mate the 3d poses of each joint group separately based on deep neural networks. after that, they are aggregated to obtain the ﬁnal 3d poses using the proposed robust optimization formula. the overall procedure can be ﬁne-tuned in an end- to-end fashion, resulting in better performance. in the experiments, the proposed framework shows the state-of-the-art performances on popular benchmark data sets, namely human3.6m and humaneva, which demonstrate the effectiveness of the proposed framework. keywords: 3d human pose estimation, multiple-partial-hypothesis-based model, articulated pose estimation "
1803.08192,". we study the relationship between singularity categories and relative singularity categories and discuss constructions of diﬀerential graded algebras of relative singularity categories. as consequences, we obtain structural results, which are known or generalise known results, on singularity categories of algebras with radical square zero, of non-commutative deformations of kleinian singularities, of sl3(c)-quotient singularities and of gorenstein toric threefolds. keywords: relative singularity category, singularity category, dg algebra, cluster category, quotient singularity msc 2010: 14b05, 16e45, 18e30 8 1 0 2   r a   m 2 2     ] . g a h t a m [     1 v 2 9 1 8 0 . 3 0 8 1 : v i x r a contents "
1803.08193,". this paper proposes new semantics for nondeterministic pro- gram execution, replacing the standard relational semantics for propo- sitional dynamic logic (pdl). under these new semantics, program ex- ecution is represented as fundamentally deterministic (i.e., functional), while nondeterminism emerges as an epistemic relationship between the agent and the system: intuitively, the nondeterministic outcomes of a given process are precisely those that cannot be ruled out in advance. we formalize these notions using topology and the framework of dynamic topological logic (dtl) [1]. we show that dtl can be used to interpret the language of pdl in a manner that captures the intuition above, and moreover that continuous functions in this setting correspond exactly to deterministic processes. we also prove that certain axiomatizations of pdl remain sound and complete with respect to the corresponding classes of dynamic topological models. finally, we extend the framework to incorporate knowledge using the machinery of subset space logic [2], and show that the topological interpretation of public announcements as given in [3] coincides exactly with a natural interpretation of test programs. 1 "
1803.08196," the relationship between dust polarization and extinction was determined for the cold dense starless molecular cloud core fest 1-457 based on the background star polarimetry of dichroic extinction at near-infrared wavelengths. owing to the known (three-dimensional) magnetic ﬁeld structure, the observed polarizations from the core were corrected by considering (a) the subtraction of the ambient po- larization component, (b) the depolarization eﬀect of inclined distorted magnetic ﬁelds, and (c) the magnetic inclination angle of the core. after these corrections, a linear relationship between polarization and extinction was obtained for the core in the range up to av ≈ 20 mag. the initial polarization vs. extinction diagram changed dramatically after the corrections of (a) to (c), with the correlation coef- ﬁcient being reﬁned from 0.71 to 0.79. these corrections should aﬀect the theo- retical interpretation of the observational data. the slope of the ﬁnally obtained polarization–extinction relationship is ph /eh−ks = 11.00 ± 0.72 % mag−1, which is close to the statistically estimated upper limit of the interstellar polarization eﬃ- ciency (jones 1989). this consistency suggests that the upper limit of interstellar polarization eﬃciency might be determined by the observational viewing angle to- ward polarized astronomical objects. 1 "
1803.08198," we propose and analyze a new stochastic gradient method, which we call stochastic unbiased curvature-aided gradient (sucag), for ﬁnite sum optimization problems. sucag constitutes an unbiased total gradient tracking technique that uses hessian information to accelerate con- vergence. we analyze our method under the general asynchronous model of computation, in which functions are selected inﬁnitely often, but with delays that can grow sublinearly. for strongly convex problems, we establish linear convergence for the sucag method. when the initialization point is suﬃciently close to the optimal solution, the established convergence rate is only dependent on the condition number of the problem, making it strictly faster than the known rate for the saga method. furthermore, we describe a markov-driven approach of implementing the sucag method in a distributed asynchronous multi-agent setting, via gossiping along a random walk on the communication graph. we show that our analysis applies as long as the undirected graph is connected and, notably, establishes an asymptotic linear convergence rate that is robust to the graph topology. numerical results demonstrate the merit of our algorithm over existing methods. 1 "
1803.08199," we detail tentative detections of low-frequency carbon radio recombination lines from within the orion molecular cloud complex observed at 99–129 mhz. these tentative detections include one alpha transition and one beta transition over three locations and are located within the diﬀuse regions of dust observed in the infrared at 100 µm, the hα emission detected in the optical, and the synchrotron radiation observed in the radio. with these observations, we are able to study the radiation mechanism transition from collisionally pumped to radiatively pumped within the hii regions within the orion molecular cloud complex. keywords: astrochemistry – atomic data – atomic processes – ism: hii regions – ism: atoms "
1803.08200," we consider coordinate descent (cd) methods with exact line search on convex quadratic problems. our main focus is to study the performance of the cd method that use random permutations in each epoch and compare it to the performance of the cd methods that use deterministic orders and random sampling with replacement. we focus on a class of convex quadratic problems with a diagonally dominant hessian matrix, for which we show that using random permutations instead of random with-replacement sampling improves the performance of the cd method in the worst-case. furthermore, we prove that as the hessian matrix becomes more diagonally dominant, the performance improvement attained by using random permuta- tions increases. we also show that for this problem class, using any ﬁxed deterministic order yields a superior performance than using random permutations. we present detailed theoretical analyses with respect to three diﬀerent convergence criteria that are used in the literature and support our theoretical results with numerical experiments. 1 "
1803.08201,". the suslin hypothesis states that there are no nonseparable complete dense linear orderings without endpoints which have the countable chain condition. zf + ad+ + v = l(p(r)) proves the suslin hypothesis. in particular, if l(r) |= ad, then l(r) satisﬁes the suslin hypothesis, which answers a question of foreman. "
1803.08202," a wide range of human-robot collaborative applications in industry, search and rescue operations, healthcare, and social interactions require an autonomous robot to follow its human companion. different operating mediums and applications pose diverse challenges by adding constraints on the choice of sensors, the degree of autonomy, and dynamics of the person following robot. researchers have addressed these challenges in many ways and contributed to the development of a large body of literature. this paper provides a comprehensive overview of the literature by categorizing different aspects of person-following by autonomous robots. also, the corresponding operational challenges are identiﬁed based on various design choices for ground, underwater, and aerial scenarios. in addition, state-of-the-art methods for perception, planning, control, and interaction are elaborately discussed, and their feasibilities are evaluated in terms of standard operational and performance metrics. furthermore, several prospective application areas are identiﬁed, and open problems are highlighted for future research. keywords person following robot, human-robot interaction "
1803.08203," while training error of most deep neural networks degrades as the depth of the network increases, residual networks appear to be an exception. we show that the main reason for this is the lyapunov stability of the gradient descent algorithm: for an arbitrarily chosen step size, the equilibria of the gradient descent are most likely to remain stable for the parametrization of residual networks. we then present an architecture with a pair of residual networks to approximate a large class of functions by decomposing them into a convex and a concave part. some parameters of this model are shown to change little during training, and this imper- fect optimization prevents overﬁtting the data and leads to solutions with small lipschitz constants, while providing clues about the generalization of other deep networks. "
1803.08204," we investigate, in the probe limit, the negative refraction in the generalized superconductors with the born-infeld electrodynamics. we observe that the system has a negative depine-lakhtakia index in the superconducting phase at small frequencies and the greater the born-infeld corrections the larger the range of frequencies or the range of temperatures for which the negative refraction occurs. furthermore, we ﬁnd that the tunable born-infeld parameter can be used to improve the propagation of light in the holographic setup. our analysis indicates that the born-infeld electrodynamics plays an important role in determining the optical properties of the boundary theory. pacs numbers: 11.25.tq, 04.70.bw, 74.20.-z 8 1 0 2   r a   m 2 2     ] h t - p e h [     1 v 4 0 2 8 0 . 3 0 8 1 : v i x r a ∗ panqiyuan@126.com † corresponding author at hwyu@hunnu.edu.cn ‡ jljing@hunnu.edu.cn 2 i. "
1803.08208," recent years have witnessed many exciting achievements for object detection using deep learning techniques. de- spite achieving signiﬁcant progresses, most existing de- tectors are designed to detect objects with relatively low- quality prediction of locations, i.e., often trained with the threshold of intersection over union (iou) set to 0.5 by de- fault, which can yield low-quality or even noisy detections. it remains an open challenge for how to devise and train a high-quality detector that can achieve more precise lo- calization (i.e., iou>0.5) without sacriﬁcing the detection performance. in this paper, we propose a novel single- shot detection framework of bidirectional pyramid net- works (bpn) towards high-quality object detection, which consists of two novel components: (i) a bidirectional fea- ture pyramid structure for more effective and robust fea- ture representations; and (ii) a cascade anchor reﬁnement to gradually reﬁne the quality of predesigned anchors for more effective training. our experiments showed that the proposed bpn achieves the best performances among all the single-stage object detectors on both pascal voc and ms coco datasets, especially for high-quality detections. "
1803.08209,"— motivated by a high demand for automated in- spection of civil infrastructure, this paper presents a novel de- sign and development of a tank-like robot for structural health monitoring. unlike most existing magnetic wheeled mobile robot designs, which is only suitable for climbing on ﬂat steel surface, our proposed tank-like robot design uses reciprocating mechanism and roller-chains to make it capable of climbing on different structural shapes (e.g., cylinder, cube) with coated or non-coated steel surfaces. the robot can transition from one surface to the other (e.g., from ﬂat surface to curving surface). taking into account of several strict considerations (including tight dimension, efﬁcient adhesion and climbing ﬂexibility) to adapt with variety shapes of steel structures, a prototype tank- like robot incorporating multiple sensors (hall effects, sonars, inertial measurement unit and camera), has been developed. rigorous analysis of robot kinematics, adhesion force, sliding failure and turn-over failure has been conducted to demonstrate the stability of the proposed design. experimental results and ﬁeld deployments conﬁrm the adhesion and climbing capability of the developed robot. "
1803.08210," new expressions are given for the fourier expansions of non-holomorphic eisenstein series with weight k. among other applications, this leads to non-holomorphic analogs of formulas of ramanujan, grosswald and berndt containing eichler integrals of holomorphic eisenstein series. "
1803.08212," using a lattice model of polymers in a tube, we deﬁne one way to characterise different conﬁgurations of a given knot as either “local” or “non-local” and, for several ring polymer models, we provide both theoretical and numerical evidence that, at equilibrium, the non- local conﬁgurations are more likely than the local ones. these characterisations are based on a standard approach for measuring the “size” of a knot within a knotted polymer chain. the method involves associating knot-types to subarcs of the chain, and then identifying a knotted subarc with minimal arclength; this arclength is then the knot-size. if the resulting knot-size is small relative to the whole length of the chain, then the knot is considered to be localised or “local”. if on the other hand the knot-size is comparable to the length of the chain, then the knot is considered to be “non-local”. using this deﬁnition, we establish that all but exponentially few sufﬁciently long self- avoiding polygons (closed chains) in a tubular sublattice of the simple cubic lattice are “non-locally” knotted. this is shown to also hold for the case when the same polygons are subject to an external tensile force, as well as in the extreme case when they are as compact as possible (no empty lattice sites). we also provide numerical evidence for small tube sizes that at equilibrium non-local knotting is more likely than local knotting, regardless of the strength of the stretching or compressing force. we note however that because of the tube conﬁnement, the occurrence of non-local knotting in walks (open chains) is signiﬁcantly different than for polygons. the relevance of these results to recent experiments involving dna knots in solid-state nanopores is also discussed. "
1803.08214," inspired by some recent works of tippett-tsang and mallary-khanna-price, we present a new spacetime model containing closed timelike curves (ctcs). this model is obtained postulating an ad hoc lorentzian metric on r4, which diﬀers from the minkowski metric only inside a spacetime region bounded by two concentric tori. the resulting spacetime is topologically trivial, free of curvature singularities and is both time and space orientable; besides, the inner region enclosed by the smaller torus is ﬂat and displays geodesic ctcs. our model shares some similarities with the time machine of ori and soen but it has the advantage of a higher symmetry in the metric, allowing for the explicit computation of a class of geodesics. the most remarkable feature emerging from this computation is the presence of future-oriented timelike geodesics starting from a point in the outer minkowskian region, moving to the inner spacetime region with ctcs, and then returning to the initial spatial position at an earlier time; this means that time travel to the past can be performed by free fall across our time machine. the amount of time travelled into the past is determined quantitatively; this amount can be made arbitrarily large keeping non-large the proper duration of the travel. an important drawback of the model is the violation of the classical energy conditions, a common feature of most time machines. other problems emerge from our computations of the required (negative) energy densities and of the tidal accelerations; these are small only if the time machine is gigantic. keywords: general relativity, closed timelike curves, time machines, energy conditions. ams subject classiﬁcations: 83c05, 83c20 . pacs: 04.20.cv, 04.20.gz, 04.90.+e . 8 1 0 2   r a   m 8 2     ] c q - r g [     2 v 4 1 2 8 0 . 3 0 8 1 : v i x r a 1 "
1803.08215," we present a drag force model for evolving chaotic few-body interactions with the inclusion of orbital energy losses, such as tidal dissipation and gravitational wave (gw) emission. the main eﬀect from such losses is the formation of two-body captures, that for compact objects result in gw mergers, and for stars lead to either compact binaries, mergers or disruptions. studying the inclusion of energy loss terms in few-body interactions is therefore likely to be important for modeling and understanding the variety of transients that soon will be observed by current and upcoming surveys. however, including especially tides in few-body codes has been shown to be technically diﬃcult and computationally heavy, which has lead to very few systematic tidal studies. in this paper we derive a drag force term that can be used to model the eﬀects from tidal, as well as other, energy losses in few-body interactions, if the two-body orbit averaged energy loss is known a priori. this drag force model is very fast to evolve, and gives results in agreement with other approaches, including the impulsive and aﬃne tide approximations. key words: gravitation – methods: numerical – stars: black holes – stars: kinematics and dynamics "
1803.08216,". for a smooth projective variety x, we consider when the diagonal ∆x is nef as a cycle on x × x. in particular, we give a classiﬁcation of complete intersections and smooth del pezzo varieties where the diagonal is nef. we also study the nefness of the diagonal for spherical varieties. "
1803.08221,"—secure message dissemination is an important issue in vehicular networks, especially considering the vulnerability of vehicle to vehicle (v2v) message dissemination to malicious attacks. traditional security mechanisms, largely based on mes- sage encryption and key management, can only guarantee secure message exchanges between known source and destination pairs. in vehicular networks however, every vehicle may learn its surrounding environment and contributes as a source, while in the meantime act as a destination or a relay of information from other vehicles, message exchanges often occur between “stranger” vehicles. this makes secure message dissemination against malicious tampering much more intricate. for secure message dissemination in vehicular networks against insider attackers, who may tamper the content of the disseminated messages, ensuring the consistency and integrity of the transmit- ted messages becomes a major concern that traditional message encryption and key management based approaches fall short to provide. however, it is challenging for a vehicle to distinguish which message is true when its received messages from multiple nearby vehicles are conﬂicting. in this paper, by incorporating the underlying network topology information, we propose an optimal decision algorithm that is able to maximize the chance of making a correct decision on the message content, assuming the prior knowledge of the percentage of malicious vehicles in the network. furthermore, a novel heuristic decision algorithm is proposed that can make decisions without the aforementioned knowledge of the percentage of malicious vehicles. simulations are conducted to compare the security performance achieved by our proposed decision algorithms with that achieved by existing ones that do not consider or only partially consider the topological information, to verify the effectiveness of the algorithms. our results show that by incorporating the network topology information, the security performance can be much improved. this work shed light on the optimum algorithm design for secure message dissemination. index terms—vehicular networks, security, message dissemi- nation, decision algorithm. "
1803.08223," we investigate theoretically the goos-h¨anchen (gh) shift of a p-polarized terahertz beam incident on a 2d material surface with complex conductiv- ity. taking monolayer graphene to be the model material, we determine the dependence of gh shifts on the fermi level and incident frequency. both spatial and angular gh shifts are present. for both gh shifts in general, we ﬁnd that increasing the fermi level shifts the incident angle at which the maximum gh shifts arise. moreover, we see that at higher frequencies, the amount of beam shift decreases with the fermi level when the incident frequency is changed. at lower frequencies, however, the shift becomes pro- portional with the fermi level. upon obtaining the measurable shifts, the angular gh shift dominates the spatial gh shift given appropriate experi- mental parameters. our results may pave the way for these material’s use in optoelectronics devices, and fundamentally, to determine properties of 2d materials with complex conductivity. keywords: optics at surfaces, physical optics, nanomaterials, materials and process characterization "
1803.08224,". we study a new construction of bodies from a given convex body in rn which are isomorphic to (weighted) ﬂoating bodies. we establish several properties of this new construction, including its relation to p-aﬃne surface areas. we show that these bodies are related to ulam’s long-standing ﬂoating body problem which asks whether euclidean balls are the only bodies that can ﬂoat, without turning, in any orientation. "
1803.08225,". we present a box-free bottom-up approach for the tasks of pose estimation and instance segmentation of people in multi-person im- ages using an eﬃcient single-shot model. the proposed personlab model tackles both semantic-level reasoning and object-part associations using part-based modeling. our model employs a convolutional network which learns to detect individual keypoints and predict their relative displace- ments, allowing us to group keypoints into person pose instances. fur- ther, we propose a part-induced geometric embedding descriptor which allows us to associate semantic person pixels with their corresponding person instance, delivering instance-level person segmentations. our sys- tem is based on a fully-convolutional architecture and allows for eﬃcient inference, with runtime essentially independent of the number of people present in the scene. trained on coco data alone, our system achieves coco test-dev keypoint average precision of 0.665 using single-scale inference and 0.687 using multi-scale inference, signiﬁcantly outperform- ing all previous bottom-up pose estimation systems. we are also the ﬁrst bottom-up method to report competitive results for the person class in the coco instance segmentation task, achieving a person category av- erage precision of 0.417. keywords: person detection and pose estimation, segmentation and grouping. "
1803.08228,"—future terabit networks are committed to dra- matically improving big data motion between geographically dispersed hpc data centers. the scientiﬁc community takes advantage of the terabit networks such as doe’s esnet and accelerates the trend to build a small world of collaboration between geospatial hpc data centers. it improves information and resource sharing for joint simulation and analysis between the hpc data centers. in this paper, we propose to build scispace (scientiﬁc collaboration workspace) for collaborative data centers. it provides a global view of information shared from multiple geo-distributed hpc data centers under a single workspace. scispace supports native data-access to gain high- performance when data read or write is required in native data center namespace. it is accomplished by integrating a metadata export protocol. to optimize scientiﬁc collaborations across hpc data centers, scispace implements search and discovery service. to evaluate, we conﬁgured two geo-distributed small-scale hpc data centers connected via high-speed inﬁniband network, equipped with lustrefs. we show the feasibility of scispace using real scientiﬁc datasets and applications. the evaluation results show average 36% performance boost when the proposed native-data access is employed in collaborations. "
1803.08229," in this paper, an algorithm based on polyphase matrix for constructing a pair of orthogonal wavelet frames is suggested, and a general form for all orthogonal tight wavelet frames on local ﬁelds of positive characteristic is described. moreover, we investigate their properties by means of the fourier transform. keywords wavelet frame; orthogonality; framelet symbol; polyphase matrix; extension principle; fourier transform; local ﬁeld mathematics subject classiﬁcation (2000) primary 42c40; secondary 42c15. 43a70. 11s85 "
1803.08230, we study the conditions for the adiabatic resonant conversion of the cold dark matter (cdm) axions into photons in existence of the astrophysically sourced strong magnetic ﬁelds such as those in the neutron star magnetosphere. we demonstrate the possibility that the forthcoming radio telescopes such as the ska (square kilometre array) can probe those photon signals from the cdm axions. 1 
1803.08234," we present the discovery of new star clusters in the central plane region (|l| < 30◦ and |b| < 6◦) of the milky way. in order to overcome the extinction problem and the spatial limit of previous surveys, we use the wide-ﬁeld infrared survey explorer (wise) data to ﬁnd clusters. we also use other infrared survey data in the archive for additional analysis. we ﬁnd 923 new clusters, of which 202 clusters are embedded clusters. these clusters are concentrated toward the galactic plane and show a symmetric distribution with respect to the galactic latitude. the embedded clusters show a stronger concentration to the galactic plane than the non-embedded clusters. the new clusters are found more in the ﬁrst galactic quadrant, while previously known clusters are found more in the fourth galactic quadrant. the spatial distribution of the combined sample of known clusters and new clusters is approximately symmetric with respect to the galactic longitude. we estimate reddenings, distances, and relative ages of the 15 class a clusters using theoretical isochrones. ten of them are relatively old (age > 800 myr) and ﬁve are young (age ≈ 4 myr). keywords: catalogs — galaxy: disk — (galaxy:) open clusters and associations: general — galaxy: structure — infrared: stars 8 1 0 2   r a   m 2 2     ] . a g h p - o r t s a [     1 v 4 3 2 8 0 . 3 0 8 1 : v i x r a corresponding author: myung gyoon lee mglee@astro.snu.ac.kr ryujh@astro.snu.ac.kr 2 ryu & lee "
1803.08235," in our previous works, we have analyzed the evolution of bulk viscous matter dominated ¨a ˙a and also universe with a more general form for bulk viscous coeﬃcient, ζ = ζ0 + ζ1 carried out the dynamical system analysis. we found that the model reasonably describes the evolution of the universe if the viscous coeﬃcient is a constant. in the present work we are contrasting this model with the standard λcdm model of the universe using the bayesian method. we have shown that, even though the viscous model gives a reasonable back ground evolution of the universe, the bayes factor of the model indicates that, it is not so superior over the λcdm model, but have a slight advantage over it. ˙a a + ζ2 1 "
1803.08236,. we propose a uniﬁed approach to the study of isometries on algebras of vector- valued lipschitz maps and those of continuously diﬀerentiable maps by means of the notion of natural c(y )-valuezations that take values in unital commutative c ∗-algebras. a precise proof of a theorem of jarosz [17] is exhibited. 
1803.08237," the recent advancement of high-intensity lasers has made all-optical compton scattering become a promising way to produce ultra-short brilliant γ-rays in an ultra-compact system. however, so far achieved compton γ-ray sources are severely limited by low conversion efﬁciency (lower than 10−5) and spectral intensity (∼ 104 photons/0.1%bw). here we present a highly efﬁcient gamma photon emitter obtained by irradiating a high-intensity laser pulse on a miniature plasma device consisting of a plasma lens and a plasma mirror. this concept exploits strong spatiotemporal laser-shaping process and high-charge electron acceleration process in the plasma lens, as well as an efﬁcient nonlinear compton scattering process enabled by the plasma mirror. our particle-in-cell simulations demonstrate that in this novel scheme, brilliant γ-rays with very high conversion efﬁciency (higher than 10−2) and spectral intensity (∼ 109 photons/0.1%bw) can be achieved by employing currently available petawatt-class lasers with intensity of 1021 w/cm2. such efﬁcient and intense γ-ray sources would ﬁnd applications in wide-ranging areas. "
1803.08238,". we show that, for a closed orientable n-manifold, with n not congruent to 3 modulo 4, the existence of a cr-regular embedding into complex (n − 1)-space ensures the existence of a totally real embedding into complex n-space. this implies that a closed orientable (4k + 1)-manifold with non-vanishing kervaire semi-characteristic possesses no cr-regular embedding into complex 4k-space. we also pay special attention to the cases of cr-regular embeddings of spheres and of simply-connected 5-manifolds. "
1803.08239," we have studied the scenario of baryogenesis via leptogenesis in an a4 ﬂavor symmetric framework considering type i seesaw as the origin of neutrino mass. because of the presence of the ﬁfth generation right handed neutrino the model naturally generates non-zero reactor mixing angle. we have considered two vev alignments for the extra ﬂavon η and studied the consequences in detail. as a whole the additional ﬂavon along with the extra right handed neutrinos allow us to study thermal leptogenesis by the decay of the lightest right handed neutrino present in the model. we have computed the matter-antimatter asymmetry for both ﬂavor dependent and ﬂavor independent leptogenesis by considering a considerably wider range of right handed neutrino mass. finally, we correlate the baryon asymmetry of the universe (bau) with the model parameters and light neutrino masses. pacs numbers: 14.60.pq, 11.30.qc 8 1 0 2   r a   m 2 2     ] h p - p e h [     1 v 9 3 2 8 0 . 3 0 8 1 : v i x r a ∗electronic address: ananyam@tezu.ernet.in †electronic address: mkdas@tezu.ernet.in ‡electronic address: jks@tezu.ernet.in 1 i. "
1803.08240," many of the leading approaches in language mod- eling introduce novel, complex and specialized architectures. we take existing state-of-the-art word level language models based on lstms and qrnns and extend them to both larger vocabular- ies as well as character-level granularity. when properly tuned, lstms and qrnns achieve state- of-the-art results on character-level (penn tree- bank, enwik8) and word-level (wikitext-103) datasets, respectively. results are obtained in only 12 hours (wikitext-103) to 2 days (enwik8) us- ing a single modern gpu. "
1803.08243,"—speech derverberation using a single microphone is addressed in this paper. motivated by the recent success of the fully convolutional networks (fcn) in many image process- ing applications, we investigate their applicability to enhance the speech signal represented by short-time fourier transform (stft) images. we present two variations: a “u-net” which is an encoder-decoder network with skip connections and a generative adversarial network (gan) with u-net as generator, which yields a more intuitive cost function for training. to evaluate our method we used the data from the reverb challenge, and compared our results to other methods under the same conditions. we have found that our method outperforms the competing methods in most cases. "
1803.08244,". the task of three-dimensional (3d) human pose estimation from a single image can be divided into two parts: (1) two-dimensional (2d) human joint detection from the image and (2) estimating a 3d pose from the 2d joints. herein, we focus on the second part, i.e., a 3d pose estimation from 2d joint locations. the problem with existing methods is that they require either (1) a 3d pose dataset or (2) 2d joint locations in consecutive frames taken from a video sequence. we aim to solve these problems. for the ﬁrst time, we propose a method that learns a 3d human pose without any 3d datasets. our method can predict a 3d pose from 2d joint locations in a single image. our system is based on the generative adversarial networks, and the networks are trained in an unsupervised manner. our primary idea is that, if the network can predict a 3d human pose correctly, the 3d pose that is projected onto a 2d plane should not collapse even if it is rotated perpendicularly. we evaluated the performance of our method using human3.6m and the mpii dataset and showed that our network can predict a 3d pose well even if the 3d dataset is not available during training. keywords: 3d human pose estimation, unsupervised learning "
1803.08246," based on the exact analytical time-varying solution of a conducting ring embedded in a textured static/dynamic electric ﬁeld, we investigate the time-resolved quantum spin transport in the struc- ture. it is shown that the interference patterns are governed by not only the aharonov-casher phase but also the instantaneous phase diﬀerence of spin precession. this spin precessing phase has strong eﬀect on the spin conductance and results in a high-frequency oscillation mode with respect to the strength of electric ﬁeld, especially in the weak value cases. our studies suggest that a quasi-static moderate ﬁeld is favor to modulate the spin polarization orientation of incident electron. 8 1 0 2   r a   m 3 2     ] l l a h - s e m . t a m - d n o c [     2 v 6 4 2 8 0 . 3 0 8 1 : v i x r a 1 "
1803.08249,". in this paper, we study the mixed dispersion fourth order nonlinear helmholtz equation ∆2u − β∆u + αu = γ|u|p−2u in rn , for positive, bounded and zn -periodic functions γ in the following three cases: (a) α < 0, β ∈ r or (b) α > 0, β < −2√α or (c) α = 0, β < 0. using the dual method of evéquoz and weth, we ﬁnd solutions to this equation and establish some of their qualitative properties. "
1803.08250,"—millimeter wave (mmwave) technology is expected to dominate the future 5g networks mainly due to large spectrum available at these frequencies. however, coverage deteriorates signiﬁcantly at mmwave frequencies due to higher path loss, es- pecially for the non-line-of-sight (nlos) scenarios. in this work, we explore the use of passive reﬂectors for improving mmwave signal coverage in nlos indoor areas. measurements are carried out using the pxi-based mmwave transceiver platforms from national instruments operating at 28 ghz, and the results are compared with the outcomes of ray tracing (rt) simulations in a similar environment. for both the measurements and ray tracing simulations, different shapes of metallic passive reﬂectors are used to observe the coverage (signal strength) statistics on a receiver grid in an nlos area. for a square metallic sheet reﬂector of size 24 × 24 in2 and 33 × 33 in2, we observe a signiﬁcant increase in the received power in the nlos region, with a median gain of 20 db when compared to no reﬂector case. the cylindrical reﬂector shows more uniform coverage on the receiver grid as compared to ﬂat reﬂectors that are more directional. index terms—coverage, electromagnetic waves, mmwave, non-line-of-sight (nlos), pxi, ray tracing, reﬂector. "
1803.08251," with the wide adoption of the multi-community setting in many popular social media platforms, the increasing user en- gagements across multiple online communities warrant re- search attention. in this paper, we introduce a novel analogy between the movements in the cyber space and the physical space. this analogy implies a new way of studying human on- line activities by modelling the activities across online com- munities in a similar fashion as the movements among loca- tions. first, we quantitatively validate the analogy by com- paring several important properties of human online activi- ties and physical movements. our experiments reveal striking similarities between the cyber space and the physical space. next, inspired by the established methodology on human mobility in the physical space, we propose a framework to study human “mobility” across online platforms. we discover three interesting patterns of user engagements in online com- munities. furthermore, our experiments indicate that people with different mobility patterns also exhibit divergent prefer- ences to online communities. this work not only attempts to achieve a better understanding of human online activities, but also intends to open a promising research direction with rich implications and applications. "
1803.08252,"—unmanned aerial vehicles (uavs) are envisioned to be an integral part of future 5g communication systems. the agile nature of uavs for serving users at different locations can help to dynamically optimize coverage and quality-of-service (qos) in future networks. in this work, we explore the small scale temporal and spatial characteristics of mmwave air-to- ground (ag) line-of-sight (los) propagation channels at 28 ghz in different environmental scenarios: dense-urban, suburban, rural, and over sea using omni-directional antennas employing wireless insite ray tracing software. we classify the received multipath components (mpcs) into persistent and non-persistent components. the small scale temporal and spatial characteristics of the ag propagation channel are found to be dependent on the scatterer properties: number, distribution, and geometry. additionally, clustering of mpcs in the time and spatial domain for different environments is found to be dependent on the scatterer properties and receiver sensitivity. when the height of the uav is comparable to the height of the scatterers, we observe large temporal and angular spreads. index terms—5g communications, air-to-ground (ag), drone, line-of-sight (los) channel, mmwave communications, multipath components (mpcs), unmanned aerial vehicle (uav). sented in [12], while [13] provides a comprehensive survey on ag propagation channels for uavs. this paper is an extension of [5], where, time dispersion and large scale propagation characteristics at 28 ghz are studied for different environmental scenarios. here, small scale characteristics of the mmwave propagation channel in both time and spatial domains are analyzed (in contrast to [5] which considers only time domain characteristics) in dense- urban, suburban, rural and over sea scenarios using wireless insite ray tracing software. the analysis of the mmwave ag propagation channel reveals that the received multipath com- ponents (mpcs) can be grouped into persistent (comprising of los and ground reﬂected component (grc)) and non- persistent (comprising of all other non-los (nlos) compo- nents). the characteristics of persistent components are mainly dependent on the geometry of the setup and are predictable for a given trajectory of the uav. on the other hand, statistics of the non-persistent components follow a random process along the uav trajectory and are dependent on the properties of the terrain cover in addition to the geometry of the setup. "
1803.08254,". begin we study the wave equation in an interval with two linearly moving endpoints. we give the exact solution by a series formula, then we show that the energy of the solution decay at the rate 1/t. we also establish observability results, at one or two endpoints, in a sharp time. moreover, using the hilbert uniqueness method, we derive exact boundary controllability results. "
1803.08255," dropout represents a typical issue to be addressed when dealing with longitudinal studies. if the mechanism leading to missing information is non-ignorable, inference based on the observed data only may be severely biased. a frequent strategy to obtain reliable parameter estimates is based on the use of individual-speciﬁc random coeﬃ- cients that help capture sources of unobserved heterogeneity and, at the same time, deﬁne a reasonable structure of dependence between the longitudinal and the missing data process. we refer to elements in this class as random coeﬃcient based dropout models (rcbdms). we propose a dynamic, semi-parametric, version of the standard rcbdm to deal with discrete time to event. time-varying random coeﬃcients that evolve over time according to a non-homogeneous hidden markov chain are considered to model dependence between longitudinal responses recorded from the same subject. a separate set of random coeﬃcients is considered to model dependence between miss- ing data indicators. last, the joint distribution of the random coeﬃcients in the two equations helps describe the dependence between the two processes. to ensure model ﬂexibility and avoid unveriﬁable assumptions, we leave the joint distribution of the ran- dom coeﬃcients unspeciﬁed and estimate it via nonparametric maximum likelihood. the proposal is applied to data from the leiden 85+ study on the evolution of cognitive functioning in the elderly. keywords: dropout; finite mixture, latent markov model; missingness; nonparamet- 1 ric maximum likelihood. 1 "
1803.08256," many countries today have “country-centric mobile apps” which are mobile apps that are primarily used by residents of a specific country. many of these country-centric apps also include a location- based service which takes advantage of the smartphone’s api access to the smartphone’s current gps location. in this paper, we investi- gate how such country-centric apps with location-based services can be employed to study the diaspora associated with ethnic and cultural groups. our methodology combines gps hacking, auto- mated task tools for mobile phones, and ocr to generate migration statistics for diaspora. as a case study, we apply our methodology to wechat, an enor- mously popular app within china and among ethnic chinese world- wide. using wechat, we collect data about the chinese diaspora in 32 cities. we also collect data about chinese businesses in each of the cities using the google places api. the combined data provides interesting insights to the modern chinese diaspora and how it has changed in recent years. keywords chinese diaspora, mobile app, global migration "
1803.08257,". let p ⊂ a be an inclusion of σ-unital c*-algebras with a ﬁnite index in the sense of izumi. then we introduce the rokhlin property for a conditional expectation e from a onto p and show that if a is simple and satisﬁes any of the property (1) ∼ (12) listed in the below, and e has the rokhlin property, then so does p . (1) simplicity; (2) nuclearity; (3) c*-algebras that absorb a given strongly self-absorbing c*-algebra d; (4) c*-algebras of stable rank one; (5) c*-algebras of real rank zero; (6) c*-algebras of nuclear dimension at most n, where n ∈ z+; (7) c*-algebras of decomposition rank at most n, where n ∈ z+; (8) separable simple c*-algebras that are stably isomorphic to af algebras; (9) separable simple c*-algebras that are stably isomorphic to ai algebras; (10) separable simple c*-algebras that are stably isomorphic to at algebras; (11) separable simple c*-algebras that are stably isomorphic to sequential direct limits of one dimensional nccw complexes; (12) separable c*-algebras with strict comparison of positive elements. in particular, when α : g → aut(a) is an action of a ﬁnite group g on a with the rokhlin property in the sense of nawata, the properties (1) ∼ (12) are inherited to the ﬁxed point algebra aα and the crossed product algebra a ⋊α g from a. "
1803.08263," we report on the results from deep h i observations, performed with the karoo array telescope and with the karl g. jansky very large array of the loose galaxy group centred on the early-type galaxy ic 1459. the main result from our observations is the detection of a nearly continuous, 500-kpc long h i tail which crosses the entire group. earlier observations with the australia telescope compact array had shown the presence of a large h i tail in this galaxy group, but because of the much larger coverage of the new data, the full extent of this tail is now visible. the h i mass of this structure is 3.1 ± 0.3 × 109 m(cid:12). based on its morphology and kinematics, we conclude that the tail consists of gas stripped from ngc 7418 through tidal interactions, with ram-pressure aﬀects playing at most a minor role. optical images of the ic 1459 group do not show many indications that galaxy interactions are common in this group. the h i data reveal a very diﬀerent picture and show that almost all gas-rich galaxies in the ic 1459 group have a distorted h i distribution indicating that many interactions are occurring in this group. this high number of interactions shows that the processes that drive galaxy transformation are also occurring in fairly loose galaxy groups. key words. ism: evolution – galaxies: intergalactic medium – galaxies: interactions – galaxies: evolution "
1803.08264," purpose the data which is available to surgeons before, during and after surgery is steadily increasing in quantity as well as diversity. when plan- ning a patient’s treatment, this large amount of information can be diﬃcult to interpret. to aid in processing the information, new methods need to be found to present multi-modal patient data, ideally combining textual, imagery, temporal and 3d data in a holistic and context-aware system. methods we present an open-source framework which allows handling of pa- tient data in a virtual reality (vr) environment. by using vr technology, the workspace available to the surgeon is maximized and 3d patient data is ren- dered in stereo, which increases depth perception. the framework organizes the data into workspaces and contains tools which allow users to control, ma- nipulate and enhance the data. due to the framework’s modular design, it can easily be adapted and extended for various clinical applications. results the framework was evaluated by clinical personnel (77 participants). the majority of the group stated that a complex surgical situation is easier to comprehend by using the framework, and that it is very well suited for ed- ucation. furthermore, the application to various clinical scenarios - including the simulation of excitation-propagation in the human atrium - demonstrated the framework’s adaptability. as a feasibility study, the framework was used during the planning phase of the surgical removal of a large central carcinoma (cid:63) these authors contributed equally to this work. micha pfeiﬀer, stefanie speidel national center for tumor diseases, dresden, germany e-mail: micha.pfeiﬀer@nct-dresden.de matthias huber karlsruhe institute of technology, institute for anthropomatics and robotics hannes kenngott, anas preukschas, lisa bettscheider, beat m¨uller-stich heidelberg university hospital, department of general-, visceral- and transplant surgery 2 micha pfeiﬀer et al. fig. 1 left: traditionally, 2d slices of ct scans are used for planning. right: in complex scenarios such as this one, where the tumor (yellow) and the various vessel trees (veins, arteries and bile ducts) lie close together, a 3d visualization could greatly beneﬁt physicians in analyzing patient data (screenshot of the virtual reality application). from a patient’s liver. conclusion the clinical evaluation showed a large potential and high accep- tance for the vr environment in a medical context. the various applications conﬁrmed that the framework is easily extended and can be used in real-time simulation as well as for the manipulation of complex anatomical structures. keywords virtual reality · surgical planning · advanced medical visual- ization "
1803.08265,". the enumeration of planar maps equipped with an eulerian orientation has at- tracted attention in both combinatorics and theoretical physics since at least 2000. the case of 4-valent maps is particularly interesting: these orientations are in bijection with properly 3-coloured quadrangulations, while in physics they correspond to conﬁgurations of the ice model. we solve both problems – namely the enumeration of planar eulerian orientations and of 4-valent planar eulerian orientations – by expressing the associated generating functions as the inverses (for the composition of series) of simple hypergeometric series. using these ex- pressions, we derive the asymptotic behaviour of the number of planar eulerian orientations, thus proving earlier predictions of kostov, zinn-justin, elvey price and guttmann. this behaviour, µn/(n log n)2, prevents the associated generating functions from being d-ﬁnite. still, these generating functions are diﬀerentially algebraic, as they satisfy non-linear diﬀer- ential equations of order 2. diﬀerential algebraicity has recently been proved for other map problems, in particular for maps equipped with a potts model. our solutions mix recursive and bijective ingredients. in the 4-valent case, we also ob- serve an unexpected connection with the enumeration of maps equipped with a spanning tree that is internally inactive in the sense of tutte. this connection remains to be explained combinatorially. "
1803.08266," this paper analyzes the approximation properties of spaces of piece- wise tensor product polynomials over box meshes with a focus on ap- plication to isogeometric analysis (iga). the errors are measured in lebesgue norms. estimates of diﬀerent types are considered: local and global, with full or reduced sobolev seminorms. attention is also paid to the dependence on the degree and exponential convergence is proved for the approximation of analytic functions. 1 "
1803.08268,". we study the morse index of self-shrinkers for the mean curvature ﬂow and, more generally, of f -minimal hypersurfaces in a weighted euclidean space endowed with a convex weight. when the hypersurface is compact, we show that the index is bounded from below by an aﬃne function of its ﬁrst betti number. when the ﬁrst betti number is large, this improves index estimates known in literature. in the complete non-compact case, the lower bound is in terms of the dimension of the space of weighted square summable f -harmonic 1-forms; in particular, in dimension 2, the procedure gives an index estimate in terms of the genus of the surface. 8 1 0 2   r a   m 2 2     ] . g d h t a m [     1 v 8 6 2 8 0 . 3 0 8 1 : v i x r a contents "
1803.08269," a persistence diagram characterizes robust geometric and topological features in data. data, which will be treated here, are assumed to be drawn from a probability distribution and then the corresponding persistence diagrams have randomness. this paper reveals relationships between prob- ability distributions and persistence diagrams in the viewpoint of (1) the strong law of large numbers and the central limit theorem, (2) conﬁdence intervals, and (3) stability theorems via the persistence weighted gaussian kernel which is a statistical method for persistence diagrams. in numerical ex- periments for distributions, our method is compared against other statistical methods for persistence diagrams. "
1803.08270," rotation and magnetism are increasingly recognized as important phenomena in stellar evolution. surface magnetic ﬁelds from a few to 20,000 g have been observed and models have suggested that magnetohydrodynamic transport of angular momentum and chemical composition could explain the peculiar composition of some stars. stellar remnants such as white dwarfs have been observed with ﬁelds from a few to more than 109 g. we investigate the origin of and the evolution, on thermal and nuclear rather than dynamical time-scales, of an averaged large-scale magnetic ﬁeld throughout a star’s life and its coupling to stellar rotation. large-scale magnetic ﬁelds sustained until late stages of stellar evolution with conservation of magnetic ﬂux could explain the very high ﬁelds observed in white dwarfs. we include these eﬀects in the cambridge stellar evolution code using three time-dependant advection-diﬀusion equations coupled to the structural and composition equations of stars to model the evolution of angular momentum and the two components of the magnetic ﬁeld. we present the evolution in various cases for a 3 m⊙ star from the beginning to the late stages of its life. our particular model assumes that turbulent motions, including convection, favour small-scale ﬁeld at the expense of large-scale ﬁeld. as a result the large-scale ﬁeld concentrates in radiative zones of the star and so is exchanged between the core and the envelope of the star as it evolves. the ﬁeld is sustained until the end of the asymptotic giant branch, when it concentrates in the degenerate core. key words: (stars:) white dwarfs stars: evolution, stars: general, stars: magnetic ﬁelds, stars: rotation, "
1803.08273," we present a quantum algorithm for simulating the dynamics of hamiltonians that are not necessarily sparse. our algorithm is based on the assumption that the entries of the hamiltonian are stored in a data structure that allows for the eﬃcient preparation of states that encode the rows of the hamiltonian. we use a linear combination of quantum walks to achieve a poly-logarithmic dependence on the precision. the time complexity measured in nkhkpolylog(n, tkhk, 1/)), where t is the terms of circuit depth of our algorithm is o(t evolution time, n is the dimension of the system, and  is the error in the ﬁnal state, which we call precision. our algorithm can directly be applied as a subroutine for unitary hamiltonians √ n) dependence for both applications. and solving linear systems, achieving a eo( "
1803.08275,". many one-dimensional lattice particle models with open boundaries, like the paradigmatic asymmetric simple exclusion process (asep), have their stationary states represented in the form of a matrix product, with matrices that do not explicitly depend on the lattice site. in contrast, the stationary state of the open one-dimensional zero-range process (zrp) takes an inhomogeneous factorized form, with site-dependent probability weights. we show that in spite of the absence of correlations, the stationary state of the open zrp can also be represented in a matrix product form, where the matrices are site-independent, non-commuting and determined from algebraic relations resulting from the master equation. we recover the known distribution of the open zrp in two diﬀerent ways: ﬁrst, using an explicit representation of the matrices and boundary vectors; second, from the sole knowledge of the algebraic relations satisﬁed by these matrices and vectors. finally, an interpretation of the relation between the matrix product form and the inhomogeneous factorized form is proposed within the framework of hidden markov chains. "
1803.08276," speaker clustering is the task of diﬀerentiating speakers in a recording. in a way, the aim is to answer ""who spoke when"" in audio recordings. a common method used in industry is feature extraction directly from the recording thanks to mfcc features, and by using well-known techniques such as gaussian mixture models (gmm) and hidden markov models (hmm). in this paper, we studied neural networks (especially cnn) followed by clustering and audio processing in the quest to reach similar accuracy to state-of-the-art methods. i. "
1803.08277,"— synchronization in the networks of coupled oscil- lators is a widely studied topic in different areas. it is well- known that synchronization occurs if the connectivity of the network dominates heterogeneity of the oscillators. despite the extensive study on this topic, the quest for sharp closed-form synchronization tests is still in vain. in this paper, we present an algorithm for ﬁnding the taylor expansion of the inverse kuramoto map. we show that this taylor series can be used to obtain a hierarchy of increasingly accurate approximate tests with low computational complexity. these approximate tests are then used to estimate the threshold of synchronization as well as the position of the synchronization manifold of the network. "
1803.08279," improper aﬃne spheres have played an important role in the development of geometric methods for the study of the hessian one equation. here, we review most of the advances we have made in this direction during the last twenty years. "
1803.08281," a detailed-level collisional-radiative model for the m1 transition spectrum of the ca-like w54+ ion as observed in an electron beam ion trap (ebit) was constructed based on atomic data calculated by the relativistic conﬁguration interaction method and distorted wave theory. the present calculated tran- sition energy, rate and intensity of w54+ m1 transitions are compared with previous theoretical and experimental values. the results are in reasonable agreement with the available experimental and theoretical data. the synthetic spectrum explained the ebit spectrum in the 12-20 nm region, while a new possibly strong transition has been predicted to be observable with an appro- priate electron beam energy. the present work provides accurate atomic data that may be used in plasma diagnostics applications. keywords: collisional-radiative model, ca-like tungsten, relativistic conﬁguration interaction, ebit spectrum simulation ✩please cite this article in press as: x.ding et al., collisional radiative model for the m1 transition spectrum of the highly-charged w54+ ions, phys. lett. a (2018), https://doi.org/10.1016/j.physleta.2018.05.046 . ∗corresponding author. email addresses: dingxb@nwnu.edu.cn (xiaobin ding), dongcz@nwnu.edu.cn (chenzhong dong) preprint submitted to physics letters a june 4, 2018 "
1803.08283," we revisit the problem of deriving local gauge invariance with spontaneous symmetry breaking in the context of an eﬀective ﬁeld theory. previous derivations were based on the condition of tree- order unitarity. however, the modern point of view considers the standard model as the leading order approximation to an eﬀective ﬁeld theory. as tree-order unitarity is in any case violated by higher-order terms in an eﬀective ﬁeld theory, it is instructive to investigate a formalism which can be also applied to analyze higher-order interactions. in the current work we consider an eﬀective ﬁeld theory of massive vector bosons interacting with a massive scalar ﬁeld. we impose the conditions of generating the right number of constraints for systems with spin-one particles and perturbative renormalizability as well as the separation of scales at one-loop order. we ﬁnd that the above conditions impose severe restrictions on the coupling constants of the interaction terms. except for the strengths of the self-interactions of the scalar ﬁeld, that can not be determined at this order from the analysis of three- and four-point functions, we recover the gauge-invariant lagrangian with spontaneous symmetry breaking taken in the unitary gauge as the leading order approximation to an eﬀective ﬁeld theory. we also outline the additional work that is required to ﬁnish this program. pacs numbers: 04.60.ds, 11.10.gh, 03.70.+k, keywords: eﬀective ﬁeld theory; quantization; constraints; renormalization 8 1 0 2   r a   m 2 2     ] h t - p e h [     1 v 3 8 2 8 0 . 3 0 8 1 : v i x r a 1 i. "
1803.08284,". we observe that automorphism groups of right-angled artin groups contain nilpotent non-abelian subgroups, namely h3(z) the three- dimensional integer heisenberg group, provided they admit a certain type of element, called an adjacent transvection. this represents a (mi- nor) extension of a result of charney-vogtmann. "
1803.08286," the harborth constant for a ﬁnite group g, denoted g(g), is the smallest integer k such that every subset {g1, . . . , gk} ⊂ g of cardinality k over g admits an ordered subsequence j=1 gij = 1. the harborth constant has been well studied for several classes of abelian groups but not so well for nonabelian groups. in this short paper, we determine the harborth constant for the dihedral group explicitly. (gi1 , . . . , gil) with l = exp(g) such that ql keywords: zero-sum problems, harborth constant, dihedral group. 2010 ams classiﬁcation code: 11b30,11b75, 20d60 "
1803.08287,"— learning-based methods have been successful in solving complex control tasks without signiﬁcant prior knowl- edge about the system. however, these methods typically do not provide any safety guarantees, which prevents their use in safety-critical, real-world applications. in this paper, we present a learning-based model predictive control scheme that provides provable high-probability safety guarantees. to this end, we exploit regularity assumptions on the dynamics in terms of a gaussian process prior to construct provably accurate conﬁdence intervals on predicted trajectories. unlike previous approaches, we do not assume that model uncertainties are independent. based on these predictions, we guarantee that trajectories satisfy safety constraints. moreover, we use a terminal set constraint to recursively guarantee the existence of safe control actions at every iteration. in our experiments, we show that the resulting algorithm can be used to safely and efﬁciently explore and learn about dynamic systems. "
1803.08288,"— this paper presents a novel control strategy for the coordination of a multi-agent system subject to high- level goals expressed as linear temporal logic formulas. in particular, each agent, which is modeled as a sphere with 2nd order dynamics, has to satisfy a given local temporal logic speciﬁcation subject to connectivity maintenance and inter-agent collision avoidance. we propose a novel continuous control protocol that guarantees navigation of one agent to a goal point, up to a set of collision-free initial conﬁgurations, while maintaining connectivity of the initial neighboring set and avoiding inter-agent collisions. based on that, we develop a hybrid switching control strategy that ensures that each agent satisﬁes its temporal logic task. simulation results depict the validity of the proposed scheme. "
1803.08291," we study a coupled bulk-surface allen–cahn system with an aﬃne linear trans- mission condition, that is, the trace values of the bulk variable and the values of the surface variable are connected via an aﬃne relation, and this serves to generalize the usual dynamic boundary conditions. we tackle the problem of well-posedness via a penalization method using robin boundary conditions. in particular, for the relax- ation problem, the strong well-posedness and long-time behavior of solutions can be shown for more general and possibly nonlinear relations. new diﬃculties arise since the surface variable is no longer the trace of the bulk variable, and uniform estimates in the relaxation parameter are scarce. nevertheless, weak convergence to the orig- inal problem can be shown. using the approach of colli and fukao (math. models appl. sci. 2015), we show strong existence to the original problem with aﬃne linear relations, and derive an error estimate between solutions to the relaxed and original problems. key words. allen–cahn equation, maximal monotone graphs, dynamic boundary con- ditions, well-posedness, penalization via robin boundary conditions. ams subject classiﬁcation. 35b40, 35d35, 35k20, 35k61, 35k86 "
1803.08292," the electronic structure, magnetic properties and phase formation of hexagonal ferromagnetic fe3sn-based alloys have been studied from ﬁrst principles and by experiment. the pristine fe3sn compound is known to fulﬁll all the requirements for a good permanent magnet, except for the magnetocrystalline anisotropy energy (mae). the latter is large, but planar, i.e. the easy mag- netization axis is not along the hexagonal c direction, whereas a good permanent magnet requires the mae to be uniaxial. here we consider fe3sn0.75m0.25, where m= si, p, ga, ge, as, se, in, sb, te and bi, and show how diﬀerent dopants on the sn sublattice aﬀect the mae and can alter it from planar to uniaxial. the stability of the doped fe3sn phases is elucidated theoretically via the calculations of their formation enthalpies. a micromagnetic model is developed in order to estimate the energy density product (bh)max and coercive ﬁeld µ0hc of a potential magnet made of fe3sn0.75sb0.25, the most promising candidate from theoretical studies. the phase sta- bility and magnetic properties of the fe3sn compound doped with sb and mn has been checked experimentally on the samples synthesised using the reactive crucible melting technique as well as by solid state reaction. the fe3sn-sb compound is found to be stable when alloyed with mn. it is shown that even small structural changes, such as a change of the c/a ratio or volume, that can be induced by, e.g., alloying with mn, can inﬂuence anisotropy and reverse it from planar to uniaxial and back. pacs numbers: 75.50.ww, 75.30.gw, 75.20.en ∗ olga.vekilova@physics.uu.se 2 i. "
1803.08293," we study the convex hull of the ﬁrst n steps of a planar random walk, and present large-n asymptotic results on its perimeter length ln, diameter dn, and shape. in the case where the walk has a non-zero mean drift, we show that ln/dn → 2 a.s., and give distributional limit theorems and variance asymptotics for dn, and in the zero-drift case we show that the convex hull is inﬁnitely often arbitrarily well- approximated in shape by any unit-diameter compact convex set containing the origin, and then lim inf n→∞ ln/dn = 2 and lim supn→∞ ln/dn = π, a.s. among the tools that we use is a zero-one law for convex hulls of random walks. key words: random walk; convex hull; perimeter length; diameter; shape; zero-one law. ams subject classiﬁcation: 60g50 (primary) 60d05; 60f05; 60f15; 60f20 (secondary) 1 model and main results 1.1 "
1803.08294,". the interface region imaging spectrograph (iris) reveals numerous small-scale (sub-arcsecond) brightenings that appear as bright dots sparkling the solar transition region in active regions. here, we report a statistical study on these transition- region bright dots. we use an automatic approach to identify 2742 dots in a si iv raster image. we ﬁnd that the average spatial size of the dots is 0.8 arcsec2 and most of them are located in the faculae area. their doppler velocities obtained from the si iv 1394 å line range from −20 to 20 km s−1. among these 2742 dots, 1224 are predominantly blue-shifted and 1518 are red-shifted. their non- thermal velocities range from 4 to 50 km s−1 with an average of 24 km s−1. we speculate that the bright dots studied here are small-scale impulsive energetic events that can heat the active region corona. "
1803.08295,". we introduce a notion of weak anticommutativity for a pair (s, t ) of self-adjoint regular operators in a hilbert c∗-module e. we prove that the sum s + t of such pairs is self-adjoint and regular on the intersection of their domains. a similar result then holds for the sum s2 + t 2 of the squares. we show that our deﬁnition is closely related to the connes-skandalis positivity criterion in kk-theory. as such we weaken a sufﬁcient condition of kucerovsky for repre- senting the kasparov product. our proofs indicate that our conditions are close to optimal. 8 1 0 2   r a   m 2 2     ] . a o h t a m [     1 v 5 9 2 8 0 . 3 0 8 1 : v i x r a contents "
1803.08297," nonlinear optical phenomena are widely used for the study of semiconductor materials. the paper presents an overview of experimental and theoretical studies of excitons by the method of optical second and third harmonics generation in various bulk semiconductors (gaas, cdte, znse, zno, cu2o, (cd,mn)te, eute, euse), and low-dimensional heterostructures znse/bete. particular attention is paid to the role of external electric and magnetic ﬁelds that modify the exciton states and induce new mechanisms of optical harmonics generation. microscopic mechanisms of harmonics generation based on the stark eﬀect, the spin and orbital zeeman eﬀects, and on the magneto-stark eﬀect speciﬁc for excitons moving in an external magnetic ﬁeld are considered. this approach makes it possible to study the properties of excitons and to obtain new information on their energy and spin structure that is not available when the excitons are investigated by linear optical spectroscopy. as a result of these studies, a large amount of information was obtained, which allows us to conclude on the establishing of a new ﬁeld of research (cid:22) exciton spectroscopy by the method of optical harmonics generation. this work was supported by the deutsche forschungsgemeinschaft (grant icrc trr160, project c8 and grant trr142, projects b01 and b04) and the russian foundation for basic research (grants 15-52-12015 and 16-02-00377). 8 1 0 2   r p a 2       ] i c s - l r t m . t a m - d n o c [     2 v 7 9 2 8 0 . 3 0 8 1 : v i x r a i. "
1803.08298,"—in this paper, we study the effects of the variations of the propagation delay over large-scale antenna-arrays used in massive multiple-input multiple-output (mimo) wideband communication systems on the statistical properties of the channel. due to its simplicity and popularity, the elliptical geometry-based stochastic channel model (gbsm) is employed to demonstrate new non-stationary properties of the channel in the frequency and spatial domains caused by the drift of delays. in addition, we show that the time of travel of multi-path components (mpcs) over large-scale arrays may result in overlooked frequency and spatial decorrelation effects. these are theoretically demonstrated by deriving the space-time- frequency correlation functions (stfcfs) of both narrowband and wideband elliptical models. closed-form expressions of the array-variant frequency correlation function (fcf), power delay proﬁle (pdp), mean delay, and delay spread of single- and multi-confocal elliptical models are derived when the angles of arrival (aoas) are von mises distributed. in such conditions, we ﬁnd that the large dimensions of the antenna array may limit the narrowband characteristic of the single-ellipse model and alter the wideband characteristics (pdp and fcf) of the multi-confocal elliptical channel model. although we present and analyze numerical and simulation results for a particular gbsm, similar conclusions can be extended to other gbsms. keywords – massive mimo, channel modeling, spatial non- stationarity, array-variant delay. "
1803.08299," deregulated energy markets, demand forecasting, and the continuously increasing share of renewable energy sources call—among others—for a structured consideration of uncertainties in optimal power ﬂow problems. the main challenge is to guarantee power balance while maintaining economic and secure operation. in the presence of gaussian uncertainties aﬃne feedback policies are known to be viable options for this task. the present paper advocates a general framework for chance-constrained opf problems in terms of continuous random variables. it is shown that, irrespective of the type of distribution, the random-variable minimizers lead to aﬃne feedback policies. introducing a three-step methodology that exploits polynomial chaos expansion, the present paper provides a constructive approach to chance-constrained optimal power ﬂow problems that does not assume a speciﬁc distribution, e.g. gaussian, for the uncertainties. we illustrate our ﬁndings by means of a tutorial example. keywords: chance-constrained optimal power ﬂow, uncertainties, aﬃne policies, polynomial chaos 8 1 0 2   r a   m 2 2     ] . c o h t a m [     1 v 9 9 2 8 0 . 3 0 8 1 : v i x r a list of symbols "
1803.08300," we demonstrate for the ﬁrst time using a robust bayesian approach to analyse the populations of radio-quiet (rq) and radio-loud (rl) gamma-ray pulsars. we quantify their diﬀerences and obtain their distributions of the radio-cone opening half-angle δ and the magnetic inclination angle α by bayesian inference. in contrast to the conventional frequentist point estimations that might be non-representative when the distribution is highly skewed or multi-modal, which is often the case when data points are scarce, bayesian statistics displays the complete posterior distribution that the uncertainties can be readily obtained regardless of the skewness and modality. we found that the spin period, the magnetic ﬁeld strength at the light cylinder, the spin-down power, the gamma-ray-to-x-ray ﬂux ratio, and the spectral curvature signiﬁcance of the two groups of pulsars exhibit signiﬁcant diﬀerences at the 99% level. using bayesian inference, we are able to infer the values and uncertainties of δ and α from the distribution of rq and rl pulsars. we found that δ is between 10◦ and 35◦ and the distribution of α is skewed towards large values. keywords: gamma rays; stars — pulsars: general; methods: statistical 8 1 0 2   r a   m 2 2 .     ] e h h p - o r t s a [     1 v 0 0 3 8 0 . 3 0 8 1 : v i x r a corresponding author: hoi-fung yu dy@kth.se 2 yu et al. "
1803.08301," i=1, αi ∈ g, is a coset partition of g, then let g be a group and h1,...,hs be subgroups of g of indices d1,...,ds respectively. in 1974, m. herzog and j. sch¨onheim con- jectured that if {hiαi}i=s d1,..,ds cannot be distinct. we consider the herzog-sch¨onheim con- jecture for free groups of ﬁnite rank and develop a new combinatorial approach, using covering spaces. we give some suﬃcient conditions on the coset partition that ensure the conjecture is satisﬁed. further- more, under a certain assumption, we show there is a ﬁnite number of cases to study in order to show the conjecture is true for every coset partition. since every ﬁnitely generated group is a quotient of a free group of ﬁnite rank, we show these results extend to ﬁnitely generated groups. 1 "
1803.08302,". given a numerical semigroup ring r = k[[s]], an ideal e of s and an odd element b ∈ s, the numerical duplication s ✶b e is a numerical semigroup, whose associated ring k[[s ✶b e]] shares many properties with the nagata’s idealization and the amalgamated duplication of r along the monomial ideal i = (te | e ∈ e). in this paper we study the associated graded ring of the numerical duplication characterizing when it is cohen-macaulay, gorenstein or complete intersection. we also study when it is a homogeneous numerical semigroup, a property that is related to the fact that a ring has the same betti numbers of its associated graded ring. on the way we also characterize when grm(i) is cohen-macaulay and when grm(ωr) is a canonical module of grm(r) in terms of numerical semigroup’s properties, where ωr is a canonical module of r. "
1803.08303,". this work is entirely devoted to construct huge families of indecomposable arithmeti- cally cohen-macaulay (resp. ulrich) sheaves e of arbitrary high rank on a general standard (resp. linear) determinantal scheme x ⊂ pn of codimension c ≥ 1, n − c ≥ 1 and deﬁned by the maximal minors of a t × (t + c − 1) homogeneous matrix a. the sheaves e are constructed as iterated extensions of sheaves of lower rank. as applications: (1) we prove that any general standard de- terminantal scheme x ⊂ pn is of wild representation type provided the degrees of the entries of the matrix a satisfy some weak numerical assumptions; and (2) we determine values of t, n and n − c for which a linear standard determinantal scheme x ⊂ pn is of wild representation type with respect to the much more restrictive category of its indecomposable ulrich sheaves, i.e. x is of ulrich wild representation type. contents "
1803.08304," persistent entropy of persistence barcodes, which is based on the shannon entropy, has been recently deﬁned and successfully applied to diﬀerent scenarios: characterization of the idiotypic immune network, detection of the transition between the preictal and ictal states in eeg signals, or the classiﬁcation problem of real long-length noisy signals of dc electrical motors, to name a few. in this paper, we study properties of persistent entropy and prove its stability under small perturbations in the given input data. from this concept, we deﬁne three summary functions and show how to use them to detect patterns and topological features. "
1803.08305," we study the interactions of a relativistic jet with a dense turbulent gaseous disk of radius ∼ 2 kpc. we have performed a suite of simulations with diﬀerent mean den- sity, jet power and orientation. our results show that: a) the relativistic jet couples strongly with the gas in the inner kpc, creating a cavity and launching outﬂows. b) the high pressure bubble inﬂated by the jet and its back-ﬂow compresses the disk at the outer edges, driving inﬂows within the disk. c) jets inclined towards the disk couple more strongly with the disk and launch sub-relativistic, wide-angle outﬂows along the minor axis. d) shocks driven directly by the jet and the jet-driven energy bubble raise the velocity dispersion throughout the disk by several times its initial value. e) compression by the jet-driven shocks can enhance the star formation rate in the disk, especially in a ring-like geometry close to the axis. however, enhanced turbulent dispersion in the disk also leads to quenching of star formation. whether positive or negative feedback dominates depends on jet power, ism density, jet orien- tation with respect to the disc, and the time-scale under consideration. qualitatively, our simulations compare favourably with kinematic and morphological signatures of several observed galaxies such as ngc 1052, ngc 3079, 3c 326 and 3c 293. key words: galaxies: jets – galaxies: ism – hydrodynamics – galaxies: evolution – galaxies: high-redshift – methods: numerical "
1803.08310," magnetic imprints, the rapid and irreversible evolution of photospheric magnetic ﬁelds as a feedback from ﬂares in the corona, have been conﬁrmed by many previous studies. these studies showed that the horizontal ﬁeld will permanently increase near the polarity inversion line (pil) after eruptions, indicating that a more horizontal topology of photospheric magnetic ﬁeld will be reconstructed. in this study, we analyze 17 near- disk x-class ﬂares in 13 active regions (ars) with heliographic angle no greater than 45o since the launch of the solar dynamics observatory (sdo). we ﬁnd that conﬁned ﬂares without or with very weak cmes tend to show very weak magnetic imprints on the photosphere. the imprint regions of the horizontal ﬁeld could locate not only near the pil but also near sunspot umbrae with strong vertical ﬁelds. making use of the observed cme mass and speed, we ﬁnd that the cmes with larger momentums will bring into stronger magnetic imprints. furthermore, a linear relationship, with a conﬁdence coefﬁcient 0.82, between the cme momentum and the change of lorentz force is revealed. based on that, we quantify the back reaction time to be 336 s, which could be further applied to independently estimate the cme mass. keywords: sun: ﬂares — sun: magnetic ﬁelds — sun: photosphere "
1803.08311," we prove the existence of scarred eigenstates for star graphs with scattering matri- ces at the central vertex which are either a fourier transform matrix, or a matrix that prohibits back-scattering. we prove the existence of scars that are half-delocalised on a single bond. moreover we show that the scarred states we construct are maximal in the sense that it is impossible to have quantum eigenfunctions with a signiﬁcantly lower entropy than our examples. these scarred eigenstates are on graphs that exhibit generic spectral statistics of random matrix type in the large graph limit, and, in contrast to other constructions, correspond to non-degenerate eigenvalues; they exist for almost all choices of lengths. 1 "
1803.08312," interventional cancer clinical trials are generally too restrictive and some patients are often excluded from them on the basis of comorbidity, past or concomitant treatments and the fact that they are over a certain age. the eﬃcacy and safety of new treatments for patients with these characteristics are not, therefore, deﬁned. in this work, we build a model with which to automatically predict whether short clinical statements were considered in- clusion or exclusion criteria. we used clinical trials protocols on cancer that have been available in public registries for the last 18 years to train word em- beddings, and constructed a dataset of 6m short free-texts labeled as eligible or not eligible. we then trained and validated a text classiﬁer, using deep neural networks with pre-trained word-embedding as its inputs, to predict whether or not short free-text statements describing clinical information were considered eligible. the best model achieved an f-measure of 0.91 and an almost perfect agreement when employing a validation set of 800k labeled statements. the trained model was also tested on an independent set of clinical statements mimicking those used in routine clinical practice, yielding a consistent performance. we additionally analyzed the semantic reasoning of the word embedding representations obtained, and were able to identify equivalent treatments for a type of tumor in an analogy with the drugs used to treat other tumors. the present work shows that representation learning using neural networks can be successfully leveraged to extract the medical knowledge available on clinical trial protocols and potentially assist practi- ∗corresponding author. email addresses: aurelia@medbravo.org (aurelia bustos), pertusa@dlsi.ua.es (antonio pertusa) preprint submitted to journal of biomedical informatics march 26, 2018 tioners when prescribing treatments. keywords: clinical trials, clinical decision support system, natural language processing, word embeddings, deep neural networks "
1803.08313,". this is the ﬁrst in a series of three notes on an investigation into core regular double stone algebras, crdsa [2], which are meant to be read in order. in [2] many useful results regarding the center and core of a crdsa that indicate the nearly boolean nature of crdsa which we extend here. these notes were born out of a model of network security where individual nodes are considered to be in one of 3 states. let j be any non-empty set of network nodes, not necessarily ﬁnite. we deﬁne the node set bounded dis- tributive lattice through the pairwise disjoint subsets of j with the well known binary operations of ternary set partitions and note j = 1 is our minimal case. we then show the resultant bounded distributive lattice is isomorphic to c j 3 where c3 is the 3 element chain crdsa. we then derive that every crdsa is a subdirect product of c3, similarly as for boolean algebras and c2. we use these results along with a few known results to show the main re- sult, namely every boolean algebra is the center of some core regular double stone algebra, crdsa. we then use that result to characterize all subalgebras of a ﬁnite core regular double stone algebras, namely a crdsa a is a subalgebra of cj 3 for some k ≤ j. applications of these results are being explored, results of at least one of which will be published at a later date. 3 for some ﬁnite j ↔ a ∼= ck lastly, we show that c3 is primal which implies that the variety gener- ated by c3 is dually equivalent to the category of stone spaces and hence the category of boolean algebras. in some sense this is a last step towards our goal of establishing crdsa as nearly boolean, but leaves us a bit dissatisﬁed as to our understanding of crdsa in the dual topological cate- gory. the following two notes, exploring core regular double stone algebras, crdsa, ii,iii, will establish a duality between the category of crdsa and speciﬁcally crafted bi-topological spaces that enables better understanding of the ”nearly boolean” nature of crdsa in the dual category. in this note we prove the following result: "
1803.08314,". the aim of image captioning is to generate similar captions by machine as human do to describe image contents. despite many ef- forts, generating discriminative captions for images remains non-trivial. most traditional approaches imitate the language structure patterns, thus tend to fall into a stereotype of replicating frequent phrases or sentences and neglect unique aspects of each image. in this work, we propose an image captioning framework with a self-retrieval module as training guidance, which encourages generating discriminative captions. it brings unique advantages: (1) the self-retrieval guidance can act as a metric and an evaluator of caption discriminativeness to assure the qual- ity of generated captions. (2) the correspondence between generated captions and images are naturally incorporated in the generation pro- cess without human annotations, and hence our approach could utilize a large amount of unlabeled images to boost captioning performance with no additional laborious annotations. we demonstrate the eﬀectiveness of the proposed retrieval-guided method on ms-coco and flickr30k captioning datasets, and show its superior captioning performance with more discriminative captions. keywords: image captioning, language and vision, text-to-image re- trieval "
1803.08316," motivated by the idea of “smart” metasurfaces, we will demonstrate a graphene- based tunable absorber in which perfect absorption can be achieved for all angles of in- cidence, only by tuning the fermi level of graphene. we place an unpatterned graphene sheet on a mushroom-type high impedance surface whose resonant frequency is stable for all incident angles. for tm-polarization, perfect absorption can be realized from normal to grazing incidence at the same frequency when modulating the fermi level of graphene from 0.18 ev to 1 ev. "
1803.08317,". inspired by the algorithm of barnsley’s chaos game, we construct an open quantum system model based on the repeated interaction process. we shown that the quantum dynamics of the appropriate fermionic/bosonic system (in interaction with an environment) provides a physical model of the chaos game. when considering fermionic operators, we follow the system’s evolution by focusing on its reduced density matrix. the system is shown to be in a gaussian state (at all time t) and the average number of particles is shown to obey the chaos game equation. considering bosonic operators, with a system initially prepared in coherent states, the evolution of the system can be tracked by investigating the dynamics of the eigenvalues of the annihilation operator. this quantity is governed by a chaos game-like equation from which diﬀerent scenarios emerge. "
1803.08319,". multi-people tracking in an open-world setting requires a special eﬀort in precise detection. moreover, temporal continuity in the detection phase gains more importance when scene cluttering introduces the challenging problems of occluded targets. for the purpose, we pro- pose a deep network architecture that jointly extracts people body parts and associates them across short temporal spans. our model explicitly deals with occluded body parts, by hallucinating plausible solutions of not visible joints. we propose a new end-to-end architecture composed by four branches (visible heatmaps, occluded heatmaps, part aﬃnity ﬁelds and temporal aﬃnity ﬁelds) fed by a time linker feature extractor. to overcome the lack of surveillance data with tracking, body part and oc- clusion annotations we created the vastest computer graphics dataset for people tracking in urban scenarios by exploiting a photorealistic videogame. it is up to now the vastest dataset (about 500.000 frames, more than 10 million body poses) of human body parts for people track- ing in urban scenarios. our architecture trained on virtual data exhibits good generalization capabilities also on public real tracking benchmarks, when image resolution and sharpness are high enough, producing reliable tracklets useful for further batch data association or re-id modules. keywords: pose estimation, tracking, surveillance, occlusions "
1803.08320," a new cuda-c code for tracing orbits around non-charged black holes is presented. this code, named malbec, take advantage of the graphic process- ing units and the cuda platform for tracking null and timelike test particles in schwarzschild and kerr. also, a new general set of equations that describe the closed circular orbits of any timelike test particle in the equatorial plane is de- rived. these equations are extremely important in order to compare the analytical behavior of the orbits with the numerical results and verify the correct implementa- tion of the runge-kutta algorithm in malbec. finally, other numerical tests are performed, demonstrating that malbec is able to reproduce some well-known results in these metrics in a faster and more efﬁcient way than a conventional cpu implementation. keywords: cuda-c, gpu, timelike test particles, geodesics around black holes. pacs numbers:02.60.cb, 04.25.dg, 04.70.-s 1 "
1803.08322," using high resolution powder x-ray and neutron diﬀraction experiments, we determined the oﬀ-centered displacement of the ions within a unit cell and magnetoelectric coupling in nanoscale bifeo3 (≈20-200 nm). we found that both the oﬀ-centered displacement of the ions and magnetoelectric coupling exhibit nonmonotonic variation with particle size. they increase as the particle size reduces from bulk and reach maximum around 30 nm. with further decrease in particle size, they decrease precipitously. the magnetoelectric coupling is determined by the anomaly in oﬀ-centering of ions around the magnetic transition temperature (tn ). the ions, in fact, exhibit large anomalous displacement around the tn which is analyzed using group theoretical approach. it underlies the nonmonotonic particle-size-dependence of oﬀ-centre displacement of ions and magnetoelectric coupling. the nonmonotonic variation of magnetoelectric coupling with particle size is further veriﬁed by direct electrical measurement of remanent ferroelectric hysteresis loops at room temperature under zero and ∼20 koe mag- netic ﬁeld. competition between enhanced lattice strain and compressive pressure appears to be causing the nonmonotonic particle-size-dependence of oﬀ-centre displacement while coupling between piezo and magnetostriction leads to nonmonotonicity in the variation of magnetoelec- tric coupling. "
1803.08323," in this work, we propose a novel approach to prioritize the depth map computation of multi-view stereo (mvs) to obtain compact 3d point clouds of high quality and completeness at low computational cost. our prioritization approach operates before the mvs algorithm is executed and consists of two steps. in the ﬁrst step, we aim to ﬁnd a good set of matching partners for each view. in the second step, we rank the resulting view clusters (i.e. key views with matching partners) according to their impact on the fulﬁllment of desired quality parameters such as completeness, ground resolution and accuracy. additional to geometric analysis, we use a novel machine learning technique for training a conﬁdence predictor. the purpose of this conﬁdence predictor is to estimate the chances of a successful depth reconstruction for each pixel in each image for one speciﬁc mvs algorithm based on the rgb images and the image constellation. the underlying machine learning technique does not require any ground truth or manually labeled data for training, but instead adapts ideas from depth map fusion for providing a supervision signal. the trained conﬁdence predictor allows us to evaluate the quality of image constellations and their potential impact to the resulting 3d reconstruction and thus builds a solid foundation for our prioritization approach. in our experiments, we are thus able to reach more than 70% of the maximal reachable quality fulﬁllment using only 5% of the available images as key views. for evaluating our approach within and across diﬀerent domains, we use two completely diﬀerent scenarios, i.e. cultural heritage preservation and reconstruction of single family houses. keywords: multi-view stereo, machine learning, conﬁdence measures, view prioritization, image clustering, view cluster ranking "
1803.08326,". we propose a novel statistical color constancy method, especially suit- able for the camera-agnostic color constancy, i.e. the scenario where nothing is known a priori about the capturing devices. the method, called dichromatic gray pixel, or dgp, relies on a novel gray pixel detection algorithm derived us- ing the dichromatic reﬂection model. dgp is suitable for camera-agnostic color constancy, since varying devices are set to make achromatic pixels look gray un- der standard neutral illumination. in the camera-agnostic scenario, the proposed method outperforms, on standard benchmarks, both state-of-the-art learning-based and statistical methods. dgp is simple, literally dozens of lines of code, and fast, processing a 1080p image in 0.4 seconds with unoptimized matlab code run- ning in a cpu intel i7 2.5 ghz. keywords: illumination estimation, color constancy, dichromatic reﬂection model, gray pixel "
1803.08328,"— in this paper we consider a distributed convex optimization problem over time-varying networks. we propose a dual method that converges r-linearly to the optimal point given that the agents’ objective functions are strongly convex and have lipschitz continuous gradients. the proposed method requires half the amount of variable exchanges per iterate than methods based on diging, and yields improved practical performance as empirically demonstrated. "
1803.08329," we consider the contributions upto the d10r4 terms in the low momentum ex- pansion of the two loop four graviton amplitude in maximal supergravity that arise in the ﬁeld theory limit of genus two modular graph functions that result from the low momentum expansion of the four graviton amplitude in toroidally compactiﬁed type ii string theory, using the worldline formalism of the ﬁrst quantized superparticle. the expression for the two loop supergravity amplitude in the worldline formalism allows us to obtain contributions from the individual graphs, unlike the expression for the same amplitude obtained using unitarity cuts which only gives the total con- tribution from the sum of all the graphs. our two loop analysis is ﬁeld theoretic, and does not make explicit use of the genus two string amplitude. 8 1 0 2   r a   m 2 2     ] h t - p e h [     1 v 9 2 3 8 0 . 3 0 8 1 : v i x r a 1email address: anirbanbasu@hri.res.in 1 "
1803.08331,". we classify certain cases when the wreath products of distinct pairs of groups generate the same variety. this allows us to investigate the subvarieties of some nilpotent- by-abelian product varieties uv with the help of wreath products of groups. in particular, using wreath products we ﬁnd such subvarieties in nilpotent-by-abelian uv, which have the same nilpotency class, the same length of solubility, and the same exponent, but which still are distinct subvarieties. obtained classiﬁcation strengthens our recent work on varieties generated by wreath products. "
1803.08332,". in this paper, we show that every singular ﬁber of the gelfand–cetlin system on coadjoint orbits of unitary groups is a smooth isotropic submanifold which is diﬀeomorphic to a 2-stage quotient of a compact lie group by free actions of two other compact lie groups. in many cases, these singular ﬁbers can be shown to be homogeneous spaces or even diﬀeomorphic to compact lie groups. we also give a combinatorial formula for computing the dimensions of all singular ﬁbers, and give a detailed description of these singular ﬁbers in many cases, including the so-called (multi-)diamond singularities. these (multi-)diamond singular ﬁbers are degenerate for the gelfand–cetlin system, but they are lagrangian submanifolds diﬀeomorphic to direct products of special unitary groups and tori. our methods of study are based on diﬀerent ideas involving complex ellipsoids, lie groupoids, and also general ideas coming from the theory of singularities of integrable hamiltonian systems. contents "
1803.08334," we develop a machine learning-based framework to predict the hi content of galaxies using more straightforwardly observable quantities such as optical photometry and environmental parameters. we train the algorithm on z = 0 − 2 outputs from the mufasa cosmological hydrodynamic simulation, which includes star formation, feedback, and a heuristic model to quench massive galaxies that yields a reasonable match to a range of survey data including hi. we employ a variety of machine learning methods (regressors), and quantify their performance using the root mean square error (rmse) and the pearson correlation coeﬃcient (r). considering sdss photometry, 3rd nearest neighbor environment and line of sight peculiar velocities as features, we obtain r > 0.8 accuracy of the hi-richness prediction, corresponding to rmse< 0.3. adding near-ir photometry to the features yields some improvement to the prediction. compared to all the regressors, random forest shows the best performance, with r > 0.9 at z = 0, followed by a deep neural network with r > 0.85. all regressors exhibit a declining performance with increasing redshift, which limits the utility of this approach to z (cid:46) 1, and they tend to somewhat over-predict the hi content of low-hi galaxies which might be due to eddington bias in the training sample. we test our approach on the resolve survey data. training on a subset of resolve, we ﬁnd that our machine learning method can reasonably well predict the hi-richness of the remaining resolve data, with rmse∼ 0.28. when we train on mock data from mufasa and test on resolve, this increases to rmse∼ 0.45. our method will be useful for making galaxy-by-galaxy survey predictions and incompleteness corrections for upcoming hi 21cm surveys such as the laduma and mightee surveys on meerkat, over regions where photometry is already available. key words: galaxies: evolution – galaxies: statistics – methods: n-body simulations 1 "
1803.08336,": this paper presents a continuous-time equilibrium model of liquidity provision in a market with multiple strategic investors with intraday trading targets. we show analytically that there are inﬁnitely many nash equilibria. we solve for the welfare-maximizing equilibrium and the competitive equilibrium, and we illustrate that these equilib- ria are diﬀerent. the model is easily computed numerically, and we provide a number of numerical illustrations. keywords: dynamic trading, twap, rebalancing, equilibrium ams subject classifications: 93e20 jel-classification: g12, g11, d53 8 1 0 2   r a m   2 2 .     ] f m n i f - q [     1 v 6 3 3 8 0 . 3 0 8 1 : v i x r a 1 1 "
1803.08337," encoder (ﬁxed) decoder (ﬁne-tuned) classiﬁer (ﬁxed) 8 1 0 2   r a   m 2 2     ] v c . s c [     1 v 7 3 3 8 0 . 3 0 8 1 : v i x r a we propose a novel way to measure and understand con- volutional neural networks by quantifying the amount of in- put signal they let in. to do this, an autoencoder (ae) was ﬁne-tuned on gradients from a pre-trained classiﬁer with ﬁxed parameters. we compared the reconstructed samples from aes that were ﬁne-tuned on a set of image classiﬁers (alexnet, vgg16, resnet-50, and inception v3) and found substantial differences. the ae learns which aspects of the input space to preserve and which ones to ignore, based on the information encoded in the backpropagated gradients. measuring the changes in accuracy when the signal of one classiﬁer is used by a second one, a relation of total order emerges. this order depends directly on each classiﬁer’s in- put signal but it does not correlate with classiﬁcation accu- racy or network size. further evidence of this phenomenon is provided by measuring the normalized mutual informa- tion between original images and auto-encoded reconstruc- tions from different ﬁne-tuned aes. these ﬁndings break new ground in the area of neural network understanding, opening a new way to reason, debug, and interpret their re- sults. we present four concrete examples in the literature where observations can now be explained in terms of the input signal that a model uses. "
1803.08338," at radio wavelengths, solar type stars emit thermal free-free and gyroresonance, gyrosyn- chrotron, and impulsive coherent emission. thermal free-free emission originates at layers where the optical depth is close to unit, while high brightness temperature, variable emission, can be due to ﬂares via gyrosynchrotron emission. we observed the α cen system with the australian telescope compact array at 2 ghz for three days and 17 ghz for one day. both stars have been detected at 17 ghz, while only an upper limit has been obtained at low fre- quency despite the longest integration time. the brightness temperatures are consistent with the temperature of the upper chromosphere of the sun. inverting the formulae of the free-free emission, the average electron density of the plasma has been inferred. the same procedure was applied to the data in the millimetre recently taken with alma. a comparison with the atmospheric solar models reveals a higher level of activity in α cen b, even if still at quies- cent level. the non detection at low frequency allow us to put a lower limit in the ﬁlling factor of active regions. the claimed detection of an earth size planet in close orbit to α cen b, although doubtful, open the opportunity to check the existence of star-planet magnetic inter- action (spmi). this could trigger auroral radio emission due to electron cyclotron maser in the stellar corona, similar to the jupiter-io interaction, which is very intense, polarized and highly beamed. however, no hints of spmi has been found. key words: radio continuum: stars – stars: individual: α centauri a b– stars: chromospheres – planet−star interactions – stars: solar-type – techniques: interferometric. 1 "
1803.08341," fast constant factor approximation algorithms are devised for an np- and w[1]- hard problem of intersecting a set of straight line segments with the smallest cardinality set of disks of ﬁxed radii r > 0, where the set of segments forms a straight line drawing g = (v, e) of a planar graph without edge crossings. exploiting tough connection of the problem with the geometric hitting set prob- lem, an(cid:16)50 + 52q 12 ε3 (cid:17) |e|2 log |e|(cid:17)- 13 + ε(cid:17)-approximate o(cid:16)(cid:16)|e|2 + |e| log |e| time and o(cid:16) |e|2 log |e| (cid:17)-space algorithm is given based on the modiﬁed agarwal- pan algorithm. more accurate (34+24√2+ε)-,(cid:0)12 + 6√3 + ε(cid:1)- and(cid:16)34 + 38q 15 19 + ε(cid:17)- approximate algorithms are also proposed for the case where g is any sub- + log |e| ε2 ε graph of either an outerplane or a gabriel graph or a delaunay triangulation respectively, which work within the same time and space complexity bounds, where ε > 0 is an arbitrary small constant. related work only tackles the case where e consists of axis-parallel segments, resulting in an o(|e| log |e|)-time and o(|e| log |e|)-space 8-approximation. keywords: approximation algorithm, geometric hitting set problem, epsilon net, geometric data structure, delaunay triangulation, gabriel graph, line segments ✩this work was supported by russian science foundation, project 14-11-00109. email address: kobylkinks@gmail.com (konstantin kobylkin) url: wwwrus.imm.uran.ru (konstantin kobylkin) preprint submitted to computational geometry: theory and applications may 23, 2018 "
1803.08342,". we study the dual relationship between quantum group convolution maps l1(g) → l∞(g) and completely bounded multipliers of bg. for a large class of locally compact quantum groups g we completely isomorphically identify the mapping ideal of row hilbert space factoriz- able convolution maps with mcb(l1( bg)), yielding a quantum gilbert representation for completely bounded multipliers. we also identify the mapping ideals of completely integral and completely nuclear convolution maps, the latter case coinciding with ℓ1(cbg), where bg is the quantum bohr compactiﬁcation of g. for quantum groups whose dual has bounded degree, we show that the completely compact convolution maps coincide with c(bg). our techniques comprise a mixture of operator space theory and  harmonic analysis, including fubini tensor products, the non- commutative grothendieck inequality, quantum eberlein compactiﬁcations, and a suitable notion of quasi-sin quantum group, which we introduce and exhibit examples from the bicrossed product construction. our main results are new even in the setting of group von neumann algebras v n (g) for quasi-sin locally compact groups g. "
1803.08343,"— cultural adaptation, i.e., the matching of a robot’s behaviours to the cultural norms and preferences of its user, is a well known key requirement for the success of any assistive application. however, culture-dependent robot behaviours are often implicitly set by designers, thus not allowing for an easy and automatic adaptation to different cultures. this paper presents a method for the design of culture-aware robots, that can automatically adapt their behaviour to conform to a given culture. we propose a mapping from cultural factors to related parameters of robot behaviours which relies on linguistic variables to encode heterogeneous cultural factors in a uniform formalism, and on fuzzy rules to encode qualitative relations among multiple variables. we illustrate the approach in two practical case studies. "
1803.08344," we report a possible dipole anisotropy on acceleration scale g† with 147 rotationally supported galaxies in local universe. it is found that a monopole and dipole correction for the radial accel- eration relation can better describe the sparc data set. the monopole term is negligible but the dipole magnitude is signiﬁcant. it is also found that the dipole correction is mostly induced by the anisotropy on the acceleration scale. the magnitude of ˆg†-dipole reaches up to 0.25 ± 0.04, and its direction is aligned to (l, b) = (171.30◦ ±4.87◦), which is very close to the maximum ±7.18◦,−15.41◦ anisotropy direction from the hemisphere comparison method. furthermore, robust check shows that the dipole anisotropy couldn’t be reproduced by isotropic mock data set. however, it is still premature to claim that the universe is anisotropic due to the small data samples and uncertainty in the current observations. 8 1 0 2   y a m 0 3       ] . o c h p - o r t s a [     2 v 4 4 3 8 0 . 3 0 8 1 : v i x r a 1 "
1803.08345,". we establish the mean-ﬁeld convergence for systems of points evolving along the gradient-ﬂow of their interaction energy when the interaction is the coulomb potential or a super-coulombic riesz potential, for the ﬁrst time in arbitrary dimension. the proof is based on a modulated energy method using a coulomb or riesz distance, assumes that the solutions of the limiting equation are regular enough and exploits a weak-strong stability property for them. the method applies as well to conservative and mixed ﬂows. 1.1. problem and background. we study here the large n limit of gradient ﬂow evolutions "
1803.08348," random sequential adsorption (rsa) is a time-dependent packing process, in which particles of certain shapes are randomly and sequentially placed into an empty space without overlap. in the inﬁnite-time limit, the density approaches a “saturation” limit. although this limit has attracted particular research interest, the majority of past studies could only probe this limit by extrapolation. we have previously found an algorithm to reach this limit using ﬁnite computational time for spherical particles, and could thus determine the saturation density of spheres with high accuracy. in this paper, we generalize this algorithm to generate saturated rsa packings of two- dimensional polygons. we also calculate the saturation density for regular polygons of three to ten sides, and obtain results that are consistent with previous, extrapolation-based studies. pacs numbers: 05.10.-a, 45.70.-n, 05.20.-y 8 1 0 2   r p a 0 3       ] t f o s . t a m - d n o c [     2 v 8 4 3 8 0 . 3 0 8 1 : v i x r a ∗ gezhang@alumni.princeton.edu 1 i. "
1803.08349," let mg,n be the moduli space of n-pointed stable genus g curves, and let mg,n be the moduli space of n-pointed smooth curves of genus g. in this paper, we obtain an asymptotic expansion for the characteristic of the free modular operad mv generated by a stable s-modulev, allowing to eﬀectively compute sn-equivariant euler characteristics of mg,n in terms of sn′-equivariant euler characteristics of mg′,n′ with 0≤ g′≤ g, max{0, 3−2g′}≤ n′≤ 2(g−g′)+n. the modular operad mv eﬀective. to illustrate how the asymptotic expansion is used, we give formulas expressing the generating series of the sn-equivariant euler characteristics of mg,n, for g= 0, 1 and 2, in terms of the corresponding this answers a question posed by getzler and kapranov by making their integral representation of the characteristic of generating series associated with mg,n. 8 1 0 2   r a m   2 2     ] . g a h t a m [     1 v 9 4 3 8 0 . 3 0 8 1 : v i x r a contents "
1803.08350," curvature k satisfying (cid:16) λ λ+1(cid:17)2 of closed geodesics is ﬁnite, then there exist [ dim m ] non-hyperbolic closed geodesics. moreover, 2 < k ≤ 1, there exist [ dim m +1 2 ] closed geodesics. if the number there are 3 closed geodesics on (m, f ) satisfying the above pinching condition when dim m = 3. key words: finsler manifolds, closed geodesics, index iteration, morse theory. ams subject classiﬁcation: 53c22, 53c60, 58e10. running head: closed geodesics on finsler manifolds 1 "
1803.08351,". it is known that for a conditional quasi-greedy basis b in a banach space x, the associated sequence (km[b])∞ m=1 of its conditionality constants veriﬁes the estimate km[b] = o(log m) and that if the reverse inequality log m = o(km[b]) holds then x is non-superreﬂexive. indeed, it is known that a quasi-greedy basis in a superreﬂexive quasi-banach space fulﬁls the estimate km[b] = o(log m)1−ǫ for some ǫ > 0. however, in the existing literature one ﬁnds very few instances of spaces possessing quasi- greedy basis with conditionality constants “as large as possible.” our goal in this article is to ﬁll this gap. to that end we enhance and exploit a technique developed by dilworth et al. in [16] and craft a wealth of new examples of both non-superreﬂexive classical banach spaces having quasi-greedy bases b with km[b] = o(log m) and superreﬂexive classical banach spaces having for every ǫ > 0 quasi-greedy bases b with km[b] = o(log m)1−ǫ. moreover, in most cases those bases will be almost greedy. "
1803.08352,". let (x, d, t ) be a topological dynamical system with speci- ﬁcation property. for α ∈ r+ and any x0 ∈ x, deﬁne dx0 α :=nx ∈ x : lim ǫ→0 lim sup n→∞ max{t ∈ n : t n(x) ∈ bt(x0, ǫ)} n ≥ αo. htop(t ) then we have hb 1 + α bowen topological entropy of dx0 α . top(t, dx0 α ) = , where hb top(t, dx0 α ) denotes the keywords and phrases: shadowing time, topological entropy, topological pressure. 8 1 0 2   r a   m 1 2     ] s d h t a m . [     1 v 2 5 3 8 0 . 3 0 8 1 : v i x r a 1 "
1803.08354,". user modeling is a very important task for making relevant suggestions of venues to the users. these suggestions are often based on matching the venues’ features with the users’ preferences, which can be collected from previously visited locations. in this paper, we present a set of relevance scores for making personalized suggestions of points of interest. these scores model each user by focusing on the diﬀerent types of information extracted from venues that they have previously visited. in particular, we focus on scores extracted from social information available on location-based social networks. our experiments, conducted on the dataset of the trec contextual suggestion track, show that social scores are more eﬀective than scores based venues’ content. 1 "
1803.08355," motivated by supervised opinion analysis, we propose a novel framework devoted to structured output learning with abstention (sola). the structure prediction model is able to abstain from predicting some labels in the structured output at a cost chosen by the user in a ﬂexible way. for that purpose, we decompose the problem into the learning of a pair of predictors, one devoted to structured abstention and the other, to struc- tured output prediction. to compare fully la- beled training data with predictions potentially containing abstentions, we deﬁne a wide class of asymmetric abstention-aware losses. learning is achieved by surrogate regression in an appropriate feature space while prediction with abstention is performed by solving a new pre-image problem. thus, sola extends recent ideas about struc- tured output prediction via surrogate problems and calibration theory and enjoys statistical guar- antees on the resulting excess risk. instantiated on a hierarchical abstention-aware loss, sola is shown to be relevant for ﬁne-grained opinion min- ing and gives state-of-the-art results on this task. moreover, the abstention-aware representations can be used to competitively predict user-review ratings based on a sentence-level opinion predic- tor. 8 1 0 2   r a   m 2 2     ] g l . s c [     1 v 5 5 3 8 0 . 3 0 8 1 : v i x r a "
1803.08357," we prove some strong results on sum-product estimates, and expanding polyno- mials over matrix rings m2(fq) over ﬁnite ﬁelds. we also provide similar results in the special linear group sl2(fq). sum-product graphs over both m2(fq) and sl2(fq), and the special unit-graph over m2(fq) play a crucial role in deriving our results. 1 "
1803.08358,". we consider a hamiltonian describing three quantum particles in dimension one inter- acting through two-body short-range potentials. we prove that, as a suitable scale parameter in the potential terms goes to zero, such hamiltonian converges to one with zero-range (also called delta or point) interactions. the convergence is understood in norm resolvent sense. the two-body rescaled σ(xσ) = ε−1vσ(ε−1xσ), where σ = 23, 12, 31 is an index that runs over potentials are of the form vε all the possible pairings of the three particles, xσ is the relative coordinate between two particles, and ε is the scale parameter. the limiting hamiltonian is the one formally obtained by replacing the potentials vσ with ασδσ, where δσ is the dirac delta-distribution centered on the coincidence hyperplane xσ = 0 and ασ =rr vσdxσ. to prove the convergence of the resolvents we make use of faddeev’s equations. keywords: point interactions; three-body hamiltonian; schr¨odinger operators. msc 2010: 81q10; 81q15; 70f07; 46n50. "
1803.08359,"—in typical software, many comparisons and subsequent branch operations are highly critical in terms of security. examples include password checks, signature checks, secure boot, and user privilege checks. for embedded devices, these security-critical branches are a preferred target of fault attacks as a single bit flip or skipping a single instruction can lead to complete access to a system. in the past, numerous redundancy schemes have been proposed in order to provide control- flow-integrity (cfi) and to enable error detection on processed data. however, current countermeasures for general purpose software do not provide protection mechanisms for conditional branches. hence, critical branches are in practice often simply duplicated. we present a generic approach to protect conditional branches, which links an encoding-based comparison result with the redundancy of cfi protection mecha- nisms. the presented approach can be used for all types of data encodings and cfi mechanisms and maintains their error-detection capabilities throughout all steps of a conditional branch. we demonstrate our approach by realizing an encoded comparison based on an-codes, which is a frequently used encoding scheme to detect errors on data during arithmetic operations. we ex- tended the llvm compiler so that standard code and conditional branches can be protected automatically and analyze its security. our design shows that the overhead in terms of size and runtime is lower than state-of-the-art duplication schemes. index terms—control-flow integrity, branch, fault attacks, countermeasures conditional "
1803.08363," exponential distributions appear in a wide range of applications in- cluding chemistry, nuclear physics, time series analyses, and stock market trends. there are conceivable circumstances in which one would be in- terested in the cumulative probability distribution of the sum of some number of exponential variables, with potentially diﬀering constants in their exponents. in this article we present a pedagogical derivation of the cumulative distribution, which reproduces the known formula from power density analyses in the limit that all of the constants are equal, and which assumes no prior knowledge of combinatorics except for some of the properties of a class of symmetric polynomials in n variables (schur polynomials). "
1803.08365," the explanation of the coronal heating problem potentially lies in the existence of nanoﬂares, nu- merous small-scale heating events occuring across the whole solar disk. in this paper, we present the ﬁrst imaging spectroscopy x-ray observations of three quiet sun ﬂares during the nustar solar campaigns on 2016 july 26 and 2017 march 21, concurrent with sdo/aia observations. two of the three events showed time lags of a few minutes between peak x-ray and extreme ultraviolet (euv) emissions. isothermal ﬁts with rather low temperatures in the range 3.2 − 4.1 mk and emission mea- sures of (0.6 − 15) × 1044 cm−3 describe their spectra well, resulting in thermal energies in the range (2 − 6) × 1026 ergs. nustar spectra did not show any signs of a nonthermal or higher temperature component. however, since the estimated upper limits of (hidden) nonthermal energy are comparable to the thermal energy estimates, the lack of a nonthermal component in the observed spectra is not a constraining result. the estimated goes classes from the ﬁtted values of temperature and emission measure fall between 1/1000 and 1/100 a class level, making them 8 orders of magnitude fainter in soft x-ray ﬂux than the largest solar ﬂares. subject headings: sun: ﬂares — sun: particle emission — sun: x-rays "
1803.08368,". this is an expository plus research paper which mainly exposes pre- liminary connection and contrast between classical complex dynamics and semi- group dynamics of holomorphic functions. classically, we expose some existing results of rational and transcendental dynamics and we see how far these results generalized to holomorphic semigroup dynamics as well as we also see what new phenomena occur. "
1803.08371," the proton spin crisis remains an unsolved problem in physics. in this paper we ﬁnd that due to the conﬁnement of partons inside the hadron the angular momentum sum rule in qcd is violated. hence we ﬁnd that the non-vanishing angular momentum ﬂux contribution of the partons in qcd should be added to the spin and angular momentum of the partons to solve the proton spin crisis. pacs numbers: 11.30.-j, 11.30.cp, 11.15.-q, 12.38.-t 8 1 0 2   y a m 9 2       ] h p - p e h [     2 v 1 7 3 8 0 . 3 0 8 1 : v i x r a ∗g. c. nayak was aﬃliated with c. n. yang institute for theoretical physics in 2004-2007. typeset by revtex 1 i. "
1803.08373, having in mind applications to particle physics we develop the diﬀer- ential calculus over jordan algebras and the theory of connections on jordan modules. in particular we focus on diﬀerential calculus over the exceptional jordan algebra and provide a complete characteriza- tion of the theory of connections for free jordan modules. 1 
1803.08374,"—supervised learning frequently boils down to de- termining hidden and bright parameters in a parameterized hypothesis space based on ﬁnite input-output samples. the hid- den parameters determine the attributions of hidden predictors or the nonlinear mechanism of an estimator, while the bright parameters characterize how hidden predictors are linearly combined or the linear mechanism. in traditional learning paradigm, hidden and bright parameters are not distinguished and trained simultaneously in one learning process. such an one-stage learning (osl) brings a beneﬁt of theoretical analysis but suﬀers from the high computational burden. to overcome this diﬃculty, a two-stage learning (tsl) scheme, featured by learning through random assignment for hidden parameters (ltrahp) is developed. ltrahp assigns randomly the hidden parameters in the ﬁrst stage and determines the bright param- eters by solving a linear least square problem in the second stage. although ltrahp works well in many applications, it suﬀers from an uncertainty problem: its performance can only be guaranteed in a certain statistical expectation sense. in this paper we propose a new tsl scheme, learning through deterministic assignment of hidden parameters (ltdahp), where we suggest to deterministically generate the hidden parameters by using minimal riesz energy points on a sphere and equally spaced points in an interval. we theoretically show that with such deterministic assignment of hidden parameters, ltdahp with a neural network realization almost shares the same generalization performance with that of osl, i.e., it does not degrade the generalization capability of osl. thus, ltdahp provides an eﬀective way to overcome both the high computational burden of osl and the uncertainty problem of ltrahp. we present a series of simulations and application examples to support the outperformance of ltdahp, as compared with the typical osl algorithm: support vector regression (svr) and an typical ltrahp algorithm. the study conducted in this paper is a novel trial to tackle supervised learning problems simply and eﬃciently. index terms—supervised learning, neural networks, hidden parameters, bright parameters, learning rate. "
1803.08375," we introduce the use of rectified linear units (relu) as the classifi- cation function in a deep neural network (dnn). conventionally, relu is used as an activation function in dnns, with softmax function as their classification function. however, there have been several studies on using a classification function other than soft- max, and this study is an addition to those. we accomplish this by taking the activation of the penultimate layer hn−1 in a neu- ral network, then multiply it by weight parameters θ to get the raw scores oi. afterwards, we threshold the raw scores oi by 0, i.e. f (o) = max(0, oi), where f (o) is the relu function. we provide class predictions ˆy through arg max function, i.e. arg max f (x). ccs concepts • computing methodologies → supervised learning by clas- sification; neural networks; keywords artificial intelligence; artificial neural networks; classification; con- volutional neural network; deep learning; deep neural networks; feed-forward neural network; machine learning; rectified linear units; softmax; supervised learning "
1803.08376," although several theories for the origin of cosmic rays in the region between the spectral ‘knee’ and ‘ankle’ exist, this problem is still unsolved. a variety of observations suggest that the transition from galactic to extragalactic sources occurs in this energy range. in this work we examine whether a galactic wind which eventually forms a termination shock far outside the galactic plane can contribute as a possible source to the observed ﬂux in the region of interest. previous work by bustard et al. (2017) estimated that particles can be accelerated up to energies above the ‘knee’ up to rmax = 1016 ev for parameters drawn from a model of a milky way wind (everett et al. 2010). a remaining question is whether the accelerated cosmic rays can propagate back into the galaxy. to answer this crucial question, we simulate the propagation of the cosmic rays using the low energy extension of the crpropa framework, based on the solution of the transport equation via stochastic diﬀerential equations. the setup includes all relevant processes, including three-dimensional anisotropic spatial diﬀusion, advection, and corresponding adiabatic cooling. we ﬁnd that, assuming realistic parameters for the shock evolution, a possible galactic termination shock can contribute signiﬁcantly to the energy budget in the ‘knee’ region and above. we estimate the resulting produced neutrino ﬂuxes and ﬁnd them to be below measurements from icecube and limits by km3net. keywords: cosmic rays — propagation of particles — galactic termination shock — neutrinos "
1803.08377,"—this paper deals with the problem of massive random access for gaussian multiple access channel (mac). we continue to investigate the coding scheme for gaussian mac proposed by a. vem et al in 2017. the proposed scheme consists of four parts: (i) the data transmission is partitioned into time slots; (ii) the data, transmitted in each slot, is split into two parts, the ﬁrst one set an interleaver of the low-density parity- check (ldpc) type code and is encoded by spreading sequence or codewords that are designed to be decoded by compressed sensing type decoding; (iii) the another part of transmitted data is encoded by ldpc type code and decoded using a joint message passing decoding algorithm designed for the t-user binary input gaussian mac; (iv) users repeat their codeword in multiple slots. in this paper we are concentrated on the third part of considered scheme. we generalized the pexit charts to optimize the protograph of ldpc code for gaussian mac. the simulation results, obtained at the end of the paper, were analyzed and compared with obtained theoretical bounds and thresholds. obtained simulation results shows that proposed ldpc code constructions have better performance under joint decoding algorithm over gaussian mac than ldpc codes considered by a. vem et al in 2017, that leads to the better performance of overall transmission system. "
1803.08378," a variety of rating-based recommendation methods have been extensively studied including the well-known col- laborative ﬁltering approaches and some network diﬀusion-based methods, however, social trust relations are not suﬃciently considered when making recommendations. in this paper, we contribute to the literature by proposing a trust-based recommendation method, named cosra+t, after integrating the information of trust relations into the resource-redistribution process. speciﬁcally, a tunable parameter is used to scale the resources received by trusted users before the redistribution back to the objects. interestingly, we ﬁnd an optimal scaling parameter for the proposed cosra+t method to achieve its best recommendation accuracy, and the optimal value seems to be universal under several evaluation metrics across diﬀerent datasets. moreover, results of extensive experiments on the two real-world rating datasets with trust relations, epinions and friendfeed, suggest that cosra+t has a remarkable improvement in overall accuracy, diversity and novelty. our work moves a step towards designing better recommendation algorithms by employing multiple resources of social network information. keywords: recommender system, trust relations, vertex similarity, network diﬀusion, complex networks "
1803.08379,". we study the general properties of certain rank 4 rigid local systems con- sidered by goursat. we analyze when they are irreducible, give an explicit integral description as well as the invariant hermitian form h when it exists. by a computer search we ﬁnd what we expect are all irreducible such systems all whose solutions are algebraic functions and give several explicit examples deﬁned over q. we also exhibit one example with inﬁnite monodromy as arising from a family of genus two curves. "
1803.08381," low-complexity non-smooth convex regularizers are routinely used to impose some structure (such as sparsity or low-rank) on the coefﬁcients for linear methods in supervised learning. model con- sistency (selecting the correct structure, for instance support or rank) is known to hold only if some non-degeneracy condition is imposed. this condition typically fails to hold for highly correlated designs and regularization methods tend to select larger models. in this work, we provide the theo- retical underpinning of this behavior using the notion of mirror-stratiﬁable regularizers. this class of regularizers encompasses most well-known ones in the literature including the (cid:96)1 or trace norms. it enjoys a strong primal-dual relation between the models, which in turn allows one to locate the structure of the solution using a speciﬁc dual certiﬁcate. we also show how this analysis is applica- ble not only to solutions of an optimization problem, but also to the iterates computed by a certain class of stochastic proximal-gradient algorithms. keywords: mirror stratiﬁcation, regularization, lasso, sparsity, low-rank. "
1803.08383,"— head-up displays (huds) were the usual viewpoints of designed originally to present at the pilot the main sensor data during aircraft missions, because of placing instrument information in the forward ﬁeld of view enhances pilots’ ability to utilize both instrument and environmental information simultaneously. the ﬁrst civilian motor vehicle had a monochrome hud that was released in 1988 by general motors as a technological improvement of head-down display (hdd) interface, which is commonly used in automobile industry. the hud reduces the number and duration of the drivers sight deviations from the road, by projecting the required information directly into the drivers line of vision. there are many studies about ways of presenting the information: standard one-earpiece presentation, two-earpiece three-dimensional audio presentation, visual only or audio-visual presentation. results [1] have shown that using a 3d auditory display the time of acquiring targets is approximately 2.2 seconds faster than using a one-earpiece way. nevertheless, a disadvantage [2] is when the driver’s attention unconsciously shifts away from the road and goes focused on processing the information presented by the hud. by this reason, the time, the way and the channel are important to represent the information on a hud. a solution is a context aware multimodal proactive recommended system [3] that features personalized content combined with the use of car sensors to determine when the information has to be presented. fig. 1. jaguar head-up display simultaneously [5]. so it is important to have a suitable information presenting model to avoid distractions of drivers attention and to manage the risk level of each indicator. this model has to be able to select the sensorial channel to use. on this paper, we discuss about the applications of a hud on "
1803.08384," magnetic ﬁeld threading a black hole ergosphere is believed to play the key role in both driving the powerful relativistic jets observed in active galactic nuclei and extracting the rotational energy from a black hole via blandford-znajek process. the magnitude of magnetic ﬁeld and the magnetic ﬂux in the vicinity of a central black hole is predicted by theoretical models. on the other hand, the magnetic ﬁeld in a jet can be estimated through measurements of either the core shift effect or the brightness temperature. in both cases the obtained magnetic ﬁeld is in the radiating domain, so its direct application to the calculation of the magnetic ﬂux needs some theoretical assumptions. in this paper we address the issue of estimating the magnetic ﬂux contained in a jet using the measurements of a core shift effect and of a brightness temperature for the jets, directed almost at the observer. the accurate account for the jet transversal structure allow us to express the magnetic ﬂux through the observed values and an unknown rotation rate of magnetic surfaces. if we assume the sources are in a magnetically arrested disk state, the lower limit for the rotation rate can be obtained. on the other hand, the ﬂux estimate may be tested against the total jet power predicted by the electromagnetic energy extraction model. the resultant expression for power depends logarithmically weakly on an unknown rotation rate. we show that the total jet power estimated through the magnetic ﬂux is in good agreement with the observed power. we also obtain the extremely slow rotation rates, which may be an indication that the majority of the sources considered are not in the magnetically arrested disk state. keywords: active galaxies, jets, bl lacertae objects, non-thermal radiation, magnetic ﬂux "
1803.08386,"—we derive sufﬁcient conditions for the solvability of the state estimation problem for a class of nonlinear control time-varying systems which includes those, whose dynamics have triangular structure. the state estimation is exhibited by means of a sequence of functionals approximating the unknown state of the system on a given bounded time interval. more assumptions guarantee solvability of the state estimation problem by means of a hybrid observer. index terms—nonlinear systems, observability, state estimation, hybrid observers. "
1803.08388, we prove that ricci-ﬂat vacuum solutions are stable under linear perturbations in a new class of weakly non-local gravitational theories ﬁnite at the quantum level. we also show that schwarzschild black holes can form by gravitational collapse. 
1803.08389," hydrodynamics with both vector and axial currents is under study within a simple holo- graphic model, consisting of canonical u (1)v × u (1)a gauge ﬁelds in asymptotically ads5 black brane. when gravitational backreaction is taken into account, the chiral electric sepa- ration eﬀect (cese), namely the generation of an axial current as the response to an external electric ﬁeld, is realized naturally. via ﬂuid/gravity calculation, ﬁrst-order transports in the hydrodynamic constitutive relations are evaluated analytically in terms of vector chemical potential µ and axial chemical potential µ5, as well as the temperature t . apart from the proportionality factor µµ5, the cese conductivity is found to depend on the dimensionless quantities µ/t and µ5/t nontrivially. as a complementary study, frequency-dependence of the transports within this holographic model is revealed through the linear response analysis, demonstrating agreement with the results obtained from ﬂuid/gravity calculation. 8 1 0 2   r a   m 2 2     ] h t - p e h [     1 v 9 8 3 8 0 . 3 0 8 1 : v i x r a ∗ yybu@hit.edu.cn † cairg@itp.ac.cn ‡ yangqing@bnu.edu.cn § yunlong.zhang@apctp.org contents "
1803.08390," using more than 6.7 billions of trades, we explore how the tick-by-tick dynamics of limit order books depends on the aggregate actions of large investment funds on a much larger (quarterly) timescale. in particular, we ﬁnd that the well-established long memory of market order signs is markedly weaker when large investment funds trade either in a directional way and even weaker when their aggregate participation ratio is large. conversely, we investigate to what respect a weaker memory of market order signs predicts that an asset is being actively traded by large funds. theoretical arguments suggest two simple mechanisms that contribute to the observed eﬀect: a larger number of active meta-orders and a modiﬁcation of the distribution of size of meta-orders. empirical evidence suggests that the number of active meta-orders is the most important contributor to the loss of market order sign memory. 1 "
1803.08391,". we study various compactiﬁcations of moduli space of newton maps. mainly, we focus on git compactiﬁaction and deligne-mumford compactiﬁcation. then we explore the relations among these compactiﬁcations. for a degree d ≥ 2 monic polynomial p with distinct roots, its newton maps is deﬁned by "
1803.08392," the prevalent interpretation of g¨odel’s second theorem states that a suﬃciently adequate and consistent theory t does not prove its con- sistency. in this paper, we will re-examine the justiﬁcation of this philo- sophical interpretation. detlefsen’s stability problem challenges such a justiﬁcation by requiring that every sentence (in the language of t ) ex- pressing t -consistency has to be shown to be unprovable in t . we will argue that the usual attempts to meet this challenge do not provide a satisfactory solution, since they employ speciﬁc g¨odel numberings, which can be seen as arbitrarily chosen “coordinate systems” in the process of arithmetisation. a satisfactory solution to the stability problem there- fore has to be based on a more general version of g¨odel’s second theorem independent of such a coordinate system. we will propose such a solu- tion by proving the invariance of g¨odel’s second theorem with regard to acceptable numberings. 1 "
1803.08393,". as the frontiers of applied statistics progress through increas- ingly complex experiments we must exploit increasingly sophisticated inferential models to analyze the observations we make. in order to avoid misleading or outright erroneous inferences we then have to be increasingly diligent in scrutinizing the consequences of those modeling assumptions. fortunately model-based methods of statistical inference naturally deﬁne procedures for quantifying the scope of inferential out- comes and calibrating corresponding decision making processes. in this paper i review the construction and implementation of the particular procedures that arise within frequentist and bayesian methodologies. 8 1 0 2   r a   m 2 2     ] e m . t a t s [     1 v 3 9 3 8 0 . 3 0 8 1 : v i x r a michael betancourt is a research scientist at symplectomorphic, llc. (e-mail: betanalpha@gmail.com). 1 2 betancourt contents inference 1 mathematical preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 model-based sensitivities and calibration . . . . . . . . . . . . . . . . . . . . . . 3.1 the model conﬁguration space . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 model-based loss functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 sensitivity and calibration of frequentist inference . . . . . . . . . . . . . . 3.3.1 frequentist inference . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.2 the frequentist calibration criterion . . . . . . . . . . . . . . . . . 3.3.3 frequentist methods in practice . . . . . . . . . . . . . . . . . . . . 3.4 sensitivity and calibration of bayesian inference . . . . . . . . . . . . . . . 3.4.1 bayesian inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.2 the bayesian calibration criterion . . . . . . . . . . . . . . . . . . . 3.4.3 bayesian methods in practice . . . . . . . . . . . . . . . . . . . . . . 3.5 limitations of model-based calibration . . . . . . . . . . . . . . . . . . . . 4 calibrating discovery claims . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1 partitioning the model conﬁguration space . . . . . . . . . . . . . . . . . . 4.2 frequentist null hypothesis signiﬁcance testing . . . . . . . . . . . . . . . 4.2.1 point hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.2 point null hypotheses and complex alternative hypotheses . . . . 4.2.3 complex hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.4 the likelihood ratio test . . . . . . . . . . . . . . . . . . . . . . . . 4.3 bayesian model comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 posterior probability of the region of practical equivalence . . . . . . . . 4.5 predictive scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 applications to limit setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 frequentist limit setting with anchored conﬁdence intervals . . . . . . . . 5.2 bayesian limit setting with posterior quantiles . . . . . . . . . . . . . . . . 6 conclusions and future directions . . . . . . . . . . . . . . . . . . . . . . . . . . 7 acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . references . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 6 6 8 8 9 9 11 13 13 15 16 17 19 19 22 22 23 26 26 27 30 31 33 33 33 34 35 35 model-based calibration 3 as observations and experiments become more sophisticated, and we ask correspondingly more detailed questions about the world around us, we must consider increasingly more complex inferential models. the more complex the model, however, the more subtle the corresponding inferences, and the decisions informed by those inferences, will behave. consequently understanding how inferences and decisions vary across the the many pos- sible realizations of a measurement becomes a critical aspect in the design and preliminary evaluation of new observational eﬀorts. such sensitivity analyses have a long history in the applied sciences but they are often built upon heuristics. fortunately, formal methods of statistical inference naturally admit procedures for understanding and then calibrating the inferential consequences of measurements within the scope of a statistical model. the ex- act mathematical construction of this calibration, and the crucial implementation details, depend critically on the exact form on inference that we consider. in this paper i review how inferential outcomes are formally calibrated within both the frequentist and bayesian perspectives. i discuss not only the procedures but also the conceptual and practical challenges in implementing these procedures in practice, and demonstrate their application towards calibrating traditional discovery and limit setting results. 1. mathematical preliminaries in order to be as explicit as possible when introducing new functions i will use the conventional mathematical notation. a function, f , that maps points x in a space x to points y = f (x) in a space y is denoted f : x → y x (cid:55)→ f (x). rn . the real number line will be denoted r with the n -dimensional real numbers denoted sets of objects are denoted with curly braces, {. . .}, and a vertical line in between braces denotes a selection condition which deﬁnes a set. for example, {x ∈ x | f (x) = 0} deﬁnes the subset of points in x that satisﬁes the condition f (x) = 0. finally, x ∼ π(x) implies that the space x (cid:51) x is endowed with the probability distribution, π. if the left- hand side is decorated with a tilde, ˜x ∼ π(x) then this implies that ˜x is a sample from the probability distribution π. 4 betancourt measurement process latent system being studied space of observations, y true data generating process, π∗ observation, ˜y inferences and decisions fig 1. inference is an inductive process that aims to quantify which phenomenological behaviors of a latent system are consistent with measured observations. formally we assume that the measurement process deﬁnes a space of observations, y , and a probability distribution quantifying the variation in those observations, π∗. any realization of the measurement process results in an observation, ˜y, from which we base our inferences about the latent system and any resulting decisions about how to interact with the latent system. 2. inference ultimately statistics is a tool to learn about the phenomenological behavior of some latent system, for example the internal structure and dynamics of a subatomic particle, the phenotypical encoding of a genome, or the response of a population of individuals to a particular stimulus. although we cannot observe these phenomena directly we can probe them through measurements of the system, or more precisely measurements of how the system interacts with a surrounding environment. these experimental probes can be passive, assembling and analyzing data collected for other purposes, or active, collecting data from dedicated experiments. formally any measurement process deﬁnes a measurement space, y , containing all of the possible realizations of a measurement. these realizations, or observations are inherently stochastic, varying from measurement to measurement. if we assume that this variation is suﬃciently well-behaved, then we can mathematically quantify it with a probability distri- bution over y . i will refer to any probability distribution over the measurement space as a data generating process. under this assumption observations of a given system are modeled as independent samples from some true data generating process, π∗. inference is any procedure that uses observations to inform our understanding of the latent system and its behaviors (figure 1). because of the inherent stochasticity of the measurement process, however, any ﬁnite observation will convey only limited information. consequently inference fundamentally concerns itself with quantifying the uncertainty in this understanding. in particular, there will be many phenomenological behaviors consistent with a given model-based calibration 5 observation and, in general, those behaviors that appear most consistent will vary along with the form of the observation itself. here i will deﬁne the sensitivity of an experiment as the distribution of inferential outcomes induced by the variation in the observations. ideally our inferences would be accurate and capture the true behavior of the latent system regardless of the details of particular observation, but there are no generic guarantees. consequently in practice we must be careful to study these sensitivities with respect to our inferential goals. sensitivity analyses become even more important when we consider decisions informed by our inferences. conventions in many ﬁelds focus not on reporting uncertainties but rather making explicit claims about the latent system being studied. these claims commonly take the form of discovery, where a particular phenomenon is claimed to exist or not exist. in order to limit the possibility that we falsely claim to have discovered the presence, or absence, of a phenomenon we have to carefully consider the sensitivity of these claims. decisions, however, are not always so obvious. even the simple presentation of our inferences requires implicit decisions in the form of how we summarize and communicate our results. to ensure that we are not biasing our audience or ourselves we have to consider how this presentation would vary with the underlying observations. ultimately, in order to ensure robust analyses we have to carefully calibrate the conse- quences of our inferences. first consider a set of actions that we can take, a ∈ a. assuming a perfect characterization of the latent system of interest we could theoretically quantify the relative loss of taking a given action with the true loss function, l∗ : a → r a (cid:55)→ l∗(a). if we convolving this loss function with an inferential decision-making process that maps observations to actions, ˆa : y → a y (cid:55)→ ˆa(y), we induces a true inferential loss function, ˆa : y → r l∗ y (cid:55)→ l∗(ˆa(y)). a sensitivity analysis considers the distribution of l∗ more optimistic readers might also consider the equivalent utility function, u∗(a) ≡ −l∗(a). ˆa(y) with varying observations, y, while a calibration considers expected values of the loss function over the possible obser- vations. for example, we might aim to calibrate an experiment to ensure that the average loss is below a certain value or that a particular quantile of the loss distribution is below a certain value. 6 betancourt π∗ π∗ py (a) m (b) py fig 2. (a) the true data generating process, π∗, belongs in the space of all possible data generating processes over the measurement space, py . (b) model-based inferences consider a model conﬁguration space, m, which contains only a limited subset of all of those possible data generating processes. these analyses, however, require that we know both the true behavior of the latent system, so that we can quantify the relative loss of each action, and the true data generating process, so that we can quantify the variation of possible observations. in practice we don’t know the true nature of the latent system or the true data generating process of the resulting measurements, but we can model them. statistical models quantify the scope of possible data generating processes, allowing us to quantify the sensitivity of inferences and then construct formal calibrations within that scope. 3. model-based sensitivities and calibration inferences that explicitly model the measurement process, or model-based inferences, naturally deﬁne the scope of the possible observations, leaving practitioners to employ their domain expertise to construct relevant loss functions. in this section i review the model conﬁguration space that underlies model-based infer- ence and how the frequentist and bayesian paradigms utilize this space to deﬁne inference and calibrate inferential outcomes. 3.1 the model conﬁguration space a statistical model establishes a model conﬁguration space, or a collection of data gener- ating processes. ideally the model conﬁguration space is designed to be suﬃciently rich to either contain the true data generating process or, more realistically, contain data generat- ing processes suﬃciently similar to the true data generating process within the resolution of our experimental probe (figure 2). each individual data generating process in the model conﬁguration space will be denoted a model conﬁguration. typically the model conﬁguration space admits a parameterization in which each model conﬁguration can be identiﬁed with a parameter value, θ ∈ θ. often these parameters decompose into components, θ = {θ1, . . . , θn} model-based calibration 7 model conﬁguration space, π(y | θ), θ ∈ θ measurement process inferences and decisions latent system being studied space of observations, y true data generating process, π∗ observation, ˜y fig 3. in model based inference the measurement process is modeled with a model conﬁguration space, θ. inferences identify which model conﬁgurations, θ ∈ θ, are consistent with a given observation, which then informs the properties of the true data generating process and the latent system under investigation. with each component responsible for quantifying only one aspect of the phenomenologi- cal behavior of the latent system, the environment containing the latent system, or the measurement process itself. such parameter decompositions make the model conﬁguration space particularly interpretable. moreover, if each model conﬁguration admits a density with respect to a common ref- erence measure over y then we can fully specify the model conﬁguration space with the family of densities π(y | θ) for y ∈ y and θ ∈ θ. in many applied ﬁelds this family of densities, or even the model conﬁguration space itself, is introduced as the likelihood. un- fortunately, that term has a more precise deﬁnition in the statistics literature: the phrase likelihood is used exclusively to denote the function over the parameter space given by evaluating each density at a particular measurement ˜y ∈ y , l˜y : θ → r θ (cid:55)→ π(˜y | θ). consequently i will avoid the use of likelihood unless the model conﬁguration densities are explicitly being evaluated at a given observation. once we have constructed a model conﬁguration space, inference becomes a means of identifying those model conﬁgurations that are consistent with an observation (figure 3). because there is no unique deﬁnition of this sort of consistency, there are multiple approaches to inference. the two employed most in practice are frequentist and bayesian inference. 8 betancourt 3.2 model-based loss functions given a means of quantifying those model conﬁgurations consistent with an observation, we can use that quantiﬁcation to motivate fruitful decisions about how to best interact with the latent system. for example we may want to intervene with the system or systems like it, introducing a treatment or altering the surrounding environment. alternatively we may want to decide on whether or not to claim the discovery of the absence or presence of a new phenomenon, or whether or not to follow up with another experiment. for model-based inference we can quantify the utility of a set of actions by deﬁning a loss function for each possible data generating process in our model, l : a × θ → r (a, θ) (cid:55)→ l(a, θ). given an inferential decision making process ˆa : y → a y (cid:55)→ ˆa(y), we can then deﬁne a model-based inferential loss function, lˆa : y × θ → r (y, θ) (cid:55)→ l(ˆa(y), θ). presuming that the true data generating process is contained within the model conﬁgu- ration space, sensitivity analyses and calibration within the scope of our model quantiﬁes the actual performance of our decisions. when the true data generating process is close to the model conﬁguration space then this process approximates the actual performance. that said, constructing a loss function alone is insuﬃcient to admit sensitivity analyses and calibrations. before the measurement we are ignorant of not only what the observation will be, and hence what action we will take, but also which model conﬁguration gives the true data generating process. in order to quantify the performance of our decisions we have to deﬁne the scope of possible observations and possible data generating processes. exactly how we do that depends intimately on the formal details of the inferences that we make. 3.3 sensitivity and calibration of frequentist inference frequentist inference (casella and berger, 2002; lehmann and casella, 2006; keener, 2011) derives from the interpretation that probability theory can model only the frequencies of repeatable processes. this deﬁnition is consistent with the use of probabilities to model the inherent variation of observations, but it does not allow us to deﬁne probabilities over the model conﬁguration space itself, as those probabilities would not correspond to the hypothetical frequencies of any repeatable process. ultimately this strong philosophical model-based calibration 9 assumption implies that we cannot use any form of weighting to quantify consistency in the model conﬁguration space because any self-consistent weighting is equivalent to the assignment of probabilities! consequently frequentist inference must take the form of deﬁnite decisions about which parts of the model conﬁguration space are consistent with an observation and which are not. from a frequentist perspective inference and decisions are one in the same! because such deﬁnite decisions can readily exclude the true data generating process, or model conﬁgurations close to the true data generating process, from consideration we have to carefully calibrate these decisions so that such exclusions are suﬃciently rare. ultimately frequentist inference does not deﬁne exactly how an observation informs which parts of the model conﬁguration space to keep and which to discard. rather fre- quentist inference establishes a means of calibrating any such procedure that might be considered. 3.3.1 frequentist inference any procedure that consumes an observation to produce a deﬁnite decision about which parts of the model conﬁguration space are considered con- sistent takes the mathematical form of an estimator. estimators are functions from the measurement space to subsets of the model conﬁguration space, mapping observations to subsets of model conﬁgurations, ˆθ : y → t , where t is the space of well-deﬁned subsets of the model conﬁguration space, θ. a common class of estimators are point estimators that identify a single point in the model conﬁguration space (figure 4a), ˆθ : y → θ, point estimators formalize the intuition of a “best ﬁt”, where inferences are summarized with a single point at the expense of ignoring the uncertainty inherent in learning from ﬁnite observations. the more general class of estimators that identify entire subsets of the model conﬁgu- ration space are known as conﬁdence sets (figure 4b), or conﬁdence intervals if the model conﬁguration space is one-dimensional. the nomenclature is meant to suggest that if a conﬁdence set has been properly constructed then we can be conﬁdent that these sets will contain the true data generating process for suﬃciently many observations. 3.3.2 the frequentist calibration criterion the actual choice of which estimator to employ in a given analysis is left to the practitioner. constraints that enforce desired properties can be imposed to restrict the space of potential estimators, but the choice of these desired properties remains the responsibility of the practitioner. regardless of how we ultimately select an estimator, however, we can use the model conﬁguration space to calibrate the estimator and determine it’s practical utility. 10 betancourt π∗ ˆθ(˜y) m py (a) π∗ ˆθ(˜y) m (b) py fig 4. (a) point estimators identify a single model conﬁguration, ˆθ(˜y), that is ideally close to the true data generating process, θ∗, for any given observation ˜y. (b) conﬁdence sets identify entire subsets of the model conﬁguration space that ideally contain the true data generating process for any given observation. in frequentist inference our actions are deﬁnite quantiﬁcations of the model conﬁguration space and, by construction, estimators are inferential decision making processes. in order to deﬁne a calibration criterion we we must ﬁrst construct a model-based loss function, l(ˆθ, θ), that quantiﬁes how well ˆθ identiﬁes the true data generating process, θ. substituting an estimator yields the model-based inferential loss function, lˆθ(y, θ) ≡ l(ˆθ(y), θ). as with estimators there is no canonical loss function in frequentist inference; instead one must be chosen using whatever domain expertise is available within the context of a particular analysis. in practice this choice often considers the structure of the estimator itself. if we knew that a given θ identiﬁed the true data generating process then the sensitivity of the loss of the estimator over the possible observations could be summarized with an expectation over that conﬁguration. this expectation yields an expected loss for each model conﬁguration, (cid:90) y lˆθ(θ) = dy π(y | θ) lˆθ(y, θ). because we don’t have any information about which model conﬁguration identiﬁes the true data generating process before a measurement is made, the frequentist calibration criterion is deﬁned as the maximum expected loss over all possible model conﬁgurations, ¯lˆθ = max θ lˆθ(θ). if the model conﬁguration space is suﬃciently rich that it contains the true data generating process, then this calibration criterion deﬁnes the worst case loss of our given estimator. model-based calibration 11 bounding the worst case loss of an estimator is an extremely powerful guarantee on its practical performance, but also a very conservative one as bounds can be dominated by unrealistic but not impossible data generating processes towards the boundaries of the model conﬁguration space. a natural loss function for point estimators is the lp distance between the estimated model conﬁguration and the presumed true data generating process, (y, θ) = ((θ − ˆθ(y))2)p/2. lp ˆθ the expected lp=2 loss is known as the variance of an estimator. similarly, a natural natural loss function for conﬁdence sets is inclusion of the presumed true data generating process, where the indicator function, i, is deﬁned as lˆθ(y, θ) = iθ[ˆθ(y)], (cid:26) 1, θ ∈ t 0, else . iθ[t ] = the expected inclusion loss, or coverage is simply how often the conﬁdence set contains the presumed true data generating process. while this calibration procedure can used to analyze the frequentist properties of a given estimator, they can also be used to optimize the choice of estimator. given a family of estimators, {ˆθx}, the optimal estimator will satisfy the minimax criterion, ˆθ∗ = argmin x = argmin x = argmin x l∗ ˆθx max θ max θ (cid:90) y lˆθx (θ) dy π(y | θ) lˆθx (y, θ). for example, a desired coverage might be established initially and then a conﬁdence set engineered to ensure that the coverage is met, or exceeded, for all of the data generating processes in the model conﬁguration space. 3.3.3 frequentist methods in practice aside from the conceptual challenge of choosing a loss function that enforces the needs of a given analysis, the computational burden of frequentist calibration is a signiﬁcant impediment to its application. in particular, even approximately scanning through the model conﬁguration space to identify the maximal expected loss often requires more computational resources than realistically available to a practitioner. many frequentist analyses assume suﬃciently simple model conﬁguration spaces, esti- mators, and loss functions such that the maximum expected loss can be computed ana- lytically. the analytic results allow, for example, optimal estimators to be chosen from 12 betancourt families of candidate estimators with strong guarantees on the performance of the best choice. the practical validity of these guarantees, however, requires that the true data generating process be simple enough that it can be contained within the relatively crude model conﬁguration space. for the complex experiments of applied interest this can be a dangerous assumption. without analytic results one might consider interpolative methods that bound the varia- tion in the expected loss between a grid of points distributed across the model conﬁguration space. at each of these points monte carlo methods can be used to simulate observations and approximate the expected loss, and then the properties of the loss function itself can be used to interpolate the expected loss amidst the grid points. these methods can yield reasonable results for low-dimensional model conﬁguration spaces, but as the dimensional- ity of the model increases even strong smoothness assumptions can become insuﬃcient to inform how to interpolate between the grid points. in order to avoid this curse of dimensionality frequentist analyses unaccommodating to analytic results often resort to asymptotics. asymptotic analyses assume that the model conﬁguration space is suﬃciently regular that as we consider more observations at once the behavior of the model conﬁguration space follows a central limit theorem. under these conditions the likelihood for any observation concentrates in an increasingly small neigh- borhood around the maximum likelihood estimator, θml(y) = argmax θ∈θ π(˜y | θ). moreover, in this limit the breadth of that neighborhood is given by the inverse of the fisher information matrix, i(˜y) = ∇2π(˜y | θ)(cid:12)(cid:12)θ=θml(y) . the concentration in the model conﬁguration space in this asymptotic limit admits conve- nient analytic approximations to the frequentist calibration procedure. asymptotic behavior also motivates the concept of proﬁling, which is of use when the parameter space separates into phenomenological parameters related to the underlying system of interest and nuisance or systematic parameters that are unrelated to that sys- tem but still eﬀect the data generating process. under certain conditions the observations inform the nuisance parameters faster than the the phenomenological parameters; in the asymptotic limit the uncertainty in these parameters becomes negligible and they can be replaced with conditional maximum likelihood estimates. more formally, if the parameterization of the model conﬁguration space decomposes into phenomenological parameters, ϑ, and nuisance parameters, σ, then we deﬁne the condi- tional maximum likelihood estimator as ˆσ(ϑ, ˜y) = argmax σ π(˜y | ϑ, σ) model-based calibration 13 and the corresponding proﬁle likelihood as ˆπ(˜y | ϑ) = π(˜y | ϑ, ˆσ(ϑ, ˜y)). the proﬁle likelihood can then be used to calibrate estimators of the phenomenological parameters, at least in this limit. the utility of these asymptotic methods depends critically on the structure of the model conﬁguration space and its behavior as we consider more observations. simpler models typically converge to the asymptotic limit faster and hence require fewer data for asymp- totic calibrations to be reasonably accurate. more complex models, however, converge more slowly and may require more data than is practical, or they may not satisfy the necessary conditions to converge at all. consequently it is crucial to explicitly verify that the asymptotic regime has been reached in a given analysis. as with analytic methods, one has to be especially careful to not employ an over-simplistic model to facilitate the applicability of the asymptotic results while compromising the practical validity of the resulting calibration. 3.4 sensitivity and calibration of bayesian inference bayesian inference (bernardo and smith, 2009; gelman et al., 2014) broadens the inter- pretation of probability theory, allowing it to be used to not only model inherent variation in observations but also provide a probabilistic quantiﬁcation of consistency between the data generating processes in the model conﬁguration space and observations. this generalization manifests in a unique procedure for constructing inferences which can then be used to inform decisions. ultimately bayesian inference decouples inference from decision making, making the assumptions underlying both more explicit and often easier to communicate. moreover, the fully probabilistic treatment of the bayesian perspective immediate deﬁnes a procedure for constructing sensitivities and calibrations. 3.4.1 bayesian inference bayesian inference compliments the data generating processes in the model conﬁguration space with a prior distribution over the model conﬁguration space itself. the prior distribution quantiﬁes any information on which model conﬁgura- tions are closer to the true data generating process than others that is available before a measurement is made. this information can come from, for example, physical considera- tions, previous experiments, or even expert elicitation. careful choices of the prior distribu- tion can go a long way towards regularizing unwelcome behavior of the model conﬁguration space. together the model conﬁguration space and the prior distribution deﬁne the bayesian joint distribution over the measurement space and the parameter space, π(y, θ) = π(y | θ)π(θ). the titular bayes’ theorem conditions this joint distribution on an observation, ˜y, to give 14 betancourt π∗ π∗ m (a) py m (b) py fig 5. (a) bayesian inference begins with a prior distribution, shown here in dark red, over the model conﬁguration space, m, that quantiﬁes information available before a measurement. (b) information encoded in an observation updates the prior distribution into a posterior distribution that ideally concentrates around the true data generating process, π∗. the posterior distribution, π(θ | ˜y) = π(˜y, θ) π(˜y) ∝ π(˜y | θ) π(θ). in words, the prior distribution quantiﬁes information available before the measurement, the model conﬁguration space decodes the information within an observations, and the posterior distribution combines both sources of information to quantify the information about the latent system being studies after a measurement (figure 5). any well-posed statistical query we might make of our system reduces to interroga- tions of the posterior distribution. mathematically this must take the form of a posterior expectation for some function, f , e[f ] = dθ π(θ | ˜y) f (θ). for example, we might consider the posterior mean or median to identify where the pos- terior is concentrating or the posterior standard deviation or the posterior quartiles to quantify the breadth of the distribution. θ posterior expectations also deﬁne a unique decision making process in bayesian inference. first we deﬁne the expected loss for a given action by averaging a model-based loss function, l(a, θ), over the posterior distribution, l(a, ˜y) = dθ π(θ | ˜y)l(a, θ). (cid:90) (cid:90) θ we can then deﬁne a decision making process by taking the action with the smallest expected loss, a∗(˜y) = min a∈a l(a, ˜y). model-based calibration 15 for example, our decision might be to summarize the posterior with a single “best ﬁt” model conﬁguration, ˆθ. given the loss function l(ˆθ, θ) = (ˆθ − θ)2 the expected losses for each possible summary becomes l(ˆθ, ˜y) = (ˆθ − µ(˜y)) + σ2(˜y), where µ(˜y) is the posterior mean and σ(˜y) is the posterior standard deviation. following the bayesian decision making process, our optimal decision is to summarize our posterior by reporting the posterior mean, ˆθ∗(˜y) = µ(˜y). 3.4.2 the bayesian calibration criterion bayes’ theorem provides a unique procedure for constructing inferences and making subsequent decisions given an observation, but there are no guarantees that these decisions will achieve any desired performance for any possi- ble observation. consequently sensitivity analysis and calibration of this decision making process across possible is still important in bayesian inference. instead of having to consider each model conﬁguration equally, however, the prior dis- tribution allows us to diminish the eﬀect of unrealistic but not impossible model conﬁgura- tions. in particular sampling from the joint distribution generates an ensemble of reasonable data generating process and corresponding observations which we can use to quantify the performance of our decisions. for example, we can quantify the sensitivity of any inferential outcome by integrating the model conﬁgurations out of the bayesian joint to give the prior data generating process, (cid:90) (cid:90) π(y) = dθ π(y, θ) = dθ π(y | θ) π(θ). θ θ the prior data generating process probabilistically aggregates the behaviors of all of the possible data generating process in the model conﬁguration space into a single probability distribution over the measurement space. we can then analyze the sensitivity of any in- ferential outcome by running our analysis over an ensemble of observations sampled from this distribution. moreover, we can calibrate a decision making process by integrating a model-based loss function against the full bayesian joint distribution, ¯la = dy dθ lˆa(y, θ)π(θ, y). this calibration immediately quantiﬁes the expected loss as both the observations and data generating processes vary within the scope of our model. y,θ (cid:90) 16 betancourt 3.4.3 bayesian methods in practice the uniﬁed probabilistic treatment of bayesian in- ference ensures that all calculations take the form of expectation values with respect the bayesian joint distribution, its marginals, such as the prior distribution and the prior data generating process, or its conditionals, such as the posterior distribution. consequently calculating expectation values, or more realistically accurately estimating them, is the sole computational burden of bayesian inference. posterior expectations are challenging to compute, and indeed much of the eﬀort on the frontiers of statistical research concerns the development and understanding of ap- proximation methods. one of the most powerful and well-understood of these is markov chain monte carlo (robert and casella, 1999; brooks et al., 2011) and its state of the art implementations like hamiltonian monte carlo (betancourt, 2017). on the other hand, expectations with respect to the bayesian joint distribution are often amenable to much simpler monte carlo methods. in particular, if we can draw exact samples from the prior distribution and each of the data generating processes in the model conﬁguration space then we can generate joint samples with the sequential sampling scheme ˜θ ∼ π(θ) ˜y ∼ π(y | ˜θ). for each simulated observation, ˜y, we can construct a subsequent posterior distribution, make posterior-informed decisions, and then compare those decisions to the simulated truth, ˜θ. as we generate a larger sample from the bayesian joint distribution we can more accurately quantify our sensitivities and calibrations. we can also quantify how sensitivity a calibration is to a particular component of the parameter space, ϑ, by sampling the complementary parameters, σ, from the corresponding conditional prior distribution, ˜σ ∼ π(σ | ϑ) ˜y ∼ π(y | (ϑ, ˜σ)). this allows us, for example, to see how our decision making process behaves as for various phenomenlogical behaviors. interestingly, the application of monte carlo to the bayesian joint distribution is not at all dissimilar to many of the heuristic schemes common in the sciences. sampling ˜y ∼ π(y | ˜θ) just simulates the experiment conditioned on the model conﬁguration, ˜θ. the addition step θ ∼ π(θ) simply simulates model conﬁgurations consistent with the given prior information instead of selecting a few model conﬁguration by hand. one inferential outcome immediately amenable to calibration is the approximation of posterior expectations themselves. cook, gelman and rubin (2006), for example, introduce a natural way to calibrate the estimation of any posterior quantiles. this then immediately provides a procedure for quantifying the accuracy of any algorithm that yields deterministic approximations to posterior quantiles, for example as demonstrated in yao et al. (2018). model-based calibration 17 bayesian sensitivity analysis is particularly useful for identifying known pathologies in bayesian inference by carefully examining the simulated analyses. consider, for example, the posterior z-score for the parameter component, ˜θn, (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) µn(˜y) − ˜θn σn(˜y) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) , zn = where µn(˜y) denotes the posterior mean of ˜θn and σn(˜y) the corresponding posterior stan- dard deviation. the posterior z-score quantiﬁes how much the posterior distribution en- velops the presumed true data generating process along this direction in parameter space. at the same time consider the posterior shrinkage of that parameter component, sn = 1 − σ2 n(˜y) τ 2 n(˜y) , were τn(˜y) is the prior standard deviation of ˜θn. the posterior shrinkage quantiﬁes how much the posterior distribution contracts from the initial prior distribution. an ideal experiment is extremely informative, with large shrinkage for every observation, while also being accurate, with small z-scores for every observation. in this case the distri- bution of posteriors derived from prior predictive observations should concentrate towards small z-scores and large posterior shrinkages for each parameter component. on the other hand, small posterior shrinkage indicates an experiment that poorly identiﬁes the given parameter component, while large z-scores indicates inferences biased away from the true data generating process. we can readily visualize this behavior by plotting the posterior z- score verses the pos- terior shrinkage. concentration to the top right of this plot indicates overﬁtting, while concentration to the top left indicates a poorly-chosen prior that biases the model conﬁgu- ration space away from the presumed true data generating process (figure 6a). because the bayesian joint distribution considers only those true data generating consistent with the prior, however, this latter behavior should be impossible within the scope of a model-based sensitivity analysis. by investigating this simple summary we can quickly identify problems with our ex- perimental design (figure 6b, c). a scatter plot that combines the outcomes for all of the parameters components into one plot ﬁrst summarizes the aggregate performance of the entire model, and then individual plots for each parameter component can be used to isolate the source of any noted pathological behavior. 3.5 limitations of model-based calibration the ultimate limitation of model-based calibration is its dependence on the model con- ﬁguration space. any model-based sensitivities or guarantees claimed by model-based cal- ibrations rely on the model conﬁguration space being rich enough to capture the true data 18 betancourt (a) (b) (c) fig 6. (a) the distribution of posterior z-scores and posterior shrinkages with respect to the bayesian joint distribution identiﬁes well-known pathologies of bayesian inference. the sensitivity of the posterior to the observations within the scope of a given model can be summarized by sampling from the bayesian joint distribution, constructing posteriors, and then plotting the corresponding posterior z-scores and shrinkages. (b) this model demonstrates good behavior for most observations, with a small tail of outcomes that overﬁt to a few observations. (c) this model is weakly identiﬁed, with the information introduced by the data doing little to improve upon the information encoded in the prior distribution. model-based calibration 19 generating process, or at least contain model conﬁgurations that approximate it suﬃciently well. unfortunately it is diﬃcult to quantify how these guarantees might change as the as the model conﬁgurations become worse approximations to the true data generating process. consequently it is up to the user to verify the suﬃciency of the assumed model conﬁguration space with, for example, predictive validations such as residual analysis for frequentist point estimators or posterior predictive checks for bayesian analyses. another point of fragility of model-based sensitivity and calibrations is that they apply only for the exact models and decisions being considered. if those models or decisions are tweaked then the guarantees no longer need apply. the only way to ensure valid calibrations is to recompute them every time the experiment is modiﬁed. an consequence of this fragility is that any sensitivity or calibration is suspect whenever the construction of the model itself depends on the observed data! the only rigorous way to maintain the validity of these results is to consider a larger model that incorporates this implicit dependence of the model conﬁguration space on the observed data. because observations are often used to critique and ultimately tune the model, this vul- nerability is almost impossible to avoid in practice. consequently model-based calibration is perhaps best considered as a tool for identifying poorly inferential behaviors in a model rather than making absolute guarantees about its performance. 4. calibrating discovery claims one of the most common decisions made in the applied sciences is whether or not to claim that a phenomenon in the system being studied exists or doesn’t exist. whether such discovery claims are good scientiﬁc practice is debatable, but given their prevalence it is important to be able to calibrate these decisions regardless. many decision making processes have been developed within in both statistics and ap- plied ﬁelds, and many of these methods have come under recent scrutiny given their failure to replicate in subsequent experiments. the underlying issue in these failed replications is often poor calibration of the original discovery claim. in this section i review how discovery claims can be constructed from a statistical model both in the frequentist and bayesian paradigms and discuss some of the practical issues with their calibration. 4.1 partitioning the model conﬁguration space in order to decide on the presence of a phenomenon we need to partition the model conﬁguration space into those model conﬁgurations that are inﬂuenced by the phenomenon and those that are not. for example we might partition the model conﬁguration space into data generating processes containing only background sources and those containing both signal and background sources. alternatively we might consider a partition into model conﬁgurations where two phenomena manifest distinct behaviors and those where they 20 betancourt behave identically. discovery claims are then informed by which of the two partitions is more consistent with the observed data. let the phenomenon of interest be characterized with a subset of parameters, ϑ = (θ), where  projects the total parameter space onto the phenomenological parameters of interest. additionally assume that the parameters are structured such that ϑ = 0 identiﬁes those data generating processes not inﬂuenced by the phenomenon being considered. in this case the model conﬁguration space partitions into an absence model θ1 = {θ ∈ θ | (θ) = 0} , and a complementary presence model, θ2 = {θ ∈ θ | (θ) (cid:54)= 0} (figure 7). this includes the case where ϑ is constrained to be positive, in which case the presence model reduces to θ > 0, or the more general case where ϑ is unconstrained and the presence model includes all positive and negative, but non-zero, parameters. for example, consider a model where our observations are generated from overlapping signal and background sources, µs and µb respectively, with gaussian measurement vari- ability σ. this yields the model conﬁguration space π(y | µs, µb, σ) = n (y | µs + µb, σ). if we are interested in understanding the signal then we would consider the projection µs = ϑ = (µs, µb, σ), with the absence model deﬁned as µs = 0, regardless of the value of the nuisance parameters µb and σ, and the presence model as the complement with µs (cid:54)= 0. because we don’t know which of the two partitions contains the true data generating process we have to calibrate our decisions with respect to both. in particular we have to consider the probability of claiming a discovery when the true data generating process is in the presence model and when it is in the absence model. we can fully characterize the four possible outcomes with two probabilities: the false discovery rate, fdr and the true discovery rate, tdr (table 1). in classical statistics the false discovery rate is also known as the type i error while one minus the true discovery rate is also known as the type ii error. given these probabilities we can compute the expected loss once we have assigned losses to the possible decision outcomes. for example, let l1 be the loss associated with claiming a discovery when the true data generating process is in the absence model and l2 the possibly negative loss associated with claiming a discovery when the true data generating process is in the presence model. the expected loss for claiming a discovery is then given by lclaim discovery = (1 − fdr) l1 + tdr l2. model-based calibration 21 absence model ϑ = 0 presence model full model ϑ < 0 ϑ = 0 ϑ > 0 ϑ < 0 ϑ > 0 fig 7. when claiming a discovery we ﬁrst identify the phenomenological parameters, ϑ, which quantify the behavior of a phenomena of interest. the remaining nuisance parameters quantify the behavior of the environment in which the phenomenon interacts and the the measurement process itself. when ϑ = 0 identiﬁes a vanishing phenomenon we can partition the full model conﬁguration space into an absence model of those model conﬁgurations satisfying ϑ = 0 and a presence model of those model conﬁgurations satisfying ϑ (cid:54)= 0. discovery is claimed when the presence model is strongly preferred over the absence model by an observation, for various deﬁnitions of “preferred”. truth decision claim θ∗ ∈ θ1 claim θ∗ ∈ θ2 θ∗ ∈ θ1 (no phenomenon) θ∗ ∈ θ2 (phenomenon) fdr 1 − tdr 1 − fdr tdr when preparing to claim a discovery we have to consider the range of inferential outcomes both for when the true data generating process, θ∗, is an element of the absence model, θ∗ ∈ θ1, and when it is an element of the presence model, θ∗ ∈ θ2. given the false discovery rate, fdr, and true discovery rate, tdr, we can compute any expected loss with respect to the claiming a discovery or not. table 1 22 betancourt truth decision claim θ∗ ∈ θ1 claim θ∗ ∈ θ2 claim nothing θ∗ ∈ θ1 (no phenomenon) θ∗ ∈ θ2 (phenomenon) p1 p(cid:48) 1 p2 p(cid:48) 2 p3 p(cid:48) 3 a decision process where we can avoid making any claim about the phenomenon of interest by not publishing our analysis is characterized by six probabilities, four of which are independent because of the normalization constraints,(cid:80)3 n=1 pn = 1 and(cid:80)3 n=1 p(cid:48) n = 1. table 2 in practice we need not limit ourselves to dichotomous decisions. we could also consider a decision process that claims the phenomenon exists, claims the phenomenon doesn’t exist, or makes no claim at all. this process would be characterized by six probabilities, four of which are independent (table 2). importantly the expected loss of a discovery claim can be miscalculated if we ignore the possibility that an analysis may not be reported for some observations and the two additional degrees of freedom needed to quantify the expected loss in this more general circumstance. 4.2 frequentist null hypothesis signiﬁcance testing the conventional approach to claiming discoveries in a frequentist framework is the null hypothesis signiﬁcance testing framework. here the null hypothesis that the true data generating process falls into the absence model is treated as something of a strawman set up to be rejected by observation. in order to reject the null hypothesis we consider how extreme an observation is with respect to the model conﬁgurations in the absence model. the more extreme our rejection threshold the smaller the false discovery rate should be. naively, if the null hypothesis is rejected then we are left with only the alternative hypothesis that the true data generating process falls into the presence model. that said, we can’t simply reject the null hypothesis in isolation — a poor ﬁt to the null hypothesis does not imply that the alternative hypothesis is any more consistent with the observation! at the very least we have to consider also how likely we are to reject the null hypothesis when the alternative hypothesis is true. exactly how the null hypothesis signiﬁcance testing framework is implemented depends on the structure of the null and alternative hypotheses. the procedure is straightforward for simple hypotheses but quickly becomes diﬃcult to implement in practice as the hypotheses become more complex. 4.2.1 point hypotheses the simplest case of null hypothesis signiﬁcance testing is when both the null hypothesis and alternative hypothesis are point hypotheses consisting of a single model conﬁguration each. in this case we’ll denote the lone data generating processes in the absence model πn (y) and the lone data generating process in the precence model πa(y). model-based calibration 23 for a point null hypothesis we can quantify the extremity of an observation, ˜y, with a tail probability or p-value, p(˜y) = dy πn (y). (cid:90) ∞ ˜y the integral might be computed analytically or with numerical methods such as quadrature for low-dimensional measurement spaces and monte carlo for high-dimensional measure- ment spaces. if we reject the null hypothesis when p(˜y) < 1 − α for some signiﬁcance, α, then by construction the false discovery rate of our claim will be (cid:90) (cid:90) 1 y fdr = dy πn (y) i [ p(y) < 1 − α] d˜p i [ ˜p < 1 − α] = = 1 − α. 0 by tuning the signiﬁcance of the null hypothesis test we can immediately achieve whatever false discovery rate is desired in a given application. the true discovery rate, also known as the power of the null hypothesis test, is the average null p-value with respect to the alternative data generating process, (cid:90) tdr = β = 1 − dy πa(y) p(y). y provided that the power is suﬃciently high, observations for which the null hypothesis is rejected will be more consistent with the alternative hypothesis in expectation. there is no guarantee, however, that the alternative will actually be more consistent for every observation. unlike the false discovery rate, the true discovery rate is a consequence of the assumed model and cannot be tuned once a signiﬁcance has been set. consequently unsatisfactorily low true discovery rates can be remedied only by modifying the experimental circumstances, for example by increasing the number of observations included in each measurement. 4.2.2 point null hypotheses and complex alternative hypotheses null hypothesis sig- niﬁcance testing becomes more complicated when the model conﬁguration space no longer consists of just two data generating processes and both hypotheses cannot be point hy- potheses. consider next the situation where the null hypothesis is still given by a single data generating process, θ = 0, and the alternative hypothesis contains the remaining model conﬁgurations θ (cid:54)= 0. this might arise, for example, when our model contains only 24 betancourt one phenomenological parameter and θ = 0 uniquely deﬁnes the circumstance where the phenomenon is absent. as before we can deﬁne the p-value, p(˜y) = (cid:90) ∞ dy π(y | θ = 0), and then reject the null hypothesis when ˜y p(˜y) < 1 − α to ensure a given false discovery rate. unfortunately there is no longer a unique way of deﬁning a power that gives the true discovery rate because the true discovery rate will, in general, be diﬀerent for each of the model conﬁguration in the alternative hypothesis, (cid:90) βθ = 1 − dy π(y | θ) p(y), θ (cid:54)= 0. y if θ is one-dimensional then we can visualize the sensitivity of the p-values as a function of θ (figure 8). when θ is two-dimensional we can no longer visualize the full variation of the sensitivity distribution, but we can visualize the variation of a summary statistic such as the power (figure 9). visualizations allow us to identify regions in the alternative model conﬁguration space of high power, but they do not deﬁne a unique power or true discovery rate for the test. one immediate strategy is to deﬁne an overall power is to consider the minimum power over all of the model conﬁgurations in the alternative hypothesis, β ≡ min θ(cid:54)=0 βθ. provided that we could accurately compute the minimum, this deﬁnition would ensure that the overall power lower bounds the true discovery rate for all model conﬁgurations in the alternative hypothesis. unfortunately, when the null hypothesis is nested within the alternative hypothesis the power can become arbitrarily small for the alternative model conﬁgurations in the neighborhood around the lone null model conﬁguration. consequently in practice we will generally be able to claim calibrated discoveries only for a subset of the data generating processes in the alternative hypothesis. in order to visualize how the power varies with the phenomenological parameters, ϑ, we might also consider deﬁning conditional powers. if the parameters partition into (ϑ, σ), with ϑ ∈ (θ) and σ ∈ (θ)c, then we could deﬁne the conditional power as (cid:90) y βϑ = 1 − max σ∈(θ)c dy π(y | ϑ, σ) p(y). model-based calibration 25 (a) (b) fig 8. when the model conﬁguration space is one-dimensional we can visualize the sensitivity of the null p-values for each of the model conﬁgurations in the alternative hypothesis, θ (cid:54)= 0. this allows us to quickly communicate the distribution of (a) the null p-values, p(y) or the equivalent (b) standard scores z(y) = φ−1(1 − p(y)). here the distributions are summarized with their medians in dark red and with the quartiles in light red. in particular we can identify which θ in the alternative hypothesis achieve a given true discovery rate, here 0.999, by seeing where the central value of the distributions surpass a dashed line. fig 9. when the model conﬁguration space is two-dimensional we can visualize how the power, βθ, varies with the model conﬁgurations in the alternative hypothesis, here (θ1, θ2) (cid:54)= (0, 1). as in the one-dimensional case we can vary the decoration to identify those model conﬁgurations achieving a given true discovery rate, here 0.999, shown in red. 26 betancourt because the optimization of the nuisance parameters, σ, may be infeasible, proﬁle likelihood methods are often utilized to approximate the conditional powers for visualization. as with any application of asymptotics, the validity of this approximations must be carefully veriﬁed for the visualization to be useful. finally we might acknowledge the conceptual advantage of having dual point hypotheses and consider not one null hypothesis test but rather an inﬁnite number of tests, where each tests is deﬁned with respect to the point null hypothesis θ = 0, and one of the alternative model conﬁgurations, θ(cid:48) (cid:54)= 0. when we reject the null hypothesis we reject it for any of the alternatives. the preponderance of alternative hypotheses, however, signiﬁcantly increases the false discovery rate unless we apply a multiple comparison correction to the signiﬁcance threshold. this increase in the false discovery rate by separating the alternative hypothesis into many point hypothesis is also known as the “look elsewhere eﬀect” in particle physics. 4.2.3 complex hypotheses unsurprisingly, implementing null hypothesis signiﬁcance test- ing becomes all the more diﬃcult when neither the null hypothesis nor the alternative hy- pothesis are point hypotheses. given the inﬂuence of systematic and environmental factors present in any experiment we rarely if ever enjoy point hypotheses when using realistic models. when there are multiple model conﬁgurations in the null hypothesis we have to consider them all. for example we might deﬁne the p-value to be the smallest tail probability across all of the null model conﬁgurations, (cid:90) ∞ p(˜y) = min θ∈θ1 ˜y dy π(y | θ). if we then reject the null when this minimal p-value is less than 1−α then the false discovery rate will be at least α for every data generating process in the null hypothesis. power and true discover rate calculations proceed as above, with all of the potential complications. in this general case the computation of the optima needed to bound the false and true discovery rates becomes a particularly signiﬁcant computational burden that must be ad- dressed with a careful combination of principled assumptions and approximations. 4.2.4 the likelihood ratio test one of the diﬃculties with the null hypothesis signiﬁ- cance testing framework presented so far is the need to compute tail probabilities over the measurement space. when the measurement space is more than a few dimensions these tail probabilities are diﬃcult to accurately approximate even with substantial computa- tional resources available. a better strategy is to construct a lower-dimensional summary of the measurement space that captures the diﬀerences between the null and alternative hypotheses while admitting tests that are easier to implement. perhaps the most ubiquitous summary for testing is the likelihood ratio λ(˜y) = maxθ∈θ1 π(˜y | θ) maxθ∈θ π(˜y | θ) , model-based calibration 27 which admits the likelihood ratio test where we reject the null hypothesis if λ(˜y) < λ0 for some 0 < λ0 < 1. the false and true discovery rates of the likelihood ratio test intimately depend on the threshold, λ0, and the particular structure of the model conﬁguration space. consequently without further assumptions the likelihood ratio test has to be explicitly calibrated for every application. the assumption of asymptotics, however, admits an analytic calibration of the likelihood ratio test. wilk’s theorem demonstrates that, under the typical asymptotic conditions, the distribution of the logarithm of the likelihood ratio with respect to the model conﬁgurations in the null hypothesis asymptotically approaches a χ2 distribution, with degrees of freedom, −2 log λ ∼ χ2 k, k = dim(θ) − dim(θ1). consequently the false discovery rate for a given threshold, −2 log λ0, can be calculated by looking up the corresponding χ2 k tail probability. indeed theoretical analysis shows that the likelihood ratio test is the optimal test in this asymptotic regime. many popular tests that have been developed in applied ﬁelds, such as the feldman-cousins test (feldman and cousins, 1998), are actually instances of the likelihood ratio test for speciﬁc classes of models. 4.3 bayesian model comparison bayesian model comparison is an immediate consequence of extending a probabilistic treatment to the absence and presence partitions of the model conﬁguration space. given that it’s not conceptually more diﬃcult, however, let’s consider the more general case where we are interested in selecting between one of n models, {m1, . . . ,mn}. each model conﬁguration space can have diﬀerent dimensions, but integrating the pa- rameters out of the corresponding bayesian joint distribution gives a marginal likelihood over the common measurement space, π(y | mn) = (cid:90) (cid:90) = = eπ(θ|mn)[π(y | θ,mn)]. dθ π(y, θ | mn) dθ π(y | θ,mn) π(θ | mn) θ θ the marginal likelihood is also often known as the bayes factor or evidence in some ﬁelds. given the marginal likelihoods we can construct a joint distribution over the measure- ment and model spaces, π(y,mn) = π(y | mn) π(mn), 28 betancourt from which bayes’ theorem gives the model posteriors, π(mn | ˜y) = π(˜y,mn) π(˜y) = (cid:80)n π(˜y | mn) π(mn) n) π(m(cid:48) n(cid:48)=1 π(˜y | m(cid:48) n) . in particular, given only two models, m1 and m2, we are immediately guided to select the ﬁrst when it exhibits a higher model posterior density, π(m1 | ˜y) > π(m2 | ˜y) (cid:80)n π(˜y | m1) π(m1) n) π(m(cid:48) n(cid:48)=1 π(˜y | m(cid:48) n) (cid:80)n π(˜y | m2) π(m2) n) π(m(cid:48) n(cid:48)=1 π(˜y | m(cid:48) n) π(˜y | m1) π(m1) > π(˜y | m2) π(m2) > π(˜y | m1) π(˜y | m2) > π(m2) π(m1) . in words, we select m1 when the odds ratio, π(˜y | m1) / π(˜y | m2), surpasses a threshold deﬁned by the the relative prior probabilities of the two models, π(m2)/π(m1). interest- ingly this procedure resembles the likelihood ratio test where we use marginal likelihoods instead of maximum likelihoods and the testing threshold is constructed from our prior distributions. calibration of this bayesian model selection then proceeds as with the calibration of any bayesian inference or decision making processs. 1. we ﬁrst sample a true model from the model prior, m˜n ∼ π(mn). 2. then we sample a true model conﬁguration from the subsequent prior distribution, ˜θ ∼ π(θ | m˜n). 3. next we sample an observation from that model conﬁguration, ˜y ∼ π(y | ˜θ,m˜n), 4. finally we calculate the marginal likelihoods to inform model selection, π(˜y | mn), and estimate the corresponding discovery rates for each model, rmn|mn(cid:48) ≈ (cid:93)[ select mn given observation from mn(cid:48)] (cid:93)[ observations from mn(cid:48)] . model-based calibration 29 in particular, if we deﬁne ma as the absence model and mp as the presence model then the false discovery rate is estimated as fdr ≈ (cid:93)[ select mp given observation from ma] (cid:93)[ observations from ma] . with the true discovery rate estimated as tdr ≈ (cid:93)[ select mp given observation from mp ] (cid:93)[ observations from mp ] . in cases like this where there are only a few models being considered it may also be easier to condition on each model and compute the corresponding discovery rates one at a time instead of sampling a model at each iteration. marginal likelihoods and bayesian model selection arise immediately once we consider probabilities over the set of models. unfortunately the theoretical elegance of this approach does not always translate into practical utility. first and foremost the marginal likelihood is extremely challenging to estimate, even in relatively simple problems. the structure of the integral frustrates typical computational tools like markov chain monte carlo and necessitates more complex, and less well estab- lished, tools like nested sampling and simulated tempering. unfortunately these methods are poorly understood relative to the more established tools and consequently their imple- mentations are still limited by our modest understanding. in particular, quantiﬁcation of the accuracy of these methods is typically limited to only heuristics. beyond the computational issues, however, is a more subtle conceptual issue. the marginal likelihood evaluates a model by comparing a given observation to all of the model conﬁg- urations in the model conﬁguration space, each weighted by only the prior distribution. consequently even the smallest details of the prior distribution can signiﬁcantly aﬀect the marginal likelihood. this is in stark contrast to the eﬀect of the prior distribution on the posterior distri- bution. here the likelihood reduces the inﬂuence of model conﬁgurations inconsistent with an observation, obscuring much of the structure of the prior distribution. even seemingly irrelevant details of the prior distribution will still strongly aﬀect the marginal likelihoods, and the practice of constructing prior distributions to ensure only well-behaved posteriors is grossly insuﬃcient for ensuring meaningful marginal likelihoods (gelman, simpson and betancourt, 2017). in practice the sensitivity of the marginal likelihoods, and hence bayesian model selec- tion, to the intricate details of the prior distribution manifests in strong dependencies on the observation and a fragility in the corresponding model selection. small changes in the observation can cause signiﬁcant changes in the marginal likelihoods, with the decision making process rapidly vacillating amongst the possible models. fortunately this behavior will manifest in sensitivity analyses and poor false discovery rates and true discovery rates and so it can be quantiﬁed provided that the test is calibrated! 30 betancourt 4.4 posterior probability of the region of practical equivalence one of the implicit diﬃculties in informing discovery claims as presented so far is that the absence model is singular with respect to the full model conﬁguration space – the absence model conﬁguration space and the presence model conﬁguration spaces are of diﬀerent dimensionality. because of this the posterior probability for all of the model conﬁgurations in the absence will always be zero for a prior that is continuous across the full model conﬁguration space. the only way to admit non-zero posterior probabilities over both models is to assign inﬁnitely more prior probability to those model conﬁgurations in the absence model relative to those in the presence model. bayesian model comparison avoids this issue by comparing only marginal likelihoods and avoiding the individual model posteriors altogether. we can inform a discovery claim using only the posterior over the full model conﬁguration space, however, if we absorb some of the presence model into the absence model. in particular, those model conﬁgurations in the presence model close to those in the absence model will generate nearly identical observations and hence indistinguishable inferences; an inﬁnitesimally weak phenomenon will be impossible to diﬀerentiate from no phenomenon without an impractical amount of data. this suggests that we redeﬁne our absence model as θ1 = {θ ∈ θ | |ϑ(θ)| ≤ ϑ0} , with the presence model becoming θ2 = {θ ∈ θ | |ϑ(θ)| > ϑ0} , for some threshold ϑ0. the neighborhood around the absence model conﬁgurations, |ϑ(θ)| ≤ ϑ0, is known as the region of practical equivalence (kruschke, 2014). notice that separating model conﬁgurations close to ϑ = 0 from the presence model is not entirely dissimilar in what we had to do when considering the power of a complex alternative model in null hypothesis signiﬁcance testing. with this modiﬁcation of the absence model we can then claim a discovery when the posterior probability in the region of practical equivalence is below a given threshold, (cid:90) ϑ0 −ϑ0 dθ π(ϑ | ˜y) < 1 − α, where π(ϑ | ˜y) is the marginal posterior over the phenomenological parameters. i have deﬁned the formal decision making process here to superﬁcially resemble that used in null hypothesis signiﬁcance testing, but we could just as easily use the complementary situation where the posterior probability outside the region of practical equivalence is above the given threshold, (cid:90) −ϑ0 −∞ (cid:90) ∞ ϑ0 dθ π(ϑ | ˜y) + dθ π(ϑ | ˜y) > 1 − α. model-based calibration 31 calibration of this method, in particular the estimation of the false discovery rate and true discovery rate, immediately follows from the bayesian calibration paradigm. 1. we ﬁrst sample a true model conﬁguration from the prior distribution over the model conﬁguration space, ˜θ ∼ π(θ). 2. next we sample an observation from that model conﬁguration, ˜y ∼ π(y | ˜θ), 3. finally we reconstruct the posterior probability of the absence model, (cid:90) ϑ0 −ϑ0 dϑ π(ϑ | ˜y). p = the false discovery rate follows as fdr ≈ (cid:93)[p ≤ 1 − α and |(˜θ)| < ϑ0] (cid:93)[|(˜θ)| < ϑ0] with the true discovery rate, tdr ≈ (cid:93)[p ≤ 1 − α and |(˜θ)| > ϑ0 (cid:93)[|(˜θ)| > ϑ0] , . by sampling from various conditional priors we can also quantify how the false and true discovery rates, or even the distribution of p itself, varies with respect to various parameters. this allows us to visualize the sensitivity of the experiment similar to figure 8, only for arbitrarily complicated models (figure 10). 4.5 predictive scores lastly we can select a model and claim discovery or no discovery by comparing the predictive performance of the possible hypotheses. here we use our inferences to construct a predictive distribution for new data and then select the model whose predictive distribution is closest to the true data generating process. predictive distributions arise naturally in many forms of inference. for example, the model conﬁguration identiﬁed by a frequentist point estimator deﬁnes the predictive dis- tribution bayesian inference immediately yields two predictive distributions: the prior predictive distribution, πp = π(y | ˆθ(˜y)). (cid:90) θ πp = π(y) = dθ π(y | θ) π(θ), 32 betancourt ϑ fig 10. by sampling from a conditional joint distribution, π(σ | ϑ), we can visualize the sensitivity of the distribution of absence model posterior probabilities with respect to a phenomenological parameter, ϑ. the average of this distribution, shown here in dark red, gives the conditional false discovery rate. given a desired false discovery rate, here 10−3, we can then identify for which phenomenological parameters we expect a discovery on average, shown here in grey. and the posterior predictive distribuiton, πp = π(y | ˜y) = (cid:90) θ dθ π(y | θ) π(θ | ˜y). regardless of how a predictive distribution is constructed, it’s similarity to the true data generating process, π∗, is deﬁned by the kullback-leibler divergence, (cid:90) (cid:90) y kl(πp || π∗) = = dy π∗(y) log π∗(y) πp (y) dy π∗(y) log π∗(y) − (cid:90) dy π∗(y) log πp (y). y y because the ﬁrst term is the same for all models, the relative predictive performance be- tween models is quantiﬁed by the predictive score, (cid:90) s = − dy π∗(y) log πp (y). y this expectation with respect to the true data generating process cannot be calcu- lated without already knowing the true data generating process, but predictive scores can be approximated using observations which, by construction, are drawn from that distri- bution. diﬀerent approximation methods combined with various predictive distributions yield a host of predictive model comparison techniques, ranging from cross validation to the akaike information criterion, to the bayesian information criterion, to bayesian cross model-based calibration 33 validation, the widely applicable information criterion, and the deviance information criterion (betancourt, 2015). error in these approximations, however, can be quite large and diﬃcult to quantify in practice, leading to poorly calibrated selection between the absence model and the presence model. consequently it is critically important to estimate their expected false discovery rate and true discovery rate using the assumed model. in the frequentist settings these rates can be quantiﬁed as the minimal performance across the model conﬁgurations in the absence and presence models, where as in the bayesian setting these rates can be quantiﬁed by their expected performance over the bayesian joint distribution. 5. applications to limit setting limit setting is a complement to claiming discovery when the experiment is not expected to be suﬃciently sensitive to the relevant phenomenon. instead of claiming a discovery we consider how strongly we can constrain the magnitude of that phenomenon and calibrate the corresponding constraint with respect to the absence model. because limit setting is derived from standard inference methods, its implementation is signiﬁcantly more straight- forward than the implementation of discovery claims. 5.1 frequentist limit setting with anchored conﬁdence intervals in the frequentist setting we can constrain the magnitude of a phenomenon by construct- ing conﬁdence intervals than span from a vanishing phenomenon to some upper limit. more formally, if the magnitude of the phenomenon is positive, so that the absence model is de- ﬁned by ϑ = 0 and the absence model is deﬁned by ϑ > 0, then we construct an anchored conﬁdence interval of the form(cid:2)0, ˆϑ(y)(cid:1) that has a given coverage, α, with respect to the full model. given an observation, ˜y, we then claim that ϑ < ˆϑ(˜y) with conﬁdence α. the sensitivity of this claim is deﬁned with respect to the possible distribution of ˆϑ(y) with respect to the data generating process in the absence model. for one and two-dimensional absence models the sensitivity can be visualized using the same techniques in section 4.2.2. 5.2 bayesian limit setting with posterior quantiles the frequentist approach to limit setting has an immediate bayesian analogue where we use posterior quantiles to bound the magnitude of the phenomenon. for a given credibility, α, we deﬁne the upper limit, ϑα as ϑα(˜y) =(cid:8)ϑ | (cid:90) ϑ dϑ π(ϑ | ˜y) = 1 − α(cid:9). 0 by deﬁning the limit in terms of the marginal posterior for the phenomenological parame- ters, ϑ, we automatically incorporate the uncertainty in any nuisance parameters into the bound. 34 betancourt the corresponding sensitivity follows by considering the distribution ϑα with respect to the bayesian joint distribution for the absence model. 1. we ﬁrst sample a true model conﬁguration from the absence model by sampling the nuisance parameters from the conditional prior distribution, ˜σ ∼ π(σ | ϑ = 0) ˜θ = (0, ˜σ) . 2. next we sample an observation from that model conﬁguration, ˜y ∼ π(y | ˜θ), 3. finally we compute the inferred upper bound, ϑα(˜y) =(cid:8)ϑ | (cid:90) ϑ dϑ π(ϑ | ˜y) = 1 − α(cid:9). 6. conclusions and future directions 0 both the frequentist and bayesian perspectives admit procedures for analyzing sensitivi- ties and calibrating decision making processes. implementing these calibrations in practice, however, is far from trivial. frequentist calibration requires bounding the expectation of a given loss function over all of the data generating processes in a given model, or partitions thereof. the deriva- tion of analytic bounds from assumptions about the structure of the model conﬁguration space and the loss function, especially those derived from asymptotic analyses, is greatly facilitated with the presumption of simple model conﬁguration spaces. numerical methods for computing the bounds are also aided by simple models. the probabilistic computa- tions required of bayesian calibration are often more straightforward to approximate but suﬃciently complex models will eventually frustrate even the most advanced bayesian computational methods. in practice these computational challenges result in a dangerous tension between models that are simple enough to admit accurate calibrations and models that are complex enough for their resulting calibrations to be relevant to the experiment being analyzed. the continued improvement in computational resources and algorithms has gradually reduced, and promises to continue to reduce, this tension. monte carlo and markov chain monte carlo methods, for example, have revolutionized our ability to compute expected losses and bayesian posterior expectations over high-dimensional measurement and model conﬁguration spaces. unfortunately the applicability of this method critically depends on the desired calibration. in particular, the square root convergence of monte carlo estima- tors is often too slow to ensure accurate calibration of rare observations. this frustrates model-based calibration 35 the calculation, for example, of the o(1 − 10−7) signiﬁcance thresholds presumed in con- temporary particle physics. computational limitations restrain not only the complexity of our models but also the complexity of the loss function we consider. statistics is a constant battle between computational feasibility and compatibility with analysis goals. ultimately it is up to the practitioner to exploit their domain expertise to identify compromises that facilitate high-performance decision making. finally there is the issue of experimental design where we tune the design of an exper- iment to achieve a given performance. as diﬃcult as it is to compute this performance, “inverting” the calibration to identify the optimal experiment is even harder. for com- plex models that don’t admit analytic results, contemporary best practice often reduces to exploring the experiment design space heuristically, guided by computed calibrations and domain expertise. an interesting future direction is the use of automatic diﬀerentiation methods to au- tomatically estimate not only the expected losses but also their gradients with respect to the experimental design. although an imposing implementation challenge, these gradients have the potential to drastically improve the exploration and optimization of experimental designs. 7. acknowledgements i thank lindley winslow, charles margossian, joe formaggio, and dan simpsons for helpful comments and discussions. references bernardo, j.-m. and smith, a. f. m. (2009). bayesian theory. wiley series in probability and mathe- matical statistics: probability and mathematical statistics. john wiley &amp; sons, ltd., chichester. betancourt, m. (2015). a uniﬁed treatment of predictive model comparison. "
1803.08394," iris recognition is used in many applications around the world, with enrollment sizes as large as over one billion persons in india’s aadhaar program. large enrollment sizes can require special optimizations in order to achieve fast database searches. one such optimiza- tion that has been used in some operational scenarios is 1:first search. in this approach, instead of scanning the entire database, the search is terminated when the ﬁrst suﬃciently good match is found. this saves time, but ignores potentially better matches that may exist in the unexamined portion of the enrollments. at least one prominent and successful border-crossing program used this approach for nearly a decade, in order to allow users a fast “token-free” search. our work investigates the search accuracy of 1:first and compares it to the traditional 1:n search. several diﬀerent scenarios are considered trying to emulate real environments as best as possible: a range of enrollment sizes, closed- and open-set conﬁgu- rations, two iris matchers, and diﬀerent permutations of the galleries. results conﬁrm the expected accuracy degradation using 1:first search, and also allow us to identify acceptable working parameters where signiﬁcant search time reduction is achieved, while maintaining accuracy similar to 1:n search. keywords: biometrics, iris recognition, error rates, identiﬁcation, accuracy, search, 1:first, 1:n, open-set "
1803.08395,". research on science ﬁction (sci-ﬁ) in scientiﬁc publications has indicated the usage of sci-ﬁ stories, movies or shows to inspire novel human-computer interaction (hci) research. yet no studies have anal- ysed sci-ﬁ in a top-ranked computer science conference at present. for that reason, we examine the chi main track for the presence and nature of sci-ﬁ referrals in relationship to hci research. we search for six sci-ﬁ terms in a dataset of 5812 chi main proceedings and code the context of 175 sci-ﬁ referrals in 83 papers indexed in the chi main track. in our results, we categorize these papers into ﬁve contemporary hci research themes wherein sci-ﬁ and hci interconnect: 1) theoretical design re- search; 2) new interactions; 3) human-body modiﬁcation or extension; 4) human-robot interaction and artiﬁcial intelligence; and 5) visions of computing and hci. in conclusion, we discuss results and implications located in the promising arena of sci-ﬁ and hci research. keywords: design ﬁction, future visions, hci inspiraton, popular cul- ture in science, science ﬁction 1 "
1803.08396," we propose a new end-to-end single image dehazing method, called densely connected pyramid dehazing net- work (dcpdn), which can jointly learn the transmission map, atmospheric light and dehazing all together. the end- to-end learning is achieved by directly embedding the atmo- spheric scattering model into the network, thereby ensur- ing that the proposed method strictly follows the physics- driven scattering model for dehazing. inspired by the dense network that can maximize the information ﬂow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder struc- ture with multi-level pyramid pooling module for estimat- ing the transmission map. this network is optimized us- ing a newly introduced edge-preserving loss function. to further incorporate the mutual structural information be- tween the estimated transmission map and the dehazed re- sult, we propose a joint-discriminator based on genera- tive adversarial network framework to decide whether the corresponding dehazed image and the estimated transmis- sion map are real or fake. an ablation study is con- ducted to demonstrate the effectiveness of each module evaluated at both estimated transmission map and dehazed result. extensive experiments demonstrate that the pro- posed method achieves signiﬁcant improvements over the state-of-the-art methods. code will be made available at: https://github.com/hezhangsprinter "
1803.08397," let ω ⊂ rn be a bounded domain and δ(x) be the distance of a point x ∈ ω to the boundary. we study the positive solutions δ(x)2 u = up in ω, where p > 0, p (cid:54)= 1 and of the problem ∆u + µ µ ∈ r, µ (cid:54)= 0 is smaller then the hardy constant. the interplay between the singular potential and the nonlinearity leads to interesting structures of the solution sets. in this paper we ﬁrst give the complete picture of the radial solutions in balls. in particular we establish for p−1 at p > 1 the existence of a unique large solution behaving like δ the boundary. in general domains we extend results of [4] and show that there exists a unique singular solutions u such that u/δβ− → c on the boundary for an arbitrary positive function c ∈ c2+γ(∂ω) (γ ∈ (0, 1)), c ≥ 0. here β− is the smaller root of β(β − 1) + µ = 0. − 2 ams subject classiﬁcation: 35j75, 35b09, 35b51, 34b16. key words: elliptic problems, hardy potential, power nonlinearities, dead core and blowup solutions, singular boundary data. 1 "
1803.08399," surface growth, by association or dissociation of material on the boundaries of a body, is ubiquitous in both natural and engineering systems. it is the fundamental mechanism by which biological materials grow, starting from the level of a single cell, and is increasingly applied in engineering processes for fabrication and self-assembly. a signiﬁcant complexity in describing the kinetics of such processes arises due to their inherent coupled interaction with the diﬀusing constituents that are needed to sustain the growth, and the inﬂuence of local stresses on the growth rates. moreover, changes in concentration of solvent within the bulk of the body, generated by diﬀusion, can aﬀect volumetric changes, thus leading to an additional interacting growth mechanism. in this paper we present a general theoretical framework that captures these complexities to describe the kinetics of surface growth while accounting for coupled diﬀusion. then, by combination of analytical and numerical tools, applied to a simple growth geometry, we show that the evolution of such growth processes rapidly tends towards a universal path that is independent of initial conditions. this path, on which surface growth mechanisms and diﬀusion act harmoniously, can be extended to analytically portray the evolution of a body from inception up to a treadmilling state, in which addition and removal of material are balanced. keywords: surface growth, driving force, universal path "
1803.08400," in order to study the performance of interatomic potentials and their reliability at higher pressures, the phase diagram of four different embedded-atom type potential models of iron is compared. the calculations were done by the nested sampling technique in the pressure range 0.1 gpa–100 gpa. the low pressure stable structure is found to be the body- centred cubic in all cases, but the higher pressure phases show a great variation, being face-centred cubic, hexagonal close-packed and – at very low temperatures – different body-centred tetragonal phases are observed as well. the melting line is overestimated considerably for three of the models, but for the one where liquid properties had been taken into account during the potential ﬁtting process, the agreement with experimental results is good, even at very high pressures. keywords: nested sampling, phase diagrams, phase transition prediction, eam models for iron "
1803.08401," the cauchy problem for the complete euler system is in general ill posed in the class of admissible (entropy producing) weak solutions. this suggests there might be sequences of approximate solutions that develop ﬁne scale oscillations. accordingly, the concept of measure–valued solution that capture possible oscillations is more suitable for analysis. we study the convergence of a class of entropy stable ﬁnite volume schemes for the barotropic and complete compressible euler equations in the multidimensional case. we establish suitable stability and consistency estimates and show that the young measure generated by numerical solutions represents a dissipative measure–valued solution of the euler system. here dissi- pative means that a suitable form of the second law of thermodynamics is incorporated in the deﬁnition of the measure–valued solutions. in particular, using the recently established weak-strong uniqueness principle, we show that the numerical solutions converge pointwise to the regular solution of the limit systems at least on the lifespan of the latter. ∗the research of e.f. and h.m. leading to these results has received funding from the european research council under the european union’s seventh framework programme (fp7/2007-2013)/ erc grant agreement 320078. the institute of mathematics of the academy of sciences of the czech republic is supported by rvo:67985840. †the research of m.l. was supported by the german science foundation under the collaborative research centers trr 146 and trr 165. 1 keywords: compressible euler equations, entropy stable ﬁnite volume scheme, entropy sta- bility, convergence, dissipative measure–valued solution ams subject classiﬁcations: 65m08, 76n10, 35l65, 35r06 contents "
1803.08404,. the pursley-sarwate criterion of a pair of ﬁnite complex- valued sequences measures the collective smallness of the aperiodic auto- correlations and the aperiodic crosscorrelations of the two sequences. it is known that this quantity is always at least 1 with equality if and only if the sequence pair is a golay pair. we exhibit pairs of complex-valued sequences whose entries have unit magnitude for which the pursley- sarwate criterion tends to 1 as the sequence length tends to inﬁnity. our constructions use diﬀerent carefully chosen chu sequences. 
1803.08405," detection of power-law behavior and studies of scaling exponents uncover the characteristics of complexity in many real world phenomena. the complexity of ﬁnancial markets has always presented challenging issues and provided interesting ﬁndings, such as the inverse cubic law in the tails of stock price ﬂuctuation distributions. motivated by the rise of novel digital assets based on blockchain technology, we study the distributions of cryptocurrency price ﬂuctuations. we consider bitcoin returns over various time intervals and from multiple digital exchanges, in order to investigate the existence of universal scaling behavior in the tails, and ascertain whether the scaling exponent supports the presence of a ﬁnite second moment. we provide empirical evidence on slowly decaying tails in the distributions of returns over multiple time intervals and diﬀerent exchanges, corresponding to a power-law. we estimate the scaling exponent and ﬁnd an asymptotic power-law behavior with 2 < α < 2.5 suggesting that bitcoin returns, in addition to being more volatile, also exhibit heavier tails than stocks, which are known to be around 3. our results also imply the existence of a ﬁnite second moment, thus providing a fundamental basis for the usage of standard ﬁnancial theories and covariance-based techniques in risk management and portfolio optimization scenarios. keywords: ﬁnancial markets, cryptocurrencies, bitcoin, power-law, scaling "
1803.08407,". we introduce a novel rgb-d patch descriptor designed for detect- ing coplanar surfaces in slam reconstruction. the core of our method is a deep convolutional neural net that takes in rgb, depth, and normal information of a planar patch in an image and outputs a descriptor that can be used to ﬁnd coplanar patches from other images. we train the network on 10 million triplets of coplanar and non-coplanar patches, and evaluate on a new coplanarity benchmark created from commodity rgb-d scans. experiments show that our learned descriptor outperforms alternatives extended for this new task by a signiﬁcant margin. in addition, we demonstrate the beneﬁts of coplanarity matching in a robust rgbd reconstruction formulation. we ﬁnd that coplanarity constraints detected with our method are sufﬁcient to get reconstruction results comparable to state-of-the-art frameworks on most scenes, but outperform other methods on standard bench- marks when combined with a simple keypoint method. keywords: rgb-d registration, co-planarity, loop closure "
1803.08408," 8 1 0 2   r a   m 2 2     ] . o c h t a m [     1 v 8 0 4 8 0 . 3 0 8 1 : v i x r a let g be a graph and t a certain connected subgraph of g. the t -structure con- nectivity κ(g; t ) (or resp., t -substructure connectivity κs(g; t )) of g is the minimum m}) number of a set of subgraphs f = {t1, t2, . . . , tm} (or resp., f = {t (cid:48) such that ti is isomorphic to t (or resp., t i is a connected subgraph of t ) for every 1 ≤ i ≤ m, and f’s removal will disconnect g. the twisted hypercube hn is a new variant of hypercubes with asymptotically optimal diameter introduced by x.d. zhu. in this paper, we will determine both κ(hn; t ) and κs(hn; t ) for t ∈ {k1,r, pk}, respectively, where 3 ≤ r ≤ 4 and 1 ≤ k ≤ n. (cid:48) 2, . . . , t (cid:48) 1, t (cid:48) keywords: twisted hypercube; t -structure connectivity; t -substructure connec- tivity 1 "
1803.08410," in laparoscopic surgery, image quality can be severely de- graded by surgical smoke, which not only introduces error for the image processing (used in image guided surgery), but also reduces the visibility of the surgeons. in this paper, we propose to enhance the laparoscopic images by decomposing them into unwanted smoke part and enhanced part using a variational approach. the proposed method relies on the ob- servation that smoke has low contrast and low inter-channel differences. a cost function is deﬁned based on this prior knowledge and is solved using an augmented lagrangian method. the obtained unwanted smoke component is then subtracted from the original degraded image, resulting in the enhanced image. the obtained quantitative scores in terms of fade, jnbm and re metrics show that our proposed method performs rather well. furthermore, the qualitative visual inspection of the results show that it removes smoke effectively from the laparoscopic images. index terms— laparoscopic images, smoke removal, dehazing, variational, quality. "
1803.08412," inspired by group-based sparse coding, recently proposed group sparsity residual (gsr) scheme has demonstrated superior perfor- mance in image processing. however, one challenge in gsr is to estimate the residual by using a proper reference of the group-based sparse coding (gsc), which is desired to be as close to the truth as possible. previous researches utilized the estimations from other algorithms (i.e., gmm or bm3d), which are either not accurate or too slow. in this paper, we propose to use the non-local sam- ples (nls) as reference in the gsr regime for image denoising, thus termed gsr-nls. more speciﬁcally, we ﬁrst obtain a good estimation of the group sparse coefﬁcients by the image nonlocal self-similarity, and then solve the gsr model by an effective iter- ative shrinkage algorithm. experimental results demonstrate that the proposed gsr-nls not only outperforms many state-of-the-art methods, but also delivers the competitive advantage of speed. index terms— image denoising, group-based sparse coding, group sparsity residual, nonlocal self-similarity, iterative shrinkage algorithm. "
1803.08414," in this paper, we adapt the faster-rcnn framework for the detection of underground buried objects (i.e. hyperbola re- ﬂections) in b-scan ground penetrating radar (gpr) images. due to the lack of real data for training, we propose to in- corporate more simulated radargrams generated from differ- ent conﬁgurations using the gprmax toolbox. our designed cnn is ﬁrst pre-trained on the grayscale cifar-10 database. then, the faster-rcnn framework based on the pre-trained cnn is trained and ﬁne-tuned on both real and simulated gpr data. preliminary detection results show that the pro- posed technique can provide signiﬁcant improvements com- pared to classical computer vision methods and hence be- comes quite promising to deal with this kind of speciﬁc gpr data even with few training samples. index terms— ground penetrating radar (gpr), object detection, deep learning, faster-rcnn "
1803.08415," vehicle-to-infrastructure (v2i) communications are increasingly supporting high- way operations such as electronic toll collection, carpooling, and vehicle platooning. in this paper we study the incentives of strategic misbehavior by individual vehicles who can exploit the security vulnerabilities in v2i communications and impact the highway operations. we consider a v2i-enabled highway segment facing two classes of vehicles (agent populations), each with an authorized access to one server (subset of lanes). vehicles are strategic in that they can misreport their class (type) to the system operator and get unauthorized access to the server dedicated to the other class. this misbehavior causes a congestion externality on the compliant vehicles, and thus, needs to be deterred. we focus on an environment where the operator is able to inspect the vehicles for misbehavior based on their reported types. the inspection is costly and successful detection incurs a ﬁne on the misbehaving vehi- cle. we formulate a signaling game to study the strategic interaction between the vehicle classes and the operator. our equilibrium analysis provides conditions on the cost parameters that govern the vehicles’ incentive to misbehave, and determine the operator’s optimal inspection strategy. index terms: cyber-physical systems security, asymmetric information games, smart highway systems, crime deterrence. 1 "
1803.08416," parametric approaches to learning, such as deep learning (dl), are highly popular in nonlinear regression, in spite of their extremely difﬁcult training with their increasing complexity (e.g. number of layers in dl). in this paper, we present an alternative semi-parametric framework which foregoes the ordinarily required feedback, by introducing the novel idea of geometric regularization. we show that certain deep learning techniques such as residual network (resnet) architecture are closely related to our approach. hence, our technique can be used to analyze these types of deep learn- ing. moreover, we present preliminary results which conﬁrm that our approach can be easily trained to obtain complex structures. index terms— supervised learning, back propagation, geometric approaches "
1803.08417," this thesis addresses questions in representation and invariant theory of ﬁnite groups. the ﬁrst concerns singularities of quotient spaces under actions of ﬁnite groups. we introduce a class of ﬁnite groups such that the quotients have at worst abelian quotient singularities. we prove that supersolvable groups belong to this class and show that nonabelian ﬁnite simple groups do not belong to it. the sec- ond question concerns the cohen-macaulayness of the invariant ring z[x1, . . . , xn]g, where g is a permutation group. we prove that this ring is cohen-macaulay if g is generated by transpositions, double transpositions, and 3-cycles, and conjecture that the converse is true as well. vi contents dedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . list of figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . list of tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . list of symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . "
1803.08419," conversational agents have become ubiquitous, ranging from goal-oriented systems for helping with reserva- tions to chit-chat models found in modern virtual assistants. in this survey paper, we explore this fascinating ﬁeld. we look at some of the pioneering work that deﬁned the ﬁeld and gradually move to the current state-of-the-art models. we look at statistical, neural, generative adversarial network based and reinforcement learning based approaches and how they evolved. along the way we discuss various challenges that the ﬁeld faces, lack of context in utterances, not having a good quantitative metric to compare models, lack of trust in agents because they do not have a consistent persona etc. we structure this paper in a way that answers these pertinent questions and discusses competing approaches to solve them. "
1803.08421," using brst-bv approach, massless and massive continuous-spin ﬁelds propagating in the ﬂat space are studied. for such ﬁelds, brst-bv gauge invariant lagrangian is obtained. the lagrangian and gauge transformations are constructed out of traceless gauge ﬁelds and traceless gauge transformation parameters. interrelation between the brst-bv lagrangian and the la- grangian for the continuous-spin ﬁelds in metric-like approach is demonstrated. considering the brst-bv lagrangian in the siegel gauge, we get gauge-ﬁxed lagrangian which is invariant under global brst and antibrst transformations. keywords: continuous-spin ﬁeld; brst-bv approach; higher-spin ﬁeld. 8 1 0 2   n u j   4     ] h t - p e h [     2 v 1 2 4 8 0 . 3 0 8 1 : v i x r a ∗ e-mail: metsaev@lpi.ru 1 1 "
1803.08422," one important problem in the theory of hardy space is to ﬁnd the best rational approxi- mation of a given order to a function in the hardy space h 2 on the unit disk. it is equivalent to ﬁnding the best blaschke form with free poles. the cyclic adaptive fourier decomposition method is based on the grid search technique. its approximative precision is limited by the grid spacing. this paper propose an enhancement of the cyclic adaptive fourier decomposition. the new method ﬁrst changes the rectangular grids to the polar ones for cyclic adaptive fourier decomposition, so that the decomposition enables us to employ fast fourier transformation to accelerate the search speed. furthermore, the proposed algorithm utilizes the gradient descent optimization to tune the best pole-tuple on the mesh grids, reaching higher precision with less computation cost. its validity and eﬀectiveness are conﬁrmed by several examples. key words. best rational approximation, blaschke products, hardy space, gradient descent method, fast fourier transform 2010 ams subject classiﬁcations. 41a20, 30h10, 30j10, 65t50 "
1803.08428, we show that there are no smooth warped ads3 solutions in 10- and 11-dimensional supergravities which preserve strictly more than 16 supersymmetries and have in- ternal space a compact without boundary manifold. 1 1 
1803.08429," the nele group, formerly known as the iannini family, is one of the youngest asteroid families in the main belt. previously, it has been noted that the pericenter longi- tudes ̟ and nodal longitudes ω of its largest member asteroids are clustered at the present time, therefore suggesting that the collisional breakup of parent body must have happened recently. here we verify this conclusion by detailed orbit-propagation of a synthetic nele family and show that the current level of clustering of secular an- gles of the largest nele family members requires an approximate age limit of 4.5 myr. additionally, we make use of an updated and largely extended nele membership to obtain, for the ﬁrst time, an age estimate of this family using the backward integration method (bim). convergence of the secular angles in a purely gravitational model and in a model including the non-gravitational forces caused by the yarkovsky eﬀect are both compatible with an age younger than 7 myr. more accurate determination of the nele family age would require additional data about the spin state of its members. key words: minor planets, asteroids: general – minor planets, asteroids: individual: nele–celestial mechanics. 1 "
1803.08430,". in this article, we focus on the left translation actions on non- commutative compact connected lie groups whose topological dimensions are 3 or 4, consisting of su(2), u(2), so(3), so(3)×s 1 and spinc(3). (the relevant conclusions of the right translation actions on these lie groups are the same as left translation actions.) we deﬁne the rotation vectors of the left actions in- duced by the elements in the maximal tori on these ﬁve non-commutative com- pact connected lie groups, and utilize the rotation vectors deﬁned to give the topologically conjugate classiﬁcations of these left actions. as a by-product, we show that for any homeomorphism f : l(p, 1) × s 1 → l(p, 1) × s 1, the induced isomorphism (π ◦ f ◦ i)∗ maps each element in the fundamental group of l(p, 1) to itself or its inverse, where i : l(p, 1) → l(p, 1) × s 1 is the natural inclusion and π : l(p, 1) × s 1 → l(p, 1) is the projection. furthermore, for the left actions on these ﬁve lie groups, we study the relationships between their topologically conjugate classiﬁcations and their algebraically conjugate classiﬁcations as well as their smooth conjugate classiﬁcations. "
1803.08432,". we introduce several parallel algorithms operating on a distributed forest of adaptive quadtrees/octrees. they are targeted at large-scale applications relying on data layouts that are more complex than required for standard ﬁnite elements. such applications appear in various contexts, examples being the hp-adaptive discontinuous galerkin method, element-based particle tracking, and in-situ post-processing and visualization. speciﬁcally, we design algorithms to derive an adapted worker forest based on sparse data, to identify owner processes in a top-down search of remote objects, and to allow for variable process counts and per-element data sizes in partitioning and parallel ﬁle i/o. we demonstrate the algorithms’ usability and performance in the context of a particle tracking example that we scale to 21e9 particles and 64ki mpi processes on the juqueen supercomputer. key words. parallel algorithms, adaptive mesh reﬁnement, forest of octrees, particle tracking ams subject classiﬁcations. 65d18, 65m50, 65y05, 68w10 "
1803.08433," currently, accurate localization system based on com- modity wifi devices is not broadly available yet. in the literature, the solutions are based on either network in- frastructure like wifi router, which have at least three antennas, or sacriﬁce accuracy with coarse-grained in- formation like rssi. in this work, we design a new lo- calizing system - dyloc which is accurate based on aoa estimation and instantly deployable on users’ devices. dyloc is designed to be dynamically constructed with user’s devices as network nodes without any network infrastructure. on the platform of laptops, our system achieve comparable localization accuracy with state-of- the-art work despite of the limitation of less number and large separation of antennas. we design multi-stage signal processing to resolve the ambiguity issue arisen in this scenario. to enable dynamic and collaborative construction, our system can accurately conduct self- localization and also eliminate the need of infrastruc- ture anchors, which is due to the dedicated two-layer algorithm design. 1. "
1803.08434," deep insights into the possible infrared dynamics of strongly-coupled nonabelian gauge theories such as qcd come from the analyses of n = 1 or n = 2 super- symmetric gauge theories. central in the whole discussion will be the topological soliton monopoles and vortices and their quantum dynamics. we review the argu- ments that nonabelian monopoles, free from the classic ”diﬃculties”, can be deﬁned semi-classically via the topology and stability connection to the better understood nonabelian vortices. recent results on cp n−1 models on 2d worldsheet of ﬁnite width, establish the quantum mechanical nature of such nonabelian monopoles. an interesting class of rg ﬂows and emergence of conﬁning vacua ”nearby” strongly- coupled infrared-ﬁxed point (irfp) conformal theories are discussed in the context of most singular vacua in n = 2 supersymmetric qcd. certain analogy with the real-world qcd is drawn. in many systems, color-ﬂavor locking emerges as a crucial mechanism for the gauge system to avoid dynamical abelianization. ∗cp 3-origins distinguished lecture, university of southern denmark, odense, 12 march 2018 1 contents "
1803.08435,". deep generative models have shown success in automatically synthesizing missing image regions using surrounding context. however, users cannot directly decide what content to synthesize with such ap- proaches. we propose an end-to-end network for image inpainting that uses a diﬀerent image to guide the synthesis of new content to ﬁll the hole. a key challenge addressed by our approach is synthesizing new con- tent in regions where the guidance image and the context of the original image are inconsistent. we conduct four studies that demonstrate our results yield more realistic image inpainting results over seven baselines. "
1803.08436," in this paper we prove the liouville type theorem for stable at inﬁnity solutions of the following equation for 1 < m − 1 < θ < θs,m := n(m−1)+3m n−3m . here θs,m is a the classic critical exponent for m− bi-harmonic equation. keywords: liouville type theorem, stable solutions, m- tri-harmonic equations, critical exponents. ∆3 mu = |u|θ−1u in rn ,     ] p a h t a m . [     1 v 6 3 4 8 0 . 3 0 8 1 : v i x r a "
1803.08437,". we compute the ´etale cohomology ring h ∗(spec ok , z/pz) for p a prime number, where ok is the ring of integers of a number ﬁeld k. as an application, we give a non-vanishing formula for an invariant deﬁned by minhyong kim. contents "
1803.08438,". let md,n(q) denote the number of monic irreducible polynomials in fq[x1, x2, . . . , xn] of degree d. we show that for a ﬁxed degree d, the se- quence md,n(q) converges q-adically to an explicitly determined rational func- tion md,∞(q). furthermore we show that the limit md,∞(q) is related to the classic necklace polynomial md,1(q) by an involutive functional equation, lead- ing to a phenomenon we call liminal reciprocity. the limiting ﬁrst moments of factorization statistics for squarefree polynomials are expressed in terms of a family of symmetric group representations as a consequence of liminal reci- procity. "
1803.08441,". it has been shown that self-assembled chains of active colloidal particles can present sustained oscillations. these oscillations are possible because of the eﬀective diﬀusiophoretic forces that mediate the interactions of colloids do not respect the action–reaction principle and hence, a hopf bifurcation is possible even for overdamped dynamics. anchoring the particles in one extreme breaks the head-tail symmetry and the oscillation is transformed into a traveling wave pattern and thus the chain behaves like a beating cilium. the net force on the anchor, estimated using the resistive force theory, vanishes before the bifurcation and thereafter grows linearly with the bifurcation parameter. if the mobilities of the particles on one extreme are reduced to mimic an elongated cargo, the traveling wave generates a net velocity on the chain that now behaves like a moving ﬂagellum. the average velocity again grows linearly with the bifurcation parameter. our results demonstrate that simpliﬁed systems, consisting only of a few particles with non-reciprocal interaction and head- tail asymmetry show beating motion and self-propulsion. both properties are present in many non-equilibrium models thus making our results a general feature of active matter. submitted to: new j. phys. 8 1 0 2   r a   m 2 2     ] t f o s . t a m - d n o c [     1 v 1 4 4 8 0 . 3 0 8 1 : v i x r a active colloidal chains with cilia- and ﬂagella-like motion 2 "
1803.08444," statistical techniques play a large role in the structural health monitoring of instrumented infras- tructure, such as a railway bridge constructed with an integrated network of ﬁbre optic sensors. one possible way to reason about the structural health of such a railway bridge, is to model the time it takes to recover to a no-load (baseline) state after a train passes over. inherently, this recovery time is random and should be modelled statistically. this paper uses a non-parametric model, based on empirical quantile approximations, to construct a space-memory eﬃcient baseline distribution for the streaming data from these sensors. a fast statistical test is implemented to detect deviations away from, and recovery back to, this distribution when trains pass over the bridge, yielding a recovery time. our method assumes that there are no temporal variations in the data. a median-based detrending scheme is used to remove the temporal variations likely due to temperature changes. this allows for the continuous recording of sensor data with a space-memory constraint. 1 "
1803.08446," context. observations of molecular gas have played a key role in developing the current understanding of the late stages of stellar evolution. aims. the survey planetary nebulae and their co reservoir with apex (pandora) was designed to study the circumstellar shells of evolved stars with the aim to estimate their physical parameters. methods. millimetre carbon monoxide (co) emission is the most useful probe of the warm molecular component ejected by low- to intermediate-mass stars. co is the second-most abundant molecule in the universe, and the millimetre transitions are easily excited, thus making it particularly useful to study the mass, structure, and kinematics of the molecular gas. we present a large survey of the co (j = 3 − 2) line using the atacama pathﬁnder experiment (apex) telescope in a sample of 93 proto-planetary nebulae and planetary nebulae. results. co (j = 3 − 2) was detected in 21 of the 93 objects. only two objects (irc+10216 and pn m 2-9) had previous co (j = 3 − 2) detections, therefore we present the ﬁrst detection of co (j = 3 − 2) in the following 19 objects: frosty leo, hd 101584, iras 19475+3119, pn m1-11, v* v852 cen, ic 4406, hen 2-113, hen 2-133, pn fg 3, pn cn 3-1, pn m 2-43, pn m1-63, pn m1-65, bd +30 3639, hen 2-447, hen 2-459, pn m3-35, ngc 3132, and ngc 6326. conclusions. co (j = 3− 2) was detected in all 4 observed ppne (100%), 15 of the 75 pne (20%), one of the 4 wide binaries (25%), and in 1 of the 10 close binaries (10%). using the co (j = 3 − 2) line, we estimated the column density and mass of each source. the h2 column density ranges from 1.7x1018 to 4.2x1021cm−2 and the molecular mass ranges from 2.7x10−4 to 1.7x10−1m(cid:12). key words. line: identiﬁcation – molecular data – catalogs – (ism:) planetary nebulae: general "
1803.08447," we use gaussian processes in combination with mcmc method to place constraints on cosmological parameters of three dark energy models including ﬂat and curved frw and bianchi type i spacetimes. recently compiled 36 measurements of the hubble parameter h(z) in the redshifts intermediate 0.07 6 z 6 2.36 have been used in our nonparametric reconstruction of the cosmological parameters. moreover, we use these models to estimate the redshift of the deceleration-acceleration transition. we consider two gaussian priors for current value of the hubble constant i.e h0 = 73 ± 1.74(68 ± 2.8) km/s/mpc to to investigate the eﬀect of the assumed h0 on our parameters estimations. for statistical analysis we use nuts sampler which is an extension of hamiltonian monte carlo algorithm to generate mcmc chains for parameters of dark energy models. in general, when we compared our results with 9 years wmap as well as planck 2016 collaboration, we found that bianchi type i model is slightly ﬁts better to the observational hubble data with respect to the ﬂat frw model. key words: bianchi type i – dark energy – hubble rate "
1803.08448," edge computing caters to a wide range of use cases from la- tency sensitive to bandwidth constrained applications. how- ever, the exact specifications of the edge that give the most benefit for each type of application are still unclear. we inves- tigate the concrete conditions when the edge is feasible, i.e., when users observe performance gains from the edge while costs remain low for the providers, for an application that re- quires both low latency and high bandwidth: video analytics. keywords edge computing, distributed systems "
1803.08449,"— the indirect approach to continuous-time system identiﬁcation consists in estimating continuous-time models by ﬁrst determining an appropriate discrete-time model. for a zero-order hold sampling mechanism, this approach usually leads to a transfer function estimate with relative degree 1, independent of the relative degree of the strictly proper real sys- tem. in this paper, a reﬁnement of these methods is developed. inspired by indirect pem, we propose a method that enforces a ﬁxed relative degree in the continuous-time transfer function estimate, and show that the resulting estimator is consistent and asymptotically efﬁcient. extensive numerical simulations are put forward to show the performance of this estimator when contrasted with other indirect and direct methods for continuous-time system identiﬁcation. index terms— system identiﬁcation; continuous-time sys- tems; parameter estimation; sampled data. "
1803.08450,"—deep learning revolutionized data science, and recently, its popularity has grown exponentially, as did the amount of papers employing deep networks. vision tasks such as human pose estimation did not escape this methodological change. the large number of deep architectures lead to a plethora of methods that are evaluated under different experimental protocols. moreover, small changes in the architecture of the network, or in the data pre-processing procedure, together with the stochastic nature of the optimization methods, lead to notably different results, making extremely difﬁcult to sift methods that signiﬁcantly outperform others. therefore, when proposing regression algorithms, practitioners proceed by trial-and-error. this situation motivated the current study, in which we perform a systematic evaluation and a statistical analysis of the performance of vanilla deep regression – short for convolutional neural networks with a linear regression top layer –. up to our knowledge this is the ﬁrst comprehensive analysis of deep regression techniques. we perform experiments on three vision problems and report conﬁdence intervals for the median performance as well as the statistical signiﬁcance of the results, if any. surprisingly, the variability due to different data pre-processing procedures generally eclipses the variability due to modiﬁcations in the network architecture. index terms—deep learning, regression, computer vision, convolutional neural networks, statistical signiﬁcance, empirical and systematic evaluation, head-pose estimation, full-body pose estimation, facial landmark detection. ! "
1803.08452," we demonstrate a method of associating the principal symbol at a k-point with a linear diﬀerential operator acting between modules over a commutative algebra, and we use it to deﬁne the ellipticity of a linear diﬀerential operator in a purely algebraic way. we prove that the ellipticity is preserved by a surjective homomorphism of algebras. as an example, we show that for every real aﬃne variety there is an elliptic linear diﬀerential operator acting on the algebra of regular functions on this variety. keywords: elliptic operators, k-points, principal symbol, universal derivations mathematics subject classiﬁcation (2010): 13n05, 13n15, 58j05 0 "
1803.08455," it has been recently argued that an embedding of the sm into a consistent theory of quantum gravity may imply important constraints on the mass of the lightest neutrino and the cosmological constant λ4. the constraints come from imposing the absence of any non-susy ads stable vacua obtained from any consistent compactiﬁcation of the sm to 3 or 2 dimensions. this condition comes as a corollary of a recent extension of the weak gravity conjecture (wgc) by ooguri and vafa. in this paper we study t 2/zn compactiﬁcations of the sm to two dimensions in which sm wilson lines are projected out, leading to a considerable simpliﬁcation. we analyze in detail a t 2/z4 compactiﬁcation of the sm in which both complex structure and wilson line scalars are ﬁxed and the potential is only a function of the area of the torus a2. we ﬁnd that the sm is not robust against the appearance of ads vacua in 2d and hence would be by itself inconsistent with quantum gravity. on the contrary, if the sm is embedded at some scale mss into a susy version like the mssm, the ads vacua present in the non-susy case disappear or become unstable. this means that wgc arguments favor a susy version of the sm, independently of the usual in a t 2/z4 compactiﬁcation in which the orbifold hierarchy problem arguments. action is embedded into the b − l symmetry the bounds on neutrino masses and the cosmological constant are recovered. this suggests that the mssm should be extended with a u (1)b−l gauge group. in other families of vacua the spectrum of susy particles is further constrained in order to avoid the appearance of new ads vacua or instabilities. we discuss a possible understanding of the little hierarchy problem in this context. contents "
1803.08456," deep reinforcement learning has been successfully applied to several visual-input tasks using model-free methods. in this paper, we propose a model-based approach that combines learning a dnn-based transition model with monte carlo tree search to solve a block-placing task in minecraft. our learned transition model predicts the next frame and the rewards one step ahead given the last four frames of the agent’s ﬁrst-person-view image and the current action. then a monte carlo tree search algorithm uses this model to plan the best sequence of actions for the agent to perform. on the proposed task in minecraft, our model-based approach reaches the performance comparable to the deep q-network’s, but learns faster and, thus, is more training sample efﬁcient. keywords: reinforcement learning, model-based reinforcement learning, deep learning, model learning, monte carlo tree search acknowledgements i would like to express my sincere gratitude to my supervisor dr. stefan uhlich for his continuous support, patience, and immense knowledge that helped me a lot during this study. my thanks and appreciation also go to my colleague anna konobelkina for insightful comments on the paper as well as to sony europe limited for providing the resources for this project. 8 1 0 2   r a   m 2 2     ] i a . s c [     1 v 6 5 4 8 0 . 3 0 8 1 : v i x r a s0s0 (cid:98)s1(cid:98)s1 t = 3 (cid:98)s2(cid:98)s2 (cid:98)s3(cid:98)s3 (cid:98)s1(cid:98)s1 (cid:98)s2(cid:98)s2 t = 4 (cid:98)s3(cid:98)s3 (cid:98)s4(cid:98)s4 a3 = “turn right” a4 = “place block” dnn model (cid:98)r5 (cid:98)s4(cid:98)s4 dnn model (cid:98)r6 (cid:98)s5(cid:98)s5 spectator view at time step t = 3 spectator view at time step t = 5 figure 1: transition model for the block-placing task in minecraft "
1803.08457,". recently, there has been increasing interest to leverage the compe- tence of neural networks to analyze data. in particular, new clustering methods that employ deep embeddings have been presented. in this paper, we depart from centroid-based models and suggest a new framework, called clustering-driven deep embedding with pairwise constraints (cpac), for non-parametric cluster- ing using a neural network. we present a clustering-driven embedding based on a siamese network that encourages pairs of data points to output similar represen- tations in the latent space. our pair-based model allows augmenting the informa- tion with labeled pairs to constitute a semi-supervised framework. our approach is based on analyzing the losses associated with each pair to reﬁne the set of con- straints. we show that clustering performance increases when using this scheme, even with a limited amount of user queries. we present state-of-the-art results on different types of datasets and compare our performance to parametric and non-parametric techniques. "
1803.08460," unseen action recognition (uar) aims to recognise novel action categories without training examples. while previous methods focus on inner-dataset seen/unseen splits, this paper proposes a pipeline using a large-scale train- ing source to achieve a universal representation (ur) that can generalise to a more realistic cross-dataset uar (cd- uar) scenario. we ﬁrst address uar as a generalised multiple-instance learning (gmil) problem and discover ‘building-blocks’ from the large-scale activitynet dataset using distribution kernels. essential visual and semantic components are preserved in a shared space to achieve the ur that can efﬁciently generalise to new datasets. pre- dicted ur exemplars can be improved by a simple se- mantic adaptation, and then an unseen action can be di- rectly recognised using ur during the test. without fur- ther training, extensive experiments manifest signiﬁcant im- provements over the ucf101 and hmdb51 benchmarks. "
1803.08461,". weak lensing peak counts are a powerful statistical tool for constraining cos- mological parameters. so far, this method has been applied only to surveys with relatively small areas, up to several hundred square degrees. as future surveys will provide weak lensing datasets with size of thousands of square degrees, the demand on the theoretical prediction of the peak statistics will become heightened. in particular, large simulations of increased cosmological volume are required. in this work, we investigate the possibility of using simu- lations generated with the fast comoving-lagrangian acceleration (cola) method, coupled to the convergence map generator ufalcon, for predicting the peak counts. we examine the systematics introduced by the cola method by comparing it with a full treepm code. we ﬁnd that for a 2000 deg2 survey, the systematic error is much smaller than the statistical error. this suggests that the cola method is able to generate promising theoretical predictions for weak lensing peaks. we also examine the constraining power of various conﬁgurations of data vectors, exploring the inﬂuence of splitting the sample into tomographic bins and combining diﬀerent smoothing scales. we ﬁnd the combination of smoothing scales to have the most constraining power, improving the constraints on the s8 amplitude parameter by at least 40% compared to a single smoothing scale, with tomography brining only limited increase in measurement precision. 8 1 0 2   r a   m 2 2     ] . o c h p - o r t s a [     1 v 1 6 4 8 0 . 3 0 8 1 : v i x r a 1corresponding author. contents "
1803.08462," √ an r-cut of a k-uniform hypergraph h is a partition of the vertex set of h into r parts and the size of the cut is the number of edges which have a vertex in each part. a classical m), and this result of edwards says that every m-edge graph has a 2-cut of size m/2 + ω( is best possible. that is, there exist cuts which exceed the expected size of a random cut by some multiple of the standard deviation. we study analogues of this and related results in hypergraphs. first, we observe that similarly to graphs, every m-edge k-uniform hypergraph m) larger than the expected size of a random r-cut. moreover, has an r-cut whose size is ω( in the case where k = 3 and r = 2 this bound is best possible and is attained by steiner triple systems. surprisingly, for all other cases (that is, if k ≥ 4 or r ≥ 3), we show that every m- edge k-uniform hypergraph has an r-cut whose size is ω(m5/9) larger than the expected size of a random r-cut. this is a signiﬁcant diﬀerence in behaviour, since the amount by which the size of the largest cut exceeds the expected size of a random cut is now considerably larger than the standard deviation. √ 1 "
1803.08463," in this report, we describe our par- ticipant named-entity recognition sys- tem at vlsp 2018 evaluation campaign. we formalized the task as a sequence labeling problem using bio encoding scheme. we applied a feature-based model which combines word, word-shape fea- tures, brown-cluster-based features, and word-embedding-based features. we com- pare several methods to deal with nested entities in the dataset. we showed that combining tags of entities at all levels for training a sequence labeling model (joint-tag model) improved the accuracy of nested named-entity recognition. "
1803.08466," 8 1 0 2   r a   m 2 2     ] . a f h t a m [     1 v 6 6 4 8 0 . 3 0 8 1 : v i x r a the purpose of this paper is to study frames for a hilbert space h, having the form {t nϕ}∞ n=0 for some ϕ ∈ h and an operator t : h → h. we characterize the frames that have such a representation for a bounded operator t, and discuss the properties of this operator. in particular, we prove that the image chain of t has ﬁnite length n in n=0 has the very particular n=n +ℓ is a frame for h for all ℓ ∈ n0. n=0 are sensitive to the ordering of the elements and to norm-perturbations of the generator ϕ and the operator t. on the other hand positive stability results are obtained by considering perturbations of the generator ϕ belonging to an invariant subspace on which t is a contraction. the overcomplete case; furthermore {t nϕ}∞ property that {t nϕ}n −1 we also prove that frames of the form {t nϕ}∞ n=0 ∪{t nϕ}∞ 1 "
1803.08467,". we introduce branchgan, a novel training method that enables unconditioned generative adversarial networks (gans) to learn image manifolds at multiple scales. what is unique about branchgan is that it is trained in multiple branches, progressively covering both the breadth and depth of the network, as resolutions of the training images increase to reveal ﬁner-scale features. speciﬁcally, each noise vector, as input to the generator network, is explicitly split into several sub-vectors, each corresponding to and trained to learn image representations at a particular scale. during training, we progressively “de-freeze” the sub- vectors, one at a time, as a new set of higher-resolution images is em- ployed for training and more network layers are added. a consequence of such an explicit sub-vector designation is that we can directly manipu- late and even combine latent (sub-vector) codes that are associated with speciﬁc feature scales. experiments demonstrate the eﬀectiveness of our training method in multi-scale, disentangled learning of image manifolds and synthesis, without any extra labels and without compromising qual- ity of the synthesized high-resolution images. we further demonstrate two new applications enabled by branchgan. keywords: generative adversarial network; image synthesis; image representation learning; multi-scale; disentanglement 1 "
1803.08471," as more aspects of social interaction are digi- tally recorded, there is a growing need to develop privacy-preserving data analysis methods. so- cial scientists will be more likely to adopt these methods if doing so entails minimal change to their current methodology. toward that end, we present a general and modular method for privatiz- ing bayesian inference for poisson factorization, a broad class of models that contains some of the most widely used models in the social sciences. our method satisﬁes local differential privacy, which ensures that no single centralized server need ever store the non-privatized data. to for- mulate our local-privacy guarantees, we introduce and focus on limited-precision local privacy—the local privacy analog of limited-precision differen- tial privacy (flood et al., 2013). we present two case studies, one involving social networks and one involving text corpora, that test our method’s ability to form the posterior distribution over la- tent variables under different levels of noise, and demonstrate our method’s utility over a na¨ıve ap- proach, wherein inference proceeds as usual, treat- ing the privatized data as if it were not privatized. "
1803.08472,". in earlier work in collaboration with pavel galashin and thomas mc- conville we introduced a version of chip-ﬁring for root systems. our investigation of root system chip-ﬁring led us to deﬁne certain polynomials analogous to ehrhart poly- nomials of lattice polytopes, which we termed the symmetric and truncated ehrhart- like polynomials. we conjectured that these polynomials have nonnegative integer coeﬃcients. here we aﬃrm “half” of this positivity conjecture by providing a positive, combinatorial formula for the coeﬃcients of the symmetric ehrhart-like polynomials. this formula depends on a subtle integrality property of slices of permutohedra which may be of independent interest. we also discuss how our formula very naturally sug- gests a conjecture for the coeﬃcients of the truncated ehrhart-like polynomials that turns out to be false in general, but which may hold in some cases. "
1905.11764," we study conﬂict situations that dynamically arise in trafﬁc scenarios, where different agents try to achieve their set of goals and have to decide on what to do based on their local perception. we distinguish several types of conﬂicts for this setting. in order to enable modelling of conﬂict situations and the reasons for conﬂicts, we present a logical framework that adopts concepts from epistemic and modal logic, justiﬁcation and temporal logic. using this framework, we illustrate how conﬂicts can be identiﬁed and how we derive a chain of justiﬁcations leading to this conﬂict. we discuss how conﬂict resolution can be done when a vehicle has local, incomplete information, vehicle to vehicle communication (v2v) and partially ordered goals. "
1905.11766,". h. guggenheimer generalized the planar volume product problem for locally convex curves c enclosing the origin k ≥ 2 times. he conjectured that the minimal volume product v (c)v (c ∗) for these curves is attained if the curve consists of the longest diagonals of a regular (2k + 1)-gon with centre 0, taken always in the positive orientation. this conjectured minimum is of the form k2 + o(k). we investigate special cases of this conjecture. we prove it for locally convex n-gons with 2k + 1 ≤ n ≤ 4k, if the central angles at 0 of all sides are equal to 2kπ/n. for 4k + 1 ≤ n we prove that for locally convex n-gons enclosing the origin k ≥ 2 times the critical (stationary) values of the volume product v (k)v (k ∗) are attained exactly when up to a non-singular linear map the vertices lie on the unit circle about 0, and the central angles of all sides are equal to 2kπ/n. for locally convex n-gons enclosing the origin k ≥ 2 times, and inscribed to the unit circle, with 2k + 1 ≤ n, we prove the conjecture up to a multiplicative factor about 0.43. 2010 mathematics subject classiﬁcation. primary: 52a40. secondary: 52a30, 52a10 key words and phrases. inverse blaschke-santal´o inequality, convex curves en- closing the origin several times "
1905.11767,". we consider the problem of comparing escape rates of the shift map on a subshift of ﬁnite type into the holes which are a union of equal number of cylinders based at words of identical length. the escape rate calculation leads to a combinatorial problem, in particular, we will see that the escape rate relates to the asymptotic behavior of the number of strings of ﬁxed length that do not contain a ﬁxed set of (forbidden) strings of identical length as substrings. we will also explore the relationship between the minimal period of the hole and the escape rate. this paper generalizes results of bunimovich and yurchenko [5] in which there was only one forbidden string. 9 1 0 2   y a m 8 2       ] s d h t a m . [     1 v 7 6 7 1 1 . 5 0 9 1 : v i x r a "
1905.11768," we propose a new algorithm—stochastic proximal langevin algorithm (spla)—for sampling from a log concave distribution. our method is a gen- eralization of the langevin algorithm to potentials expressed as the sum of one stochastic smooth term and multiple stochastic nonsmooth terms. in each itera- tion, our splitting technique only requires access to a stochastic gradient of the smooth term and a stochastic proximal operator for each of the nonsmooth terms. we establish nonasymptotic sublinear and linear convergence rates under convex- ity and strong convexity of the smooth term, respectively, expressed in terms of the kl divergence and wasserstein distance. we illustrate the efﬁciency of our sampling technique through numerical simulations on a bayesian learning task. 1 "
1905.11769," extreme classiﬁcation seeks to assign each data point, the most relevant labels from a universe of a million or more labels. this task is faced with the dual challenge of high precision and scalabil- ity, with millisecond level prediction times being a benchmark. we propose defrag, an adaptive feature agglomeration technique to accelerate ex- treme classiﬁcation algorithms. despite past works on feature clustering and selection, defrag dis- tinguishes itself in being able to scale to millions of features, and is especially beneﬁcial when feature sets are sparse, which is typical of recommendation and multi-label datasets. the method comes with provable performance guarantees and performs ef- ﬁcient task-driven agglomeration to reduce feature dimensionalities by an order of magnitude or more. experiments show that defrag can not only re- duce training and prediction times of several lead- ing extreme classiﬁcation algorithms by as much as 40%, but also be used for feature reconstruction to address the problem of missing features, as well as offer superior coverage on rare labels. "
1905.11770,. we consider positively curved riemannian manifolds whose universal covers have the same cohomology as bazaikin spaces and prove some structural results for the fundamental group in the presence of torus symmetry. 
1905.11771," we assess and compare diﬀerent methods for including leading threshold logarithms at next- to-leading-power in prompt photon production at hadron colliders, for both the direct and parton fragmentation mechanisms. we do this in addition to next-to-leading logarithmic threshold and joint resummation at leading power. we study the size of these eﬀects and their scale variations for lhc kinematics. we ﬁnd that the next-to-leading power eﬀects have a noticeable eﬀect on the photon transverse momentum distribution, typically of order o(10%), depending on the method of inclusion. our results indicate that next-to-leading power terms can reduce the scale dependence of the distribution considerably. †deceased. we dedicate this paper to his memory. 1 1 "
1905.11772," the determination of the speed of coronal mass ejections (cmes) is usually done by tracking brighter features (such as the cme front and core) in visible light coronagraphic images and by deriving unidimensional proﬁles of the cme speed as a function of altitude or time. nevertheless, cmes are usually characterized by the presence of signiﬁcant density inhomogeneities propagating outward with diﬀerent radial and latitudinal projected speeds, resulting in a complex evolution eventually forming the interplanetary cme. in this work, we demonstrate for the ﬁrst time how coronagraphic image sequences can be analyzed with cross-correlation technique to derive 2d maps of the almost instantaneous plasma speed distribution within the body of cmes. the technique is ﬁrst tested with the analysis of synthetic data, and then applied to real observations. results from this work allow to characterize the distribution and time evolution of kinetic energy inside cmes, as well as the mechanical energy (combined with the kinetic and potential energy) partition between the core and front of the cme. in the future, cmes will be observed by two channels (vl and uv ly-α) coronagraphs, such as metis on-board esa solar orbiter mission as well as ly-α solar telescope (lst) on-board chinese advanced space-based solar observatory (aso-s) mission. our results will help the analysis of these future observations, helping in particular to take into account the 2d distribution of ly-α doppler dimming eﬀect. keywords: sun: coronal mass ejections (cmes) − sun: uv radiation − methods: data analysis −magnetohydrodynamics (mhd) − techniques: polarimetric "
1905.11773," chronic obstructive pulmonary disease (copd) is a leading cause of morbidity and mortality worldwide. identifying those at highest risk of deterioration would allow more eﬀective distribution of preventative and surveillance resources. secondary pulmonary hypertension is a manifestation of advanced copd, which can be reliably diagnosed by the main pulmonary artery (pa) to ascending aorta (ao) ratio. in eﬀect, a pa diameter to ao diameter ratio of greater than 1 has been demonstrated to be a reliable marker of increased pulmonary arterial pressure. although clinically valuable and readily visualized, the manual assessment of the pa and the ao diameters is time consuming and under-reported. the present study describes a non invasive method to measure the diameters of both the ao and the pa from contrast-enhanced chest computed tomography (ct). the solution applies deep learning techniques in order to select the correct axial slice to measure, and to segment both arteries. the system achieves test pearson correlation coeﬃcient scores of 93% for the ao and 92% for the pa. to the best of our knowledge, it is the ﬁrst such fully automated solution. keywords: deep learning, chest computed tomography, pulmonary hypertension, chronic obstructive pul- monary disease, computer aided diagnosis. "
1905.11774,". this paper shows how the theorem of residues (tr) and the gelfand-fuchs cocycle can be deduced in a simple way from the weil reci- procity law (wrl). indeed, if one understand wrl as the triviality of cer- tain extension of groups, then tr is the same statement at the level of lie algebras. finally, the gelfand-fuchs cocycle can also be obtained in this way. "
1905.11776,". the aim of this paper is to present some results about the space lφ(ν), where ν is a vector measure on a compact (not necessarily abelian) group and φ is a young function. we show that under certain conditions, the space lφ(ν) becomes an l1(g)-module with respect to the usual convolution of functions. we also deﬁne one more convolution structure on lφ(ν). 9 1 0 2   y a m 8 2       ] . a f h t a m [     1 v 6 7 7 1 1 . 5 0 9 1 : v i x r a "
1905.11777," assemblies of anisotropic particles commonly appear in studies of active many-body systems. however, in two dimensions, the geometric ramiﬁcations of the ﬁnite den- sity of such objects are not entirely understood. to fully characterize these eﬀects, we perform an in-depth study of random assemblies generated by a slow compression of frictionless elliptical particles. the obtained conﬁgu- rations are then analysed using the set voronoi tessel- lation which takes the particle shape into the account. not only that we analyse most scalar and vectorial mor- phological measures, which are commonly discussed in the literature or which have been recently addressed in experiments, but also systematically explore the correla- tions between them. while in a limited range of param- eters similarities with ﬁndings in 3d assemblies could be identiﬁed, important diﬀerences are found when a broad range of aspect ratios and packing fractions are consid- ered. the data discussed in this study should thus pro- vide a unique reference set such that geometric eﬀects and diﬀerences from random assemblies could be clearly identiﬁed in more complex systems, including ones with soft and active particles that are typically found in bio- logical systems. "
1905.11778," in this paper, we study the cp violating processes in a general two-higgs-doublet model (2hdm) with tree-level ﬂavor changing neutral currents. in this model, sizable yukawa couplings involving top and charm quarks are still allowed by the collider and ﬂavor experiments, while the other couplings are strongly constrained experimentally. the sizable couplings, in general, have imaginary parts and could largely contribute to the cp violating observables concerned with the b and k mesons. in particular, the contribution may be so large that it aﬀects the direct cp violating k meson decay, where the discrepancy between the experimental result and the standard model prediction is reported. we discuss how well the anomaly is resolved in the 2hdm, based on study of the other ﬂavor observables. we also propose the way to test our 2hdm at the lhc. 9 1 0 2   y a m 8 2       ] h p - p e h [     1 v 8 7 7 1 1 . 5 0 9 1 : v i x r a 1 "
1905.11779," we propose a new methodology to perform mineralogic inversion from wellbore logs based on a bayesian linear regression model. our method essentially relies on three steps. the ﬁrst step makes use of approximate bayesian computation (abc) and selects from the bayesian generator a set of candidates-volumes corresponding closely to the wellbore data responses. the second step gathers these candidates through a density-based clustering algorithm. a mineral scenario is assigned to each cluster through direct mineralogical inversion, and we provide a conﬁdence estimate for each lithological hypothesis. the advantage of this approach is to explore all possible mineralogy hypotheses that match the wellbore data. this pipeline is tested on both synthetic and real datasets. keywords : mineralogical inversion; inverse problem; wellbore log; approximate bayesian computation; clustering "
1905.11780,". this work investigates how context should be taken into ac- count when performing continuous authentication of a smartphone user based on touchscreen and accelerometer readings extracted from swipe gestures. the study is conducted on the publicly available hmog dataset consisting of 100 study subjects performing pre-deﬁned reading and nav- igation tasks while sitting and walking. it is shown that context-speciﬁc models are needed for diﬀerent smartphone usage and human activity sce- narios to minimize authentication error. also, the experimental results suggests that utilization of phone movement improves swipe gesture-based veriﬁcation performance only when the user is moving. 1 "
1905.11781," recent years have witnessed the great advance of deep learning in a variety of vision tasks. many state-of-the- art deep neural networks suffer from large size and high complexity, which makes it difﬁcult to deploy in resource- limited platforms such as mobile devices. to this end, low- precision neural networks are widely studied which quan- tize weights or activations into the low-bit format. though being efﬁcient, low-precision networks are usually hard to train and encounter severe accuracy degradation. in this paper, we propose a new training strategy through expand- ing low-precision networks during training and removing the expanded parts for network inference. first, we equip each low-precision convolutional layer with an ancillary full-precision convolutional layer based on a low-precision network structure, which could guide the network to good local minima. second, a decay method is introduced to reduce the output of the added full-precision convolution gradually, which keeps the resulted topology structure the same to the original low-precision one. experiments on svhn, cifar and ilsvrc-2012 datasets prove that the proposed method can bring faster convergence and higher accuracy for low-precision neural networks. "
1905.11782,". we study a portfolio optimization problem for competitive agents with crra utilities and a common ﬁnite time horizon. the utility of an agent depends not only on her absolute wealth and consumption but also on her relative wealth and consumption when compared to the averages among the other agents. we derive a closed form solution for the n-player game and the corresponding mean ﬁeld game. this solution is unique in the class of equilibria with constant investment and continuous time-dependent consumption, both independent of the wealth of the agent. compared to the classical merton problem with one agent, the competitive model exhibits a wide range of highly nonlinear and non-monotone dependence on the agents’ risk tolerance and competitiveness parameters. counter-intuitively, competitive agents with high risk tolerance may behave like non- competitive agents with low risk tolerance. "
1905.11784," finding clothes that ﬁt is a hot topic in the e-commerce fashion industry. most approaches addressing this prob- lem are based on statistical methods relying on historical data of articles purchased and returned to the store. such approaches suffer from the cold start problem for the thou- sands of articles appearing on the shopping platforms every day, for which no prior purchase history is available. we propose to employ visual data to infer size and ﬁt charac- teristics of fashion articles. we introduce sizenet, a weakly- supervised teacher-student training framework that lever- ages the power of statistical models combined with the rich visual information from article images to learn visual cues for size and ﬁt characteristics, capable of tackling the chal- lenging cold start problem. detailed experiments are per- formed on thousands of textile garments, including dresses, trousers, knitwear, tops, etc. from hundreds of different brands. "
1905.11785,"—the performance of voice-based parkinson’s disease (pd) detection systems degrades when there is an acoustic mis- match between training and operating conditions caused mainly by degradation in test signals. in this paper, we address this mismatch by considering three types of degradation commonly encountered in remote voice analysis, namely background noise, reverberation and nonlinear distortion, and investigate how these degradations inﬂuence the performance of a pd detection system. given that the speciﬁc degradation is known, we explore the effectiveness of a variety of enhancement algorithms in compensating this mismatch and improving the pd detection accuracy. then, we propose two approaches to automatically control the quality of recordings by identifying the presence and type of short-term and long-term degradations and protocol violations in voice signals. finally, we experiment with using the proposed quality control methods to inform the choice of enhancement algorithm. experimental results using the voice recordings of the mpower mobile pd data set under different degradation conditions show the effectiveness of the quality con- trol approaches in selecting an appropriate enhancement method and, consequently, in improving the pd detection accuracy. this study is a step towards the development of a remote pd detection system capable of operating in unseen acoustic environments. preprint symptoms from voice signals [4]–[6]. moreover, advances in smart phone technology provide new opportunities for remote monitoring of pd symptoms by bypassing the logistical and practical limitations of recording voice samples in controlled experimental conditions in clinics [5], [7]. however, there is a higher risk outside controlled lab conditions that participants may not adhere to the test protocols, which probe for speciﬁc symptoms, due to lack of training, misinterpretation of the test protocol or negligence. moreover, voice signals in remote voice analysis might be subject to a variety of degradations during recording or transmission. processing the degraded recordings or those which do not comply with the assumptions of the test protocol can produce misleading, non-replicable and non-reproducible results [8] that could have signiﬁcant ramiﬁcations for the patients’ health. in addition, degradation of voice signals produces an acoustic mismatch between the training and operating conditions in automatic pd detection. a variety of techniques have been developed for compensating this type of mismatch in different speech-based applications [9]–[15] which can, in general, be categorized into four classes: (1) searching for robust features which parameterize speech regardless of degradations; (2) transforming a degraded signal to the acoustic condition of the training data using a signal enhancement algorithm1; (3) compensating the effects of degradation in the feature space by applying feature en- hancement; and (4) transforming the parameters of the devel- oped model to match the acoustic conditions of the degraded signal at operating time. however, to the best of the authors’ knowledge, there is a lack of studies of the impact of acoustic mismatch and the effect of compensation on the performance of pd detection systems. vasquez-correa et al. proposed a pre-processing scheme by applying a generalized subspace speech enhancement technique to the voiced and unvoiced segments of a speech signal to address the pd detection in non- controlled noise conditions [16]. they showed that applying speech enhancement to the unvoiced segments leads to an improvement in detection accuracy while the enhancement of voiced segments degrades the performance. however, this study is limited in terms of degradation types as it only considered the additive noise. moreover, they only evaluated the impact of an unsupervised enhancement method on pd detection performance, while the supervised algorithms have, in general, shown to reconstruct higher quality signals as they incorporate more prior information about the speech and noise. parkinson’s disease (pd) is a neurodegenerative disorder which progressively makes the patients unable to control their movement normally and, consequently, decreases the patients’ quality of life [1]. since there is no cure for pd, it is necessary to develop tools to diagnose this disease in early stages in order to control its symptoms. speech is known to reﬂect the pd symptoms since the majority of pd patients suffer from some forms of vocal disorder [2]. it has been demonstrated in [3] that early changes of clinical symptoms of pd are more reﬂected and pronounced in acoustic analysis of voice signals than in perceptual evaluation of voice by a therapist. this has motivated researchers to take advantage of advanced speech signal processing and machine learning algorithms to develop highly accurate and data-driven methods for detecting pd a.h. poorjam, m.s. kavalekalam, l. shi, j.r. jensen and m.g. christensen are with the audio analysis lab, create, aalborg university, aalborg 9000, denmark (e-mail: {ahp,msk,ls,jrj,mgc}@create.aau.dk). this work was funded by independent research fund denmark: dff 4184- "
1905.11786," we propose a novel deep learning method for local self-supervised representation learning that does not require labels nor end-to-end backpropagation but exploits the natural order in data instead. inspired by the observation that biological neural networks appear to learn without backpropagating a global error signal, we split a deep neural network into a stack of gradient-isolated modules. each module is trained to maximize the mutual information between its consecutive outputs us- ing the infonce bound from oord et al. [2018]. despite this greedy training, we demonstrate that each module improves upon the output of its predecessor, and that the representations created by the top module yield highly competitive results on downstream classiﬁcation tasks in the audio and visual domain. the proposal en- ables optimizing modules asynchronously, allowing large-scale distributed training of very deep neural networks on unlabelled datasets. 1 "
1905.11787," pruning ﬁlters is an effective method for accelerating deep neural networks (dnns), but most existing approaches prune ﬁlters on a pre-trained network directly which limits in ac- celeration. although each ﬁlter has its own effect in dnns, but if two ﬁlters are same with each other, we could prune one safely. in this paper, we add an extra cluster loss term in the loss function which can force ﬁlters in each cluster to be similar online. after training, we keep one ﬁlter in each cluster and prune others and ﬁne-tune the pruned network to compensate the loss. particularly, the clusters in every layer can be deﬁned ﬁrstly which is effective for pruning dnns within residual blocks. extensive experiments on cifar10 and cifar100 benchmarks demonstrate the competitive per- formance of our proposed ﬁlter pruning method. index terms— deep neural networks, similar ﬁlter, ﬁlter pruning, cluster loss "
1905.11788," (cid:140)e continuously increasing degree of automation in many ar- eas (e.g. manufacturing engineering, public infrastructure) lead to the construction of cyber-physical systems and cyber-physical networks. to both, time and energy are the most critical operating resources. considering for instance the tactile internet speci(cid:128)- cation, end-to-end latencies in these systems must be below 1ms, which means that both communication and system latencies are in the same order of magnitude and must be predictably low. as control loops are commonly handled over di(cid:130)erent variants of net- work infrastructure (e.g. mobile and (cid:128)bre links) particular a(cid:138)ention must be payed to the design of reliable, yet fast and energy-e(cid:129)cient data-transmission channels that are robust towards unexpected transmission failures. as design goals are o(cid:137)en con(cid:131)icting (e.g. high performance vs. low energy), it is necessary to analyze and investigate trade-o(cid:130)s with regards to design decisions during the construction of cyber-physical networks. in this paper, we present ∆elta, an approach towards a tool- supported construction process for cyber-physical networks. ∆elta extends the previously presented x-lap tool by new analysis fea- tures, but keeps the original measurements facilities unchanged. ∆elta jointly analyzes and correlates the runtime behavior (i.e. per- formance, latency) and energy demand of individual system compo- nents. it provides an automated analysis with precise thread-local time interpolation, control-(cid:131)ow extraction, and examination of la- tency criticality. we further demonstrate the applicability of ∆elta with an evaluation of a prototypical implementation. ccs concepts •networks →cyber-physical networks; network reliability; •computer systems organization →real-time systems; embed- ded and cyber-physical systems; dependable and fault-tolerant sys- tems and networks; permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro(cid:128)t or commercial advantage and that copies bear this notice and the full citation on the (cid:128)rst page. copyrights for third-party components of this work must be honored. for all other uses, contact the owner/author(s). ecrts-rtn’18, barcelona, spain © 2018 copyright held by the owner/author(s). wolfgang schr¨oder-preikschat friedrich-alexander university erlangen-n¨urnberg wosch@cs.fau.de keywords performance evaluation, simulation and modelling tools of real- time networks (automotive, aerospace, multimedia, etc.), net- worked embedded systems and sensors, cyber-physical systems, internet of (cid:140)ings acm reference format: stefan reif, andreas schmidt, timo h¨onig, (cid:140)orsten herfet, and wolfgang schr¨oder-preikschat. 2018. ∆elta: di(cid:130)erential energy-e(cid:129)ciency, latency, and timing analysis for real-time networks. in proceedings of rtn’2018 (cid:138)e 16th international workshop on real-time networks, barcelona, spain, july 2018 (ecrts-rtn’18), 6 pages. h(cid:138)ps://doi.org/10.1145/3314206.3314211 "
1905.11789,". it has become obvious that certain singular phenomena cannot be explained by a mere investigation of the conﬁguration space, deﬁned as the solution set of the loop closure equations. for example, it was observed that a particular 6r linkage, constructed by combination of two goldberg 5r linkages, exhibits kinematic singularities at a smooth point in its conﬁguration space. such problems are addressed in this paper. to this end, an algebraic framework is used in which the constraints are formulated as polynomial equations using study parameters. the algebraic object of study is the ideal generated by the constraint equations (the constraint ideal). using basic tools from commutative algebra and algebraic geometry (primary decom- position, hilbert’s nullstellensatz), the special phenomenon is related to the fact that the constraint ideal is not a radical ideal. with a primary decomposition of the constraint ideal, the associated prime ideal of one primary ideal contains strictly into the associated prime ideal of another primary ideal which also gives the smooth conﬁguration curve. this analysis is extended to shaky and kinematotropic linkages, for which examples are presented. "
1905.11790," let γ ∈ (0, 2), let h be the planar gaussian free ﬁeld, and let dh be the associated γ-liouville quantum gravity (lqg) metric. we prove that for any random borel set x ⊂ c which is independent from h, the hausdorﬀ dimensions of x with respect to the euclidean metric and with respect to the γ-lqg metric dh are a.s. related by the kpz formula. we also derive “worst-case” bounds relating the euclidean and γ-lqg dimensions of x when x and h are not necessarily independent. using these bounds, we obtain an upper bound for the euclidean hausdorﬀ dimension of a γ-lqg geodesic which equals 1.312 . . . when γ =(cid:112)8/3; and an upper boundary of a(cid:112)8/3-lqg metric ball. bound of 1.9428 . . . for the euclidean hausdorﬀ dimension of a connected component of the contents "
1905.11793," correlation between microstructure noise and latent ﬁnancial logarithmic returns is an em- pirically relevant phenomenon with sound theoretical justiﬁcation. with few notable exceptions, all integrated variance estimators proposed in the ﬁnancial literature are not designed to explic- itly handle such a dependence, or handle it only in special settings. we provide an integrated variance estimator that is robust to correlated noise and returns. for this purpose, a generaliza- tion of the forward filtering backward sampling algorithm is proposed, to provide a sampling technique for a latent conditionally gaussian random sequence. we apply our methodology to intra-day microsoft prices, and compare it in a simulation study with established alternatives, showing an advantage in terms of root mean square error and dispersion. keywords: forward filtering and backward sampling; integrated variance; kalman filter- ing; state space models. "
1905.11794," a colored complete graph is said to be gallai-colored if it contains no rainbow triangle. this property has been shown to be equivalent to the existence of a partition of the vertices (of every induced subgraph) in which at most two colors appear on edges between the parts and at most one color appears on edges in between each pair of parts. we extend this notion by deﬁning a coloring of a complete graph to be k-gallai if every induced subgraph has a nontrivial partition of the vertices such that there are at most k colors present in between parts of the partition. the generalized (k, ℓ) gallai-ramsey number of a graph h is then deﬁned to be the minimum number of vertices n such that every k-gallai coloring of a complete graph kn with n ≥ n using at most ℓ colors contains a monochromatic copy of h. we prove bounds on these generalized (k, ℓ) gallai-ramsey numbers based on the structure of h, extending recent results for gallai colorings. "
1905.11795," in this paper, the credit scoring problem is studied by incorporating network information, where the advantages of such incorporation are investigated in two scenarios. firstly, a bayesian optimal ﬁlter is proposed to provide a prediction for lenders assuming that published credit scores are estimated merely from structured individual data. such prediction is used as a monitoring indicator for the risk warning in lenders’ future ﬁnancial decisions. secondly, we further propose a recursive bayes estimator to improve the accuracy of credit scoring estimation by incorporating the dynamic interaction topology of clients as well. it is shown that under the proposed evolution framework, the designed estimator has a higher precision than any eﬃcient estimator, and the mean square errors are strictly smaller than the cram´er– rao lower bound for clients within a certain range of scores. finally, simulation results for a speciﬁc case illustrate the eﬀectiveness and feasibility of the proposed methods. keywords: credit scoring, network information, bayesian ﬁltering, average risk minimization jel classiﬁcation: g20, g32, g15 9 1 0 2   y a m 8 2       ] . h t n o c e [     1 v 5 9 7 1 1 . 5 0 9 1 : v i x r a 1. "
1905.11796," we explore self-supervised models that can be potentially deployed on mobile devices to learn general purpose audio representations. speciﬁcally, we propose methods that exploit the temporal context in the spectrogram domain. one method estimates the temporal gap between two short audio segments extracted at random from the same audio clip. the other methods are inspired by word2vec, a popular technique used to learn word embeddings, and aim at reconstructing a temporal spectrogram slice from past and future slices or, alternatively, at reconstructing the context of surrounding slices from the current slice. we focus our evaluation on small encoder architectures, which can be potentially run on mobile devices during both inference (re-using a common learned representation across multiple downstream tasks) and training (capturing the true data distribution without com- promising users’ privacy when combined with federated learning). we evaluate the quality of the embeddings produced by the self-supervised learning models, and show that they can be re-used for a variety of downstream tasks, and for some tasks even approach the performance of fully supervised models of similar size. 1 "
1905.11797," we study a setting in which a learner faces a sequence of a/b tests and has to make as many good decisions as possible within a given amount of time. each a/b test n is associated with an unknown (and potentially negative) reward µn ∈ [−1, 1], drawn i.i.d. from an unknown and ﬁxed distribution. for each a/b test n, the learner sequentially draws i.i.d. samples of a {−1, 1}-valued random variable with mean µn until a halting criterion is met. the learner then decides to either accept the reward µn or to reject it and get zero instead. we measure the learner’s performance as the sum of the expected rewards of the accepted µn divided by the total expected number of used time steps (which is diﬀerent from the expected ratio between the total reward and the total number of used time steps). we design an algorithm and prove a data-dependent regret bound against any set of policies based on an arbitrary halting criterion and decision rule. though our algorithm borrows ideas from multiarmed bandits, the two settings are signiﬁcantly diﬀerent and not directly comparable. in fact, the value of µn is never observed directly in our setting—unlike rewards in stochastic bandits. moreover, the particular structure of our problem allows our regret bounds to be independent of the number of policies. 1 "
1905.11799," appearance and motion are two key components to depict and characterize the video content. cur- rently, the two-stream models have achieved state- of-the-art performances on video classiﬁcation. however, extracting motion information, speciﬁ- cally in the form of optical ﬂow features, is ex- tremely computationally expensive, especially for large-scale video classiﬁcation. in this paper, we propose a motion hallucination network, namely monet, to imagine the optical ﬂow features from the appearance features, with no reliance on the op- tical ﬂow computation. speciﬁcally, monet mod- els the temporal relationships of the appearance features and exploits the contextual relationships of the optical ﬂow features with concurrent con- nections. extensive experimental results demon- strate that the proposed monet can effectively and efﬁciently hallucinate the optical ﬂow features, which together with the appearance features con- sistently improve the video classiﬁcation perfor- mances. moreover, monet can help cutting down almost a half of computational and data-storage burdens for the two-stream video classiﬁcation. our code is available at: https://github. com/yongyitang92/monet-features. "
1905.11800," two types of osmolytes, i.e., trimethylamin n-oxide (tmao) and urea, demonstrate dramatically dif- ferent properties in a protein folding process. even with the great progresses in revealing the potential underlying mechanism of these two osmolyte systems, many problems still remain unsolved. in this pa- per, we propose to use the persistent homology, a newly-invented topological method, to systematically study the osmolytes molecular aggregation and their hydrogen-bonding network from a global topological perspective. it has been found that, for the ﬁrst time, tmao and urea show two extremely diﬀerent topological behaviors, i.e., extensive network and local cluster. in general, tmao forms highly consis- tent large loop or circle structures in high concentrations. in contrast, urea is more tightly aggregated locally. moreover, the resulting hydrogen-bonding networks also demonstrate distinguishable features. with the concentration increase, tmao hydrogen-bonding networks vary greatly in their total number of loop structures and large-sized loop structures consistently increase. in contrast, urea hydrogen-bonding networks remain relatively stable with slight reduce of the total loop number. moreover, the persistent entropy (pe) is, for the ﬁrst time, used in characterization of the topological information of the aggrega- tion and hydrogen-bonding networks. the average pe systematically increases with the concentration for both tmao and urea, and decreases in their hydrogen-bonding networks. but their pe variances have totally diﬀerent behaviors. finally, topological features of the hydrogen-bonding networks are found to be highly consistent with those from the ion aggregation systems, indicating that our topological invariants can characterize intrinsic features of the “structure making” and “structure breaking” systems. key words: persistent homology, molecular aggregation, hydrogen-bonding network, persistent betti num- ber, persistent entropy. 9 1 0 2   y a m 8 2       ] . m q o i b - q [     1 v 0 0 8 1 1 . 5 0 9 1 : v i x r a ∗address correspondences to kelin xia. e-mail:xiakelin@ntu.edu.sg 1 1 "
1905.11801,"  optically addressable paramagnetic point defects in semiconductors are among the most  promising systems for quantum-information technologies. however, the fidelities of spin  initialization and readout rely on optical cycling, and charge-state instabilities during this  cycle have proven to be an important unresolved problem. in this work, we demonstrate  that the quantum well of an extended defect can stabilize the charge state of a point  defect. in particular, we establish that the pl6 center in silicon carbide is a divacancy  within a stacking fault, and that the quantum well deriving from this stacking fault  stabilizes the defect’s charge state, leading to its extraordinary performance at elevated  temperatures and unusual robustness to optical cycling. the generalization of our results  provides a material-based solution to the optical instability of color centers in  semiconductors that may facilitate the development of a new class of robust single- photon sources and spin qubits.        "
1905.11802,". in this paper, we investigate the monotonicity and inequalities for some functions involving the arc lemniscate and the hyperbolic arc lemniscate functions. in particular, sharp shafer-fink type inequalities for the arc lemniscate and the hyperbolic arc lemniscate functions are proved. keywords. arc lemniscate functions, hyperbolic arc lemniscate functions, lemniscate functions, hyperbolic lemniscate functions, shafer-fink type inequalities mathematics subject classification (2010). 26d07, 33e05 "
1905.11804,"  field canals improvement projects (fcips) are one of the ambitious projects  constructed to save fresh water. to finance this project, conceptual cost models are  important to accurately predict preliminary costs at early stages of the project. the  first step is to develop a conceptual cost model to identify key cost drivers affecting  the project. therefore, input variables selection remains an important part of model  development,  as  the  poor  variables  selection  can  decrease  model  precision.  the  study  discovered  the  most  important  drivers  of  fcips  based  on  a  qualitative  approach  and  a  quantitative  approach.  subsequently,  the  study  has  developed  a  parametric  cost  model  based  on  machine  learning  methods  such  as  regression  methods, artificial neural networks, fuzzy model and case based reasoning.  there are several methods to achieve prediction for project preliminary cost.  however, cost model inputs identification remains a challenging part during model  development.  therefore,  this  study  has  conducted  two  procedures  consisted  of  traditional  delphi  method,  fuzzy  delphi  method  (fdm)  and  the  fuzzy  analytic  hierarchy process (fahp) to determine these drivers. a delphi rounds and likert  scale  were  used  to  determine  the  most  important  factors  from  viewpoints  of  consultant engineers and involved contractors. the study concluded that proposed  approaches provided satisfying and consistent results. finally, cost drivers of fcips  were identified and can be used to develop a reliable conceptual cost model. on the  other  hand,  the  study  has  determined  the  key  cost  drivers  for  fcips  based  on  quantitative data and statistical techniques. factor analysis, regression methods and  correlation methods are utilized to identify cost drivers. in addition, this study has  developed two hybrid models based on correlation matrix and stepwise regression  which have identified the cost drivers more effectively that the other techniques. the  key cost drivers are command area, pvc length, construction year and a number of  irrigation  valves  where  the  number  of  irrigation  valves  can  be  calculated  as  a  function of the pvc length.  once  the  key  cost  drivers  of  a  project  are  identified,  the  parametric  (algorithmic) cost model for the fcips is be developed. to develop the parametric  cost model, two models are developed one by multiple linear regression and the other  by  artificial  neural networks  (anns). the  results  reveal the  ability  of  both  linear  regression and anns model to predict cost estimate with an acceptable degree of  accuracy. sensitivity analysis is conducted to determine the contribution of selected  key  parameters.  finally,  a  simple  friendly  project  data-input  screen  is  created  to  facilitate usage and manipulation of the developed model. the research contribution    iii    has  developed  a  reliable  parametric  model  for  predicting  the  conceptual  cost  of  fcips  with  acceptable  accuracy  (9.12%  and  7.82%  for  training  and  validation  respectively).      fuzzy systems have the ability to model numerous applications and to solve  many kinds of problems with uncertainty nature such as cost prediction modeling.  however, traditional fuzzy modeling cannot capture any kind of learning or adoption  which  formulates  a  problem  in  fuzzy  rules  generation.  therefore,  hybrid  fuzzy  models can be conducted to automatically generate fuzzy rules and optimally adjust  membership  functions  (mfs). this  study  has  reviewed  two types of  hybrid  fuzzy  models: neuro-fuzzy and evolutionally fuzzy modeling. moreover, a case study is  applied  to  compare  the  accuracy  and  performance  of  traditional  fuzzy  model  and  hybrid  fuzzy  model  for  cost  prediction  where  the  results  show  a  superior  performance of hybrid fuzzy model than traditional fuzzy model.                                    iv    contents  acknowledgement ..................................................................................................... i   ...................................................................................................................... i  list of tables .......................................................................................................... vii  list of figures ........................................................................................................ viii  list of abbreviations ................................................................................................. x  chapter 1 .............................................................................................................. 1  "
1905.11805," recent face reenactment studies have achieved remarkable success either between two identities or in the many-to-one task. however, existing methods have limited scalability when the target person is not a predeﬁned speciﬁc identity. to address this limitation, we present a novel many-to-many face reenactment framework, named faceswapnet, which allows transferring facial expressions and movements from one source face to arbitrary targets. our proposed approach is composed of two main modules: the landmark swapper and the landmark-guided generator. instead of maintaining independent models for each pair of person, the former module uses two encoders and one decoder to adapt anyone’s face landmark to target persons. using the neutral expression of the target person as a reference image, the latter module leverages geometry information from the swapped land- mark to generate photo-realistic and emotion-alike images. in addition, a novel triplet perceptual loss is proposed to force the generator to learn geometry and appearance information simultaneously. we evaluate our model on rafd dataset and the results demonstrate the superior quality of reenacted images as well as the ﬂexibility of transferring facial movements between identities. 1 "
1905.11806," in a human-machine dialog scenario, deciding the appropriate time for the machine to take the turn is an open research prob- lem. in contrast, humans engaged in conversations are able to timely decide when to interrupt the speaker for competitive or non-competitive reasons. in state-of-the-art turn-by-turn dialog systems the decision on the next dialog action is taken at the end of the utterance. in this paper, we propose a token-by-token pre- diction of the dialog state from incremental transcriptions of the user utterance. to identify the point of maximal understanding in an ongoing utterance, we a) implement an incremental dia- log state tracker which is updated on a token basis (idst) b) re-label the dialog state tracking challenge 2 (dstc2) dataset and c) adapt it to the incremental turn-taking experimental sce- nario. the re-labeling consists of assigning a binary value to each token in the user utterance that allows to identify the ap- propriate point for taking the turn. finally, we implement an in- cremental turn taking decider (ittd) that is trained on these new labels for the turn-taking decision. we show that the pro- posed model can achieve a better performance compared to a deterministic handcrafted turn-taking algorithm. index terms: incremental dialog state tracking, incremental turn-taking decider, dialog systems, recurrent neural net- works, long short-term memory "
1905.11807,". this paper describes a possible way to improve computer security by implementing a program which implements the following three features related to a weak notion of artiﬁcial consciousness: (partial) self-monitoring, ability to compute the truth of quantiﬁer-free propositions and the ability to communicate with the user. the integrity of the program could be enhanced by using a trusted computing approach, that is to say a hardware module that is at the root of a chain of trust. this paper outlines a possible approach but does not refer to an implementation (which would need further work), but the author believes that an implementation using current processors, a debugger, a monitoring program and a trusted processing module is currently possible. "
1905.11808," in this work, a method is proposed for combining diﬀerential and integral benchmark experimental data within a bayesian framework for nuclear data adjustments and multi-level uncertainty propagation using the total monte carlo method. first, input parameters to basic nuclear physics models implemented within the state of the art nuclear reactions code, talys, were sampled from uniform distributions and randomly varied to produce a large set of random nuclear data ﬁles. next, a probabilistic data assimilation was carried out by computing the likelihood function for each random nuclear data ﬁle based ﬁrst on only diﬀerential experimental data (1st update) and then on integral benchmark data (2nd update). the individual likelihood functions from the two updates were then combined into a global likelihood function which was used for the selection of the ﬁnal ’best’ ﬁle. the proposed method has been applied for the adjustment of 208pb in the fast neutron energy region below 20 mev. the ’best’ ﬁle from the adjustments was compared with available experimental data from the exfor database as well as evaluations from the major nuclear data libraries and found to compare favourably. keywords: bayesian update, diﬀerential and integral experiments, data adjustments, global likelihood function, ﬁle weights, total monte carlo. "
1905.11809," we consider the local well-posedness of strong and classical solutions to the three-dimensional barotropic compressible navier-stokes equations with density containing vacuum initially. we ﬁrst prove the local existence and uniqueness of the strong solutions, where the initial compatibility condition proposed in [2–4] is removed under suitable sense. then, the continuous of strong solutions on the initial data is derived under an additional compatibility condition. moreover, for the initial data satisfying some additional regularity and compatibility condition, the strong solution is proved to be a classical one. keywords: compressible navier-stokes equations; vacuum; strong solutions; classi- cal solutions 1 "
1905.11810," the full description of a superconductor requires that it has an inﬁnite dc conductivity (or zero electrical resistivity) as well as expels the external magnetic ﬁelds. thus, for any holographic superconductor which is dual to a real supercon- ductor, it is necessary to examine, simultaneously, these two features based on the gauge/gravity duality. in this paper, we explore numerically these two aspects of the higher dimensional holographic superconductors, in the presence of a power-maxwell electrodynamics as the gauge ﬁeld. at ﬁrst, we calculate the critical temperature, condensation, conductivity, and superconducting gap, in the absence of magnetic ﬁeld and disclose the eﬀects of both power parameter, s, as well as the spacetime dimensions, d, on this quantities. then, we immerse the superconductor into an external magnetic ﬁeld, b, and observe that with increasing the magnetic ﬁeld, the starting point of condensation occurs at temperature less than the critical tempera- ture, tc, in the absence of magnetic ﬁeld. this implies that at a ﬁxed temperature, we can deﬁne a critical magnetic ﬁeld, above which the critical temperature goes to zero which is similar to the meissner eﬀect in superconductor. in these indications, we also try to show the distinction of the conformal invariance of the power-maxwell lagrangian that occurs for s = d/4. 9 1 0 2   y a m 6 2       ] h t - p e h [     1 v 0 1 8 1 1 . 5 0 9 1 : v i x r a ∗asheykhi@shirazu.ac.ir 1 1 "
1905.11811," we study a thermo-mechanical system comprised by an alpha stirling engine and a ﬂywheel from the perspective of dynamical systems theory. thermodynamics establish a static relation between the ﬂywheel’s angle and the forces exerted by the two power pistons that constitute the engine. mechanics, in turn, provide a dynamic relation between the forces and the angle, ultimately leading to a closed dynamical model. we are interested in the diﬀerent behaviors that the engine displays as parameters are varied. the temperature of the hot piston and the mechanical phase between both pistons constitute our bifurcation parameters. considering that energy conversion in the engine can only take place through cyclic motions, we are particularly interested in the appearance of limit cycles. keywords: stirling engine, modeling, bifurcation analysis, cylindrical state space "
1905.11813," in this note, we extend the connection between the hydrogen atom and π to the number e via the lerch’s transcendent. 9 1 0 2   y a m 0 3       ] . m g h t a m [     2 v 3 1 8 1 1 . 5 0 9 1 : v i x r a 1 "
1905.11814," a wide variety of dnn applications increasingly rely on the cloud to perform their huge computation. this heavy trend toward cloud-hosted inference services raises serious privacy concerns. this model requires the sending of private and privileged data over the network to remote servers, exposing it to the service provider. even if the provider is trusted, the data can still be vulnerable over communication channels or via side-channel attacks [1, 2] at the provider. to that end, this paper aims to reduce the information content of the communicated data without compromising the cloud service’s ability to provide a dnn inference with acceptably high accuracy. this paper presents an end-to-end framework, called shredder, that, without altering the topology or the weights of a pre-trained network, learns an additive noise distribution that signiﬁcantly reduces the information content of communicated data while maintaining the inference accuracy. shredder learns the additive noise by casting it as a tensor of trainable parameters enabling us to devise a loss functions that strikes a balance between accuracy and information degradation. the loss function exposes a knob for a disciplined and controlled asymmetric trade-off between privacy and accuracy. while keeping the dnn intact, shredder enables inference on noisy data without the need to update the model or the cloud. experimentation with real-world dnns shows that shredder reduces the mutual information between the input and the communicated data to the cloud by 70.2% compared to the original execution while only sacriﬁcing 1.46% loss in accuracy. "
1905.11815, in this paper we compute in some new cases the cardinalities of the ﬁbers of cer- tain natural ﬁbrations that appear in the analysis of the conﬁguration space of the heisenberg ring. this is done by means of certain cyclic group actions on some subsets of restricted partitions. 1 
1905.11816,". in this paper, we shall give an extension of operator bellman inequality. this result is estimated via kantorovich constant. "
1905.11817," the information-theoretic analysis by russo and van roy [25] in combination with minimax duality has proved a powerful tool for the analysis of online learning algorithms in full and partial information settings. in most applications there is a tantalising similarity to the classical analysis based on mirror descent. we make a formal connection, showing that the information-theoretic bounds in most applications can be derived from existing techniques for online convex optimisation. besides this, for k-armed adversarial bandits we provide an efﬁcient algorithm with regret that matches the best information-theoretic upper bound and improve best known regret guarantees for online linear optimisation on (cid:96)p-balls and bandits with graph feedback. 1 "
1905.11818,". in this paper, we study the cauchy-dirichlet problem for parabolic complex monge-amp`ere equations on a strongly pseudoconvex domain by the viscosity method. we extend the results in [egz15b] on the existence of solution and the convergence at inﬁnity. we also establish the h¨older regularity of the solutions when the cauchy-dirichlet data are h¨older continuous. "
1905.11820,". we ﬁrst prove, for pairs consisting of a simply connected complex reductive group together with a connected subgroup, the equivalence between two diﬀerent notions of gelfand pairs. this partially answers a question posed by gross, and allows us to use a criterion due to aizenbud and gourevitch, and based on gelfand-kazhdan’s theorem, to study the gelfand property for com- plex symmetric pairs. this criterion relies on the regularity of the pair and its descendants. we introduce the concept of a pleasant pair, as a means to prove regularity, and study, by recalling the classiﬁcation theorem, the pleasantness of all complex symmetric pairs. on the other hand, we prove a method to compute all the descendants of a complex symmetric pair by us- ing the extended satake diagram, which we apply to all pairs. finally, as an application, we prove that eight out of the twelve exceptional complex sym- metric pairs, together with the inﬁnite family (spin4q+2, spin4q+1), satisfy the gelfand property, and state, in terms of the regularity of certain symmetric pairs, a suﬃcient condition for a conjecture by van dijk and a reduction of a conjecture by aizenbud and gourevitch. "
1905.11821,". in this article, for a polyadic group (g, f ) = derθ,b(g, ·), we give a necessary and suﬃcient condition in terms of the group (g, ·), the automorphism θ, and the element b, in order that the polyadic group becomes free. "
1905.11822,".         distance threshold queries keeping in mind real life application such as collision   detection on robotic bees. we will focus on spatial hashes designed to store 3d points and capable of fastly determining which of them surpass a specific threshold   from any other. in this paper we will discuss related literature, explain in depth the   data structure chosen with its design criteria, operations and speed and memory   efficiency analysis.                                                                                                                                                                                                                                                  keywords: ​spatial data structures, complexity, collision detection, efficiency, distance  threshold.    "
1905.11823,". we show that the empirical process associated to a system of weakly interacting diﬀusion processes exhibits a form of noise-induced metastability. the result is based on an analysis of the associated mckean–vlasov free energy, which for suitable attractive interaction potentials has at least two distinct global minimisers at the critical parameter value β = βc. on the torus, one of these states is the spatially homogeneous constant state and the other is a clustered state. we show that a third critical point exists at this value. as a result, we obtain that the probability of transition of the empirical process from the constant state scales like exp(−n∆), with ∆ the energy gap at β = βc. the proof is based on a version of the mountain pass theorem for lower semicontinuous and λ-geodesically convex functionals on the space of probability measures p(m) equipped with the w2 wasserstein metric, where m is a riemannian manifold or rd. "
1905.11824,"—cyber threat intelligence is one of the emerging ar- eas of focus in information security. much of the recent work has focused on rule-based methods and detection of network attacks using intrusion detection algorithms. in this paper we propose a framework for inspecting and modelling the behavioural aspect of an attacker to obtain better insight predictive power on his future actions. for modelling we propose a novel semi-supervised algorithm called fusion hidden markov model (fhmm) which is more robust to noise, requires comparatively less training time, and utilizes the beneﬁts of ensemble learning to better model temporal relationships in data. this paper evaluates the performances of fhmm and compares it with both traditional algorithms like markov chain, hidden markov model (hmm) and recently developed deep recurrent neural network (deep rnn) architectures. we conduct the experiments on dataset con- sisting of real data attacks on a cowrie honeypot system. fhmm provides accuracy comparable to deep rnn architectures at signiﬁcant lower training time. given these experimental results, we recommend using fhmm for modelling discrete temporal data for signiﬁcantly faster training and better performance than existing methods. index terms—attacker behaviour modelling, cyber security, fusion hidden markov models, hidden markov models, honeypot, markov chain, sequence models, threat intelligence. "
1905.11825,". the increasing luminosities of future large hadron collider runs and next generation of collider experiments will require an unprecedented amount of simulated events to be produced. such large scale productions are extremely demanding in terms of computing resources. thus new approaches to event generation and simulation of detector responses are needed. in lhcb, the accurate simulation of cherenkov detectors takes a sizeable fraction of cpu time. an alternative approach is described here, when one generates high-level reconstructed observables using a generative neural network to bypass low level details. this network is trained to reproduce the particle species likelihood function values based on the track kinematic parameters and detector occupancy. the fast simulation is trained using real data samples collected by lhcb during run 2. we demonstrate that this approach provides high-ﬁdelity results. "
1905.11826," deep convolutional neural networks (dcnns) are powerful models that yield im- pressive results at object classiﬁcation. however, recent work has shown that they do not generalize well to partially occluded objects and to mask attacks. in con- trast to dcnns, compositional models are robust to partial occlusion, however, they are not as discriminative as deep models. in this work, we integrate dc- nns and compositional object models to retain the best of both approaches: a discriminative model that is robust to partial occlusion and mask attacks. our model is learned in two steps. first, a standard dcnn is trained for image clas- siﬁcation. subsequently, we cluster the dcnn features into dictionaries. we show that the dictionary components resemble object part detectors and learn the spatial distribution of parts for each object class. we propose mixtures of com- positional models to account for large changes in the spatial activation patterns (e.g. due to changes in the 3d pose of an object). at runtime, an image is ﬁrst classiﬁed by the dcnn in a feedforward manner. the prediction uncertainty is used to detect partially occluded objects, which in turn are classiﬁed by the com- positional model. our experimental results demonstrate that such compositional convolutional networks resolve a fundamental problem of current deep learning approaches to computer vision: they recognize occluded objects with exceptional performance, even when they have not been exposed to occluded objects during training, while at the same time maintaining high discriminative performance for non-occluded objects. 1 "
1905.11827," we present a comparative study of the size-line width relation for substructures within six molec- ular clouds in the large magellanic cloud (lmc) mapped with the atacama large millime- ter/submillimeter array (alma). our sample extends our previous study, which compared a planck detected cold cloud in the outskirts of the lmc with the 30 doradus molecular cloud and found the typical line width for 1 pc radius structures to be 5 times larger in 30 doradus. by observing clouds with intermediate levels of star formation activity, we demonstrate a clear correlation between line width at ﬁxed size and both local and cloud-scale 8 µm intensity. at the same time, line width at a given size appears to independently correlate with measures of mass surface density. our results sug- gest that both virial-like motions due to gravity and local energy injection by star formation feedback play important roles in determining intracloud dynamics. keywords: galaxies: ism — radio lines: ism — ism: molecules — magellanic clouds "
1905.11828," asymmetric distributed constraint optimization problems (adcops) are an emerging model for co- ordinating agents with personal preferences. how- ever, the existing inference-based complete algo- rithms which use local eliminations cannot be ap- plied to adcops, as the parent agents are required to transfer their private functions to their children. rather than disclosing private functions explicitly to facilitate local eliminations, we solve the prob- lem by enforcing delayed eliminations and propose asymdpop, the ﬁrst inference-based complete al- gorithm for adcops. to solve the severe scala- bility problems incurred by delayed eliminations, we propose to reduce the memory consumption by propagating a set of smaller utility tables instead of a joint utility table, and to reduce the computa- tion efforts by sequential optimizations instead of joint optimizations. the empirical evaluation in- dicates that asymdpop signiﬁcantly outperforms the state-of-the-art, as well as the vanilla dpop with peav formulation. "
1905.11829," this paper presents a novel spline-based meshing technique that allows for usage of boundary-conforming meshes for unsteady ﬂow and temperature simulations in co-rotating twin-screw extruders. spline-based descriptions of arbitrary screw geometries are generated using elliptic grid generation. they are evaluated in a number of discrete points to yield a coarse classical mesh. the use of a special control mapping allows to ﬁne-tune properties of the coarse mesh like orthogonality at the boundaries. the coarse mesh is used as a ‘scaﬀolding’ to generate a boundary-conforming mesh out of a ﬁne background mesh at run-time. storing only a coarse mesh makes the method cheap in terms of memory storage. additionally, the adaptation at run-time is extremely cheap compared to computing the ﬂow solution. furthermore, this method circumvents the need for expensive re-meshing and projections of solutions making it eﬃcient and accurate. it is incorporated into a space-time ﬁnite element framework. we present time-dependent test cases of non-newtonian ﬂuids in 2d and 3d for complex screw designs. they demonstrate the potential of the method also for arbitrarily complex industrial applications. keywords: co-rotating twin-screw extruder, elliptic grid generation, isogeometric analysis, space-time finite elements, mesh update method, boundary-conforming finite elements, non-isothermal flow "
1905.11830," transportation cost is an attractive similarity measure between probability distributions due to its many useful theoretical properties. however, solving optimal transport exactly can be prohibitively expensive. therefore, there has been signiﬁcant effort towards the design of scalable approximation algorithms. pre- vious combinatorial results [sharathkumar, agarwal stoc ’12, agarwal, sharathkumar stoc ’14] have focused primarily on the design of strongly polynomial multiplicative approximation algorithms. there has also been an effort to design approximate solutions with additive errors [cuturi nips ’13, altschuler et al. nips ’17, dvurechensky et al. icml ’18, quanrud, sosa ’19] within a time bound that is linear in the size of the cost matrix and polynomial in c/δ; here c is the largest value in the cost matrix and δ is the additive error. we present an adaptation of the classical graph algorithm of gabow and tarjan and provide a novel analysis of this algorithm that bounds its execution time by o( n2c δ2 ). our algorithm is extremely simple and executes, for an arbitrarily small constant ε, only (cid:98) 2c (1−ε)δ(cid:99) + 1 iterations, where each iteration consists only of a dijkstra search followed by a depth-ﬁrst search. we also provide empirical results that suggest our algorithm signiﬁcantly outperforms existing approaches in execution time. δ + nc2 9 1 0 2   y a m 8 2       ] g l . s c [     1 v 0 3 8 1 1 . 5 0 9 1 : v i x r a ∗authors are ordered alphabetically by last name. all authors contributed equally. †department of computer science, virginia tech. email: lahnn@vt.edu ‡department of computer science, virginia tech. email: deepikak@vt.edu §department of computer science, virginia tech. email: sharathr@vt.edu 1 "
1905.11831,"—mouse dynamics is a potential means of authen- ticating users. typically, the authentication process is based on classical machine learning techniques, but recently, deep learning techniques have been introduced for this purpose. although prior research has demonstrated how machine learning and deep learning algorithms can be bypassed by carefully crafted adversarial samples, there has been very little research performed on the topic of behavioural biometrics in the adversarial domain. in an attempt to address this gap, we built a set of attacks, which are applications of several generative approaches, to construct adversarial mouse trajectories that bypass authentication models. these generated mouse sequences will serve as the adversarial samples in the context of our experiments. we also present an analysis of the attack approaches we explored, explaining their limitations. in contrast to previous work, we consider the attacks in a more realistic and challenging setting in which an attacker has access to recorded user data but does not have access to the authentication model or its outputs. we explore three different attack strategies: 1) statistics-based, 2) imitation- based, and 3) surrogate-based; we show that they are able to evade the functionality of the authentication models, thereby impacting their robustness adversely. we show that imitation- based attacks often perform better than surrogate-based attacks, unless, however, the attacker can guess the architecture of the authentication model. in such cases, we propose a potential detection mechanism against surrogate-based attacks. "
1905.11832," adversarial attacks have exposed a signiﬁcant security vulnerability in state-of-the- art machine learning models. among these models include deep reinforcement learning agents. the existing methods for attacking reinforcement learning agents assume the adversary either has access to the target agent’s learned parameters or the environment that the agent interacts with. in this work, we propose a new class of threat models, called snooping threat models, that are unique to reinforcement learning. in these snooping threat models, the adversary does not have the ability to personally interact with the environment, and can only eavesdrop on the action and reward signals being exchanged between agent and environment. we show that adversaries operating in these highly constrained threat models can still launch devastating attacks against the target agent by training proxy models on related tasks and leveraging the transferability of adversarial examples. 1 "
1905.11833," neural network models for nlp are typically implemented without the explicit encoding of language rules and yet they are able to break one performance record after another. despite much work, it is still unclear what the representations learned by these networks correspond to. we propose here a novel approach for interpreting neural networks that relies on the only processing system we have that does understand language: the human brain. we use brain imaging recordings of subjects reading complex natural text to interpret word and sequence embeddings from 4 recent nlp models - elmo, use, bert and transformer-xl. we study how their representations differ across layer depth, context length, and attention type. our results reveal differences in the context-related representations across these models. further, in the transformer models, we ﬁnd an interaction between layer depth and context length, and between layer depth and attention type. we ﬁnally use the insights from the attention experiments to alter bert: we remove the learned attention at shallow layers, and show that this manipulation improves performance on a wide range of syntactic tasks. cognitive neuroscientists have already begun using nlp networks to study the brain, and this work closes the loop to allow the interaction between nlp and cognitive neuroscience to be a true cross-pollination. "
1905.11835," in order to characterize 22 new globular cluster (gc) candidates in the galactic bulge, we present their colour-magnitude diagrams (cmds) and ks-band luminosity func- tions (lfs) using the near-infrared vvv database as well as gaia-dr2 proper motion dataset. cmds were obtained, on one hand, after properly decontaminating the ob- served diagrams from background/foreground disc stars and other sources. on the other hand, cmds were also obtained based upon star selection in proper motion diagrams. taking into account our deep cmds and lfs analyses, we ﬁnd that 17 out of 22 new gc candidates may be real and should therefore be followed-up, while 5 candidates were discarded from the original sample. we also search for rr lyrae and mira variable stars in the ﬁelds of these new gc candidates. in particular, we conﬁrm that minni 40 may be a real cluster. if conﬁrmed by further follow-up analysis, it would be the closest gc to the galactic centre in projected angular distance, located only 0.5 deg away from it. we consider that it is very diﬃcult to conﬁrm the physical reality of these small, poorly-populated bulge gcs so in many cases alternative techniques are needed to corroborate our ﬁndings. key words: galaxy: bulge — globular clusters: general "
1905.11836,":  in  this  paper  we  explain  how  we  deal  with  the  problems  related  to  the  constitution of the aliento database, the complexity of which has to do with the type  of phrases we work with, the differences between languages, the type of information  we  want  to  see  emerge.  the  correct  tagging  of  the  specific  polysemy  of  brief  sapiential units is an important step in the preparation of the text within the corpus  which will be submitted to compute similarities and posterity of the units.       "
1905.11837,"— principal component analysis (pca) has been used to study the pathogenesis of diseases. to enhance the inter- pretability of classical pca, various improved pca methods have been proposed to date. among these, a typical method is the so-called sparse pca, which focuses on seeking sparse loadings. however, the performance of these methods is still far from satisfactory due to their limitation of using unsupervised learning methods; moreover, the class ambiguity within the sample is high. to overcome this problem, this paper developed a new pca method, which is named the supervised discriminative sparse pca (sdspca). the main innovation of this method is the incorporation of discriminative information and sparsity into the pca model. speciﬁcally, in contrast to the traditional sparse pca, which imposes sparsity on the loadings, here, sparse components are obtained to represent the data. furthermore, via the linear transformation, the sparse components approximate the given label information. on the one hand, sparse components improve interpretability over the traditional pca, while on the other hand, they are have discriminative abilities suitable for classiﬁcation purposes. a simple algorithm is developed, and its convergence proof is provided. sdspca has been applied to the common-characteristic gene selection and tumor classiﬁcation on multiview biological data. the sparsity and classiﬁcation performance of sdspca are empirically veriﬁed via abundant, reasonable, and effective experiments, and the obtained results demonstrate that sdspca outperforms other state-of-the-art methods. manuscript received may 7, 2017; revised november 25, 2017, june 24, 2018, and october 8, 2018; accepted january 2, 2019. this work was supported by nsfc under grant 61872220, grant 61572284, and grant 61702299. (corresponding authors: yong xu; jin-xing liu.) c.-m. feng is with the bio-computing research center, harbin insti- tute of technology, shenzhen 518055, china (e-mail: fengchunmei0304@ foxmail.com). y. xu is with the bio-computing research center, shenzhen graduate school, harbin institute of technology, shenzhen 518055, china, and also with the key laboratory of network oriented intelligent computation, shenzhen 518055, china (e-mail: yongxu@ymail.com). j.-x. liu is with the school of information science and engineering, qufu normal university, rizhao 276826, china (e-mail: sdcavell@126.com). y.-l. gao is with the library of qufu normal university, qufu normal university, rizhao 276826, china (e-mail: yinliangao@126.com). c.-h. zheng is with the school of computer science and technology, anhui university, hefei 230000, china (e-mail: zhengch99@126.com). this paper has supplementary downloadable material http://ieeexplore.ieee.org, provided by the author. available at color versions of one or more of the ﬁgures in this paper are available online at http://ieeexplore.ieee.org. digital object identiﬁer 10.1109/tnnls.2019.2893190 index terms— common-characteristic (com-characteristic) gene component analysis (pca), sparse constraint, supervised learning, tumor classiﬁcation. selection, multiview data, principal "
1905.11838," we study the parameterized complexity of the optimal defense and optimal attack problems in voting. in both the problems, the input is a set of voter groups (every voter group is a set of votes) and two integers ka and kd corresponding to respectively the number of voter groups the attacker can attack and the number of voter groups the defender can defend. a voter group gets removed from the election if it is attacked but not defended. in the optimal defense problem, we want to know if it is possible for the defender to commit to a strategy of defending at most kd voter groups such that, no matter which ka voter groups the attacker attacks, the outcome of the election does not change. in the optimal attack problem, we want to know if it is possible for the attacker to commit to a strategy of attacking ka voter groups such that, no matter which kd voter groups the defender defends, the outcome of the election is always different from the original (without any attack) one. we show that both the optimal defense problem and the optimal attack problem are computationally intractable for every scoring rule and the condorcet voting rule even when we have only 3 candidates. we also show that the optimal defense problem for every scoring rule and the condorcet voting rule is w[2]-hard for both the parameters ka and kd, while it admits a ﬁxed parameter tractable algorithm parameterized by the combined parameter (ka, kd). the optimal attack problem for every scoring rule and the condorcet voting rule turns out to be much harder – it is w[1]-hard even for the combined parameter (ka, kd). we propose two greedy algorithms for the optimal defense problem and empirically show that they perform effectively on reasonable voting proﬁles. keywords and phrases parameterized complexity, election control, optimal attack, optimal defense 1 "
1905.11839,". let e 3 ⊂ t m n be a smooth 3-distribution on a smooth manifold of dimension n and w ⊂ e a line ﬁeld such that [w,e] ⊂ e. under some orientability hypothesis, we give a necessary condition for the existence of a plane ﬁeld d2 such that w ⊂ d and [d,d] = e. moreover we study the case where a section of w is non-singular morse-smale and we get a suﬃcient condition for the global existence of d. as a corollary we get conditions for a non-singular vector ﬁeld w on a 3-manifold to be legendrian for a contact structure d. similarly with these techniques we can study when an even contact structure e ⊂ t m 4 is induced by an engel structure d. "
1905.11840,": india and belgium have jointly established two 4 meter class optical telescopes at devasthal lo- cated in nainital, india. after successful installation of the 3.6-m modern new technology devasthal optical telescope (dot) in 2015, it was technically activated by premiers of both countries from brussels on march 30, 2016. since then, the 3.6-m dot has been used for both optical and near-infrared (nir) observations for a number of research proposals. the best angular resolution achieved is 0.(cid:48)(cid:48)4 indicating that the optics of the 3.6-m dot is good and capable of providing images of the celestial bodies with sub-arc-second resolution. the observations provide proof that the care taken in the construction of the telescope dome building has paid a rich dividend as their thermal mass is so low that it has not degraded the natural atmospheric seeing at devasthal measured about two decades ago during 1997 to 1999 using differential image motion monitor. a few preliminary scientiﬁc results obtained from recent observations are presented along with perfor- mance and global potential of the 3.6-m dot in the ﬁeld of galactic astronomy. the 3.6-m dot is capable of providing internationally competitive science once high resolution spectrograph and other planned modern back-end instruments become operational. geographical location of the observatory has global importance for the time domain and multi-wavelength astrophysical studies. keywords: galactic astrophysics – devasthal observatory – optical – near-infrared 1 "
1905.11841,". in his paper [1], markus reineke proposed a conjecture that exists a stable weight system θ for every indecomposable representation of dynkin type quiver. in this paper, we showed that the conjecture is true for quivers of an-type. contents 1. "
1905.11842," assessing world-wide ﬁnancial integration constitutes a recurrent challenge in macroeconometrics, often addressed by visual inspections searching for data patterns. econophysics literature enables us to build complementary, data-driven measures of ﬁnancial integration using graphs. the present con- tribution investigates the potential and interests of a novel 3-step approach that combines several state-of-the-art procedures to i) compute graph-based representations of the multivariate dependence structure of asset prices time series representing the ﬁnancial states of 32 countries world-wide (1955-2015); ii) compute time series of 5 graph-based indices that characterize the time evolution of the topologies of the graph; iii) segment these time evolutions in piece-wise constant eras, using an optimization framework constructed on a multivariate multi-norm total variation penalized functional. the method shows ﬁrst that it is possible to ﬁnd endogenous stable eras of world-wide ﬁnancial integration. then, our results suggest that the most relevant global- ization eras would be based on the historical patterns of global capital ﬂows, while the major regulatory events of the 1970s would only appear as a cause of sub-segmentation. keywords: financial integration; graph topology; time segmentation; multivariate time series; econophysics preprint submitted to physica a may 29, 2019 "
1905.11843," detailed applications of minisuperspace methods are presented and compared with results obtained in recent years by means of causal dynamical triangulations (cdts), mainly in the form of eﬀective actions. the analysis sheds light on con- ceptual questions such as the treatment of time or the role and scaling behavior of statistical and quantum ﬂuctuations. in the case of ﬂuctuations, several analytical and numerical results show agreement between the two approaches and oﬀer possible explanations of eﬀects that have been seen in causal dynamical triangulations but whose origin remained unclear. the new approach followed here suggests “cdt ex- periments” in the form of new simulations or evaluations motivated by theoretical predictions, testing cdts as well as the minisuperspace approximation. 1 "
1905.11844," grb 190114c is a famous gamma-ray burst (grb) due to its detection at sub-t ev energies by magic, seen at redshift z = 0.42. this burst is one of the brightest grb detected by fermi . we present a detailed analysis of grb 190114c prompt emission, using the two fermi detectors: gbm and lat. the lat low energy events (lle) data is also considered. a joint gbm-lat analysis reveals a sub-gev spectral cutoﬀ. a similar high energy cutoﬀ was reported in grb 160509a and grb 100724b earlier, and a handful of other sources. the cutoﬀ can be explained by the intrinsic opacity due to pair production within the emitting region. such morphology in these grbs suggests that they belong to one speciﬁc class having a similar source of the radiation mechanism. grb 190114c shows a transition from non-thermal to a quasi-thermal-like spectrum along with radiation due to external shock. from spectrum analysis and lorentz factor evolution from the trigger time to late emission, considering the fact that sub-t ev photons are detected in magic, we are able to draw an emission mechanism picture, where the prompt emission spectrum is more consistent with spectrum via photospheric dissipation with presence of external shock emission simultaneously. keywords: gamma-ray burst: grb 190114c , grb 160509a radiation mechanisms: thermal, non- thermal methods: data analysis "
1905.11845," asynchronous distributed methods are a popular way to reduce the communication and synchronization costs of large-scale optimization. yet, for all their success, little is known about their convergence guarantees in the challenging case of general non-smooth, non-convex objectives, beyond cases where closed-form proximal operator solutions are available. this is all the more surprising since these objectives are the ones appearing in the training of deep neural networks. in this paper, we introduce the ﬁrst convergence analysis covering asynchronous methods in the case of general non-smooth, non-convex ob- jectives. our analysis applies to stochastic sub-gradient descent methods both with and without block variable partitioning, and both with and without momentum. it is phrased in the context of a general probabilistic model of asynchronous scheduling accurately adapted to modern hard- ware properties. we validate our analysis experimentally in the context of training deep neural network architectures. we show their overall successful asymptotic convergence as well as exploring how momentum, synchronization, and partitioning all aﬀect performance. 1 "
1905.11846," notions of depth in regression have been introduced and studied in the literature. regression depth (rd) of rousseeuw and hubert (1999) (rh99), the most famous, exempliﬁes a direct extension of tukey location depth (tukey (1975)) to regression. the extension of another prevailing location depth, the projection depth (liu (1992), and zuo and serﬂing (2000)), to regression is called the projection regression depth (prd) (zuo (2018a)). the computation issues of rd of rh99 have been discussed in rh99, rousseeuw and struyf (1998), and liu and zuo (2014). those of prd have never been dealt with. this article addresses the computation issues of prd and its induced median (maximum depth estimator) in a regression setting, proposing exact algorithms for prd with cost o(np+1) and ap- proximate algorithms for prd and its induced median with cost o(nvn) and o(r(nvnβn + pnβ + nvnitern)), respectively, where nv is the total number of unit directions v tried, nβ is the total number of candidate regression parameters β tried, niter is the total number of iterations carried out in an optimization algorithm, and r is the total number of replications. examples and a simulation study reveal that the maximum depth estimator induced from prd is favorable in terms of robustness and eﬃciency, compared with its major competitor, the maximum depth estimator induced from rd. ams 2000 classiﬁcation: primary 62g08, 62g99; secondary 62j05, 62j99. key words and phrase: regression depth, maximum depth estimator, computation, approximate and exact algorithms. running title: computation of maximum regression depth estimator. 1 "
1905.11847,". preﬁx normal words are binary words in which each preﬁx has at least the same number of 1s as any factor of the same length. firstly introduced by fici and lipt´ak in 2011, the problem of deter- mining the index of the preﬁx equivalence relation is still open. in this paper, we investigate two aspects of the problem, namely preﬁx nor- mal palindromes and so-called collapsing words (extending the notion of critical words). we prove characterizations for both the palindromes and the collapsing words and show their connection. based on this, we show that still open problems regarding preﬁx normal words can be split into certain subproblems. 1 "
1905.11851,". arising from the factorizations of dedekind zeta-functions associated with non-galois cubic ﬁelds, we obtain artin l-functions of two-dimensional representations. in this paper, we study the value-distribution of such artin l-functions in the aspect of discriminants of cubic ﬁelds. we prove that averages of values of the artin l-functions are expressed by integrals involving a density function which can be explicitly described. as a corollary, we obtain asymptotic formulas of counting functions for certain families of cubic ﬁelds. "
1905.11852," ludovic denoyer facebook ai research denoyer@fb.com with the advent of deep neural networks, some research focuses towards un- derstanding their black-box behavior. in this paper, we propose a new type of self-interpretable models, that are, architectures designed to provide explanations along with their predictions. our method proceeds in two stages and is trained end-to-end: ﬁrst, our model builds a low-dimensional binary representation of any input where each feature denotes the presence or absence of concepts. then, it computes a prediction only based on this binary representation through a simple linear model. this allows an easy interpretation of the model’s output in terms of presence of particular concepts in the input. the originality of our approach lies in the fact that concepts are automatically discovered at training time, without the need for additional supervision. concepts correspond to a set of patterns, built on local low-level features (e.g a part of an image, a word in a sentence), easily identiﬁable from the other concepts. we experimentally demonstrate the relevance of our approach using classiﬁcation tasks on two types of data, text and image, by showing its predictive performance and interpretability. 1 "
1905.11853,". we provide algorithms to reconstruct rational ruled surfaces in three-dimensional projective space from the “apparent contour” of a single projection to the projective plane. we deal with the case of tangent devel- opables and of general projections to p3 of rational normal scrolls. in the ﬁrst case, we use the fact that every such surface is the projection of the tangent developable of a rational normal curve, while in the second we start by re- constructing the rational normal scroll. in both instances we then reconstruct the correct projection to p3 of these surfaces by exploiting the information contained in the singularities of the apparent contour. "
1905.11855,"  fundamental  working  mechanisms  of  perovskite  solar  cells  remain  an  elusive  topic of research. impedance spectroscopy (is) has been key to characterize the main  physical processes governing the behavior of dye and quantum dot sensitized solar cells,  which  are  considered,  in  many  aspects,  predecessor  technologies.  in  contrast,  is  application  to  perovskite-based  devices  generates  uncommon  features  and  misleading  outputs, mainly due to the lack of a stablished model for the results interpretation. in this  study we fabricate a series of perovskite-based devices ranging from sensitized to thin- film perovskite solar cells by the control of the perovskite precursor concentration. this  transition  is  characterized  by  a  change  in  the  working  principles,  determined  by  an  evolution  of  the  dominant  capacitance:  from  the  intermediate  frequency  chemical  capacitance of tio2 in devices with isolated perovskite domains, to a large low-frequency    1  capacitance  which  divides  the  spectra  in two  sections. this  study  gives  access  to  the  characteristic features of the system, which we leverage to provide a rationalized is model  with the focus on transport and recombination processes. the model, by means of an ac  branch,  paves  the  way  to  study  the  material  and  interfacial  properties  distinctive  of  perovskite  solar  cells.  concurrently,  it  provides  for  the  first  time  the  distribution  of  transport-related and recombination-related losses, a crucial tool for further development  of perovskite photovoltaics.     "
1905.11858,"—we showcase the practicability of an indoor posi- tioning system (ips) solely based on neural networks (nns) and the channel state information (csi) of a (massive) multiple-input multiple-output (mimo) communication system, i.e., only build on the basis of data that is already existent in today’s systems. as such our ips system promises both, a good accuracy without the need of any additional protocol/signaling overhead for the user localization task. in particular, we propose a tailored nn struc- ture with an additional phase branch as feature extractor and (compared to previous results) a signiﬁcantly reduced amount of trainable parameters, leading to a minimization of the amount of required training data. we provide actual measurements for indoor scenarios with up to 64 antennas covering a large area of 80 m2. in the second part, several robustness investigations for real-measurements are conducted, i.e., once trained, we analyze the recall accuracy over a time-period of several days. further, we analyze the impact of pedestrians walking in-between the measurements and show that ﬁnetuning and pre-training of the nn helps to mitigate effects of hardware drifts and alterations in the propagation environment over time. this reduces the amount of required training samples at equal precision and, thereby, decreases the effort of the costly training data acquisition. "
1905.11860,". in order to study projections of smooth curves, we introduce mul- tiﬁltrations obtained by combining ﬂags of osculating spaces. we ﬁrst use these multiﬁltrations to show that under the assumption 2ρg − 2 < d < 2n, the arithmetic genus of any non-degenerate degree d curve in pn of geometric genus ρg is at most d − n. we classify all conﬁgurations of singularities occur- ring for a projection of a smooth curve embedded by a complete linear system away from a projective linear space of dimension at most two. in particular, we determine all conﬁgurations of singularities of non-degenerate degree d ra- tional curves in pn when d − n ≤ 3 and d < 2n. along the way, we describe the schubert cycles giving rise to these projections. "
1905.11861," k∗(m ) → k∗(c∗γ) → ··· . under suitable assumptions on the group γ we construct two pairings, ﬁrst let γ be a ﬁnitely generated discrete group and let (cid:102)m be a galois γ-covering of a smooth compact manifold m . let u : x → bγ be the associated classifying map. finally, let sγ∗ ((cid:102)m ) be the analytic structure group, a k-theory group appearing in the higson-roe analytic surgery sequence ··· → sγ∗ ((cid:102)m ) → between sγ∗ ((cid:102)m ) and the delocalized part of the cyclic cohomology of the complex group ring of γ, and secondly between sγ∗ ((cid:102)m ) and the relative cohomology h∗(m → bγ). both are compatible with known numbers associated to the rho class ρ((cid:101)d) ∈ sγ∗ ((cid:102)m ) of an invertible γ-equivariant dirac type operator on (cid:102)m . this applies, for example, to the rho class ρ(g) of a positive scalar curvature metric g on m , if m pairings associated with the other terms in the higson-roe sequence. in particular, we deﬁne higher rho is spin. regarding the ﬁrst pairing we establish in fact a more general result, valid without additional assumptions on γ: indeed, we prove that it is possible to map the higson-roe analytic surgery sequence to the long exact sequence in even/odd-graded noncommutative de rham homology j∗ / ··· / h del∗−1(aγ) with aγ a dense holomorphically closed subalgebra of c∗ of the delocalized homology and the homology localized at the identity element, respectively. r γ and h del∗ (aγ) and h e∗ (aγ) denoting versions / h∗−1(aγ) / h e∗ (aγ) / ··· j∗ δ ι contents "
1905.11862," recently, the applications of person re-identiﬁcation in visual surveillance and human-computer interaction are sharply in- creasing, which signiﬁes the critical role of such a problem. in this paper, we propose a two-stream convolutional net- work (convnet) based on the competitive similarity aggrega- tion scheme and co-attentive embedding strategy for video- based person re-identiﬁcation. by dividing the long video sequence into multiple short video snippets, we manage to utilize every snippet’s rgb frames, optical ﬂow maps and pose maps to facilitate residual networks, e.g., resnet, for feature extraction in the two-stream convnet. the extracted features are embedded by the co-attentive embedding method, which allows for the reduction of the effects of noisy frames. finally, we fuse the outputs of both streams as the embed- ding of a snippet, and apply competitive snippet-similarity aggregation to measure the similarity between two sequences. our experiments show that the proposed method signiﬁcantly outperforms current state-of-the-art approaches on multiple datasets. index terms— person re-identiﬁcation, two-stream con- volutional network, co-attentive embedding. "
1905.11864," in the past, laboratory experiments and theoretical calculations showed a mismatch in derived stick- ing properties of silicates in the context of planetesimal formation. it has been proposed by kimura et al. (2015) that this mismatch is due to the value of the surface energy assumed, supposedly cor- related to the presence or lack of water layers of diﬀerent thickness on a grain’s surface. we present tensile strength measurements of dust aggregates with diﬀerent water content here. the results are in support of the suggestion by kimura et al. (2015). dry samples show increased strengths by a factor of up to 10 over wet samples. a high value of γ = 0.2 j/m2 likely applies to the dry low pressure conditions of protoplanetary disks and should be used in the future. keywords: methods: miscellaneous, planets and satellites: formation, planets and satellites: funda- mental parameters, protoplanetary disks "
1905.11865," high-intensity secondary beams play a vital role in today’s particle physics and materials science research and require suitable detection techniques to adjust beam characteristics to optimally match experimental conditions. to this end we have developed a non-invasive, ultra-thin, csi(tl) luminophore foil detector system, based on ccd-imaging. we have used this to quantify the beam characteristics of an intensity-frontier surface muon beam used for next-generation charged lepton-ﬂavour violation (clfv) search experiments at the paul scherrer institut (psi) and to assess the possible use for a future high-intensity muon beam (himb-project), currently under study at psi. an overview of the production and intrinsic characteristics of such foils is given and their application in a high-intensity beam environment. keywords: luminophore, beam diagnostics, high intensity, csi(tl), thin scintillator ﬁlms "
1905.11866," in semi-supervised classiﬁcation, one is given access both to labeled and unlabeled data. as unlabeled data is typically cheaper to acquire than labeled data, this setup becomes advantageous as soon as one can exploit the unlabeled data in order to produce a better classiﬁer than with labeled data alone. however, the conditions under which such an im- provement is possible are not fully understood yet. our analysis focuses on improvements in the minimax learning rate in terms of the number of labeled examples (with the number of unlabeled examples being allowed to depend on the number of labeled ones). we argue that for such improvements to be realistic and indisputable, certain speciﬁc conditions should be satisﬁed and previous analyses have failed to meet those conditions. we then demonstrate examples where these conditions can be met, in particular showing rate changes from 1/√ℓ to e−cℓ and from 1/√ℓ to 1/ℓ. these results improve our understanding of what is and isn’t possible in semi-supervised learning. 1 "
1905.11867," we study the problem of inverse reinforcement learning (irl) with the added twist that the learner is assisted by a helpful teacher. more formally, we tackle the following algorithmic question: how could a teacher provide an informative sequence of demonstrations to an irl learner to speed up the learning process? we present an interactive teaching framework where a teacher adaptively chooses the next demonstration based on learner’s current pol- icy. in particular, we design teaching algorithms for two concrete settings: an omniscient setting where a teacher has full knowledge about the learner’s dy- namics and a blackbox setting where the teacher has minimal knowledge. then, we study a sequential variant of the popular mce-irl learner and prove convergence guarantees of our teaching algorithm in the omniscient setting. extensive experiments with a car driving simulator environment show that the learning progress can be speeded up drastically as compared to an uninformative teacher. "
1905.11868," we analyze a system of stochastic differential equations describing the joint motion of a massive (inert) particle in a viscous ﬂuid in the presence of a gravitational ﬁeld and a brow- nian particle impinging on it from below, which transfers momentum proportional to the local time of collisions. we study the long-time ﬂuctuations of the velocity of the inert par- ticle and the gap between the two particles, and we show convergence in total variation to the stationary distribution is exponentially fast. we also produce matching upper and lower bounds on the tails of the stationary distribution and show how these bounds depend on the system parameters. a renewal structure for the process is established, which is the key technical tool in proving the mentioned results. keywords and phrases: reﬂected brownian motion; viscosity; gravitation; local time; inert drift; renewal time; exponential ergodicity; regenerative process; total variation distance; harris recurrence; petite sets. 2010 mathematics subject classiﬁcation: primary 60j60, 60k05; secondary 60j55, 60h20. contents "
1905.11869, we present a method for calculating the brauer group of a surface given by a diagonal equation in projective space. for diagonal quartic surfaces with coeﬃcients in q we determine the brauer groups over q and q(i). contents 1 surfaces with torsion-free picard group 1.1 cohomological tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 brauer group of varieties dominated by products of curves 6 6 . . . . . . 11 2 cohomology of fermat surfaces 12 2.1 primitive cohomology and the cup-product . . . . . . . . . . . . . . . 12 2.2 hodge decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.3 recovering full cohomology from primitive cohomology . . . . . . . . 15 2.4 galois representation over the cyclotomic ﬁeld . . . . . . . . . . . . . 17 3 the fermat quartic surface 19 3.1 explicit transcendental lattice . . . . . . . . . . . . . . . . . . . . . . 19 3.2 explicit galois representation . . . . . . . . . . . . . . . . . . . . . . 20 3.3 diagonal quartic surfaces . . . . . . . . . . . . . . . . . . . . . . . . . 23 4 main theorem 27 4.1 reduction to ﬁnite group cohomology . . . . . . . . . . . . . . . . . . 27 4.2 the case of q(i) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 4.3 the case of q . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 1 
1905.11870," we present a kinematical study of rr lyrae stars associated with two oosterhoﬀ groups in the galactic bulge. we used data published in the ﬁrst paper of the series, plus proper motions from the gaia data release 2, and radial velocities from the literature. a 6d kinematical and spatial solution was obtained for 429 rr lyrae stars. we use a model of the galactic gravitational potential to infer stellar orbits. we did not ﬁnd a diﬀerence between the oosterhoﬀ groups in the individual components of the space velocity. we report that foreground and background stars with respect to the galactic bulge stand out in the mean v velocity component, which we interpret as a sign of the galactic rotation. the movement of the studied stars in the central region of the galactic bulge is consistent with random motions expected for a classical bulge component. from the orbital integration, we estimate that 8 % of the rr lyrae stars are halo interlopers currently located in the galactic bulge. the majority of the stars’ orbits are within a 3 kpc radius from the galactic bulge. the fraction of oosterhoﬀ ii stars increases with increasing galactic latitude, as well as towards longer orbital periods. we found several rr lyrae stars with high space velocities, one of which has an extremely long orbital period of ∼1 gyr. we conclude that based on their kinematics, the vast majority of the stars in our sample do not seem to contribute to the boxy/peanut component of the galactic bulge. key words: galaxy: bulge – galaxy: kinematics and dynamics – stars: variables: rr lyrae "
1905.11871," recent research studies communication emer- gence in communities of deep network agents assigned a joint task, hoping to gain insights on human language evolution. we propose here a new task capturing crucial aspects of the human environment, such as natural ob- ject affordances, and of human conversation, such as full symmetry among the participants. by conducting a thorough pragmatic and se- mantic analysis of the emergent protocol, we show that the agents solve the shared task through genuine bilateral, referential commu- nication. however, the agents develop mul- tiple idiolects, which makes us conclude that full symmetry is not a sufﬁcient condition for a common language to emerge. 1 "
1905.11872," following the works by lin et al. (circuits syst. signal process. 20(6): 601-618, 2001) and liu et al. (circuits syst. signal process. 30(3): 553-566, 2011), we investigate how to factorize a class of multivariate polynomial matrices. the main theorem in this paper shows that an l × m polynomial matrix admits a factorization with respect to a polynomial if the polynomial and all the (l − 1) × (l − 1) reduced minors of the matrix generate the unit ideal. this result is a further generalization of previous works, and based on this, we give an algorithm which can be used to factorize more polonomial matrices. in addition, an illustrate example is given to show that our main theorem is non-trivial and valuable. keywords: multivariate polynomial matrices, matrix factorizations, reduced minors, reduced gr¨obner basis "
1905.11873," as the size and source of network traﬃc increase, so does the chal- lenge of monitoring and analysing network traﬃc. therefore, sampling algorithms are often used to alleviate these scalability issues. however, the use of high entropy data streams, through the use of either encryption or compression, further compounds the challenge as current state of the art algorithms cannot accurately and eﬃciently diﬀerentiate between en- crypted and compressed packets. in this work, we propose a novel traﬃc classiﬁcation method named hedge (high entropy distinguisher) to distinguish between compressed and encrypted traﬃc. hedge is based on the evaluation of the randomness of the data streams and can be ap- plied to individual packets without the need to have access to the entire stream. findings from the evaluation show that our approach outperforms current state of the art. we also make available our statistically sound dataset, based on known benchmarks, to the wider research community. 1 "
1905.11874," (cid:139)ality-diversity optimization is a new family of optimization al- gorithms that, instead of searching for a single optimal solution to solving a task, searches for a large collection of solutions that all solve the task in a di(cid:130)erent way. (cid:140)is approach is particularly promising for learning behavioral repertoires in robotics, as such a diversity of behaviors enables robots to be more versatile and resilient. however, these algorithms require the user to manually de(cid:128)ne behavioral descriptors, which is used to determine whether two solutions are di(cid:130)erent or similar. (cid:140)e choice of a behavioral de- scriptor is crucial, as it completely changes the solution types that the algorithm derives. in this paper, we introduce a new method to automatically de(cid:128)ne this descriptor by combining (cid:139)ality-diversity algorithms with unsupervised dimensionality reduction algorithms. (cid:140)is approach enables robots to autonomously discover the range of their capabilities while interacting with their environment. (cid:140)e results from two experimental scenarios demonstrate that robot can autonomously discover a large range of possible behaviors, without any prior knowledge about their morphology and environment. furthermore, these behaviors are deemed to be similar to hand- cra(cid:137)ed solutions that uses domain knowledge and signi(cid:128)cantly more diverse than when using existing unsupervised methods. keywords behavioral repertoires, (cid:139)ality-diversity optimization, evolution- ary robotics, deep learning, representation learning, auto-encoders. acm reference format: antoine cully. 2019. autonomous skill discovery with (cid:139)ality-diversity and unsupervised descriptors. in proceedings of genetic and evolutionary computation conference, prague, czech republic, july 13–17, 2018 (gecco ’19), 9 pages. doi: 10.1145/3321707.3321804 "
1905.11875," determining a lack of association between an outcome variable and a number of diﬀerent explanatory variables is frequently necessary in order to disregard a pro- posed model (i.e., to conﬁrm the lack of an association between an outcome and predictors). despite this, the literature rarely oﬀers information about, or technical recommendations concerning, the appropriate statistical methodology to be used to accomplish this task. this paper introduces non-inferiority tests for anova and linear regression analyses, that correspond to the standard widely used f -test for ˆη2 and r2, respectively. a simulation study is conducted to examine the type i error rates and statistical power of the tests, and a comparison is made with an alternative bayesian testing approach. the results indicate that the proposed non- inferiority test is a potentially useful tool for “testing the null.” keywords equivalence testing, non-inferiority testing, anova, f -test, linear regression contact harlan campbell. email: harlan.campbell@stat.ubc.ca "
1905.11876," we consider bayesian classiﬁcation with gaussian processes (gps) and deﬁne robustness of a classiﬁer in terms of the worst-case difference in the classiﬁcation probabilities with respect to input perturbations. for a subset of the input space t ⊆ rm such properties reduce to computing the inﬁmum and supremum of the classiﬁcation probabilities for all points in t . unfortunately, computing the above values is very challenging, as the classiﬁcation probabilities cannot be expressed analytically. nevertheless, using the theory of gaussian processes, we develop a framework that, for a given dataset d, a compact set of input points t ⊆ rm and an error threshold  > 0, computes lower and upper bounds of the classiﬁcation probabilities by over-approximating the exact range with an error bounded by . we provide experimental comparison of several approximate inference methods for classiﬁcation on tasks associated to mnist and spam datasets showing that our results enable quantiﬁcation of uncertainty in adversarial classiﬁcation settings. 1 "
1905.11877," we study the problem of chasing convex bodies online: given a sequence of convex bodies kt ⊆ rd the algorithm must respond with points xt ∈ kt in an online fashion (i.e., xt is chosen before kt+1 is revealed). the objective is to minimize the total distance between successive points in this sequence. recently, bubeck et al. (stoc 2019) gave a 2o(d)-competitive algorithm for this problem. we give an algorithm that is o(min(d, d log t ))-competitive for any sequence of length t . √ 1 "
1905.11878," we present 20 newly discovered candidates for binary systems with an rr lyrae com- panion. using the photometric data from the optical gravitational lensing experi- ment (ogle) and korea microlensing telescope network (kmtnet) for the galactic bulge, we searched for binary candidates among non-modulated fundamental-mode rr lyrae variables. we constructed and investigated over 9000 o − c diagrams for individual pulsators, in order to ﬁnd signs of the light-travel time eﬀect. we found 20 variables exhibiting cyclic variation in the o−c (time of the observed minus calculated brightness maximum) diagram, which can be associated with a second component in the system, but for conﬁrmation of binarity, a long-term radial velocity study is nec- essary. the modeling of the o−c diagrams yields orbital parameters, estimates of the semi-amplitude of the radial velocity curve, and the mass function. the orbital peri- ods of our candidates range from 3 to 15 years. two of the binary candidates display a minimum mass higher than the assumed mass of the rr lyrae component, which points towards an evolved companion that can under some circumstances contribute signiﬁcantly to the total light of the system. key words: binaries: general – stars: variables: rr lyrae – stars: variables: hori- zontal branch "
1905.11880," modern malware can take various forms, and has reached a very high level of sophistication in terms of its penetration, persistence, communi- cation and hiding capabilities. the use of cryptography, and of covert communication channels over public and widely used protocols and ser- vices, is becoming a norm. in this work, we start by introducing re- source identiﬁer generation algorithms. these are an extension of a well-known mechanism called domain generation algorithms, which are frequently employed by cybercriminals for bot management and commu- nication. our extension allows, beyond dns, the use of other protocols. more concretely, we showcase the exploitation of the interplanetary ﬁle system (ipfs). this is a solution for the “permanent web”, which enjoys a steadily growing community interest and adoption. the ipfs is, in ad- dition, one of the most prominent solutions for blockchain storage. we go beyond the straightforward case of using the ipfs for hosting mali- cious content, and explore ways in which a botmaster could employ it, to manage her bots, validating our ﬁndings experimentally. finally, we discuss the advantages of our approach for malware authors, its eﬃcacy and highlight its extensibility for other distributed storage services. 1 "
1905.11881," jzhzhang@mit.edu tianxing@mit.edu suvrit@mit.edu jadbabai@mit.edu we provide a theoretical explanation for the fast convergence of gradient clip- ping and adaptively scaled gradient methods commonly used in neural network training. our analysis is based on a novel relaxation of gradient smoothness con- ditions that is weaker than the commonly used lipschitz smoothness assumption. we validate the new smoothness condition in experiments on large-scale neural network training applications where adaptively-scaled methods have been empir- ically shown to outperform standard gradient based algorithms. under this new smoothness condition, we prove that two popular adaptively scaled methods, gra- dient clipping and normalized gradient, converge faster than the theoretical lower bound of ﬁxed-step gradient descent. we verify this fast convergence empirically in neural network training for language modeling and image classiﬁcation. "
1905.11882,". we prove several fundamental statistical bounds for entropic ot with the squared euclidean cost between subgaussian probability measures in arbitrary dimension. first, through a new sample com- plexity result we establish the rate of convergence of entropic ot for empirical measures. our analysis improves exponentially on the bound of genevay et al. (2019) and extends their work to unbounded mea- sures. second, we establish a central limit theorem for entropic ot, based on techniques developed by del barrio and loubes (2019). pre- viously, such a result was only known for ﬁnite metric spaces. as an application of our results, we develop and analyze a new technique for estimating the entropy of a random variable corrupted by gaussian noise. ams 2000 subject classiﬁcations: statistics. key words and phrases: optimal transport, entropic regularization, central limit theorem, sample complexity. "
1905.11883,": photovoltaic (pv) systems depend on irradiance, ambient temperature and module temperature. a solar eclipse causes signiﬁcant changes in these parameters, thereby impacting pv generation proﬁle, performance, and power quality of larger grid where they connect to. this paper presents a case study to evaluate the impacts of the solar eclipse of august 21, 2017 on two real-world grid-tied pv systems (1.4mw and 355kw) in miami and daytona, florida, the feeders they are connected to, and the management areas they belong to. four types of analyses are conducted to obtain a comprehensive picture of the impacts using 1-minute pv generation data, hourly weather data, real feeder parameters, and daily reliability data. these analyses include: individual pv system performance measurement using power performance index; power quality analysis at the point of interconnection; a study on the operation of voltage regulating devices on the feeders during eclipse peak using an ieee 8500 test case distribution feeder; and reliability study involving a multilayer perceptron framework for forecasting system reliability of the management areas. results from this study provide a unique insight into how solar eclipses impact the behavior of pv systems and the grid, which would be of concern to electric utilities in future high penetration scenarios. 1 "
1905.11884," detector developments are currently enabling new capabilities in the ﬁeld of transmission electron microscopy (tem). we have investigated the limits of a hybrid pixel detector, medipix3, to record dynamic, time varying, electron signals. operating with an energy of 60 kev, we have utilised electrostatic deﬂection to oscillate electron beam position on the detector. adopting a pump-probe imaging strategy we have demonstrated that temporal resolutions three orders of magnitude smaller than available for typically used tem imaging detectors are possible. our experiments have shown that energy deposition of the primary electrons in the hybrid pixel detector limits overall temporal resolution. through adjustment of user speciﬁable thresholds or the use of charge summing mode, we have obtained images composed from summing 10,000s frames containing single electron events to achieve temporal resolution less than 100 ns. we propose that this capability can be directly applied to studying repeatable material dynamic processes but also to implement low-dose imaging schemes in scanning transmission electron microscopy. keywords: time resolved, medipix3, pixelated detector, direct electron detector, transmission electron microscopy, stroboscopic imaging "
1905.11885," sorting an array is a fundamental routine in machine learning, one that is used to compute rank-based statistics, cumulative distribution functions (cdfs), quantiles, or to select closest neighbors and labels. the sorting function is however piece- wise constant (the permutation of a vector does not change if the entries of that vector are inﬁnitesimally perturbed) and therefore has no gradient information to backpropagate. we propose a framework to sort elements that is algorithmically differentiable. we leverage the fact that sorting can be seen as a particular instance of the optimal transport (ot) problem on r, from input values to a predeﬁned array of sorted values (e.g. 1, 2, . . . , n if the input array has n elements). building upon this link , we propose generalized cdfs and quantile operators by varying the size and weights of the target presorted array. because this amounts to using the so-called kantorovich formulation of ot, we call these quantities k-sorts, k-cdfs and k-quantiles. we recover differentiable algorithms by adding to the ot problem an entropic regularization, and approximate it using a few sinkhorn iterations. we call these operators s-sorts, s-cdfs and s-quantiles, and use them in various learning settings: we benchmark them against the recently proposed neu- ralsort [11], propose applications to quantile regression and introduce differentiable formulations of the top-k accuracy that deliver state-of-the art performance. "
1905.11886," in the cdm paradigm, the halo mass function is a sensitive probe of the cosmic structure. in observations, halo mass is typically estimated from its relation with other observables. the resulting halo mass function is subject to systematic bias, such as the eddington bias, due to the scatter or uncertainty in the observable - mass relation. exact correction for the bias is not easy, as predictions for the observables are typically model-dependent in simulations. in this paper, we point out an interesting feature in the halo mass function of the concordence λcdm model: the total halo mass within each evenly-spaced logarithmic mass bin is approximately the same over a large mass range. we show that this property allows us to construct an almost bias-free halo mass function using only an observable (as a halo mass estimator) and stacked weak lensing measurements as long as the scatter between the true halo mass and the observable-inferred mass has a stable form in logarithmic units. the method is not sensitive to the form of the mass-observable relation. we test the idea using cosmological simulations, and show that the method performs very well for realistic observables. subject headings: cosmology: halo mass function - statistics: eddington bias - gravitational lensing: weak "
1905.11887," we give a formal and complete characterization of the explicit regularizer induced by dropout in deep linear networks with squared loss. we show that (a) the explicit regularizer is composed of an (cid:96)2-path regularizer and other terms that are also re- scaling invariant, (b) the convex envelope of the induced regularizer is the squared nuclear norm of the network map, and (c) for a sufﬁciently large dropout rate, we characterize the global optima of the dropout objective. we validate our theoretical ﬁndings with empirical results. "
1905.11888," we consider the problems of distribution estimation and heavy hitter (frequency) estimation under privacy and communication constraints. while these constraints have been studied sep- arately, optimal schemes for one are sub-optimal for the other. we propose a sample-optimal ε-locally diﬀerentially private (ldp) scheme for distribution estimation, where each user com- municates only one bit, and requires no public randomness. we show that hadamard response, a recently proposed scheme for ε-ldp distribution estimation is also utility-optimal for heavy hitter estimation. finally, we show that unlike distribution estimation, without public ran- domness where only one bit suﬃces, any heavy hitter estimation algorithm that communicates o(min{log n, log k}) bits from each user cannot be optimal. "
1905.11889,": we investigate the pair production of right-handed neutrinos from the decay of an additional neutral z(cid:48) boson in the gauged b − l model. taking into account current constraints on the z(cid:48) mass and the associated gauge coupling g(cid:48) 1, we analyse the sensitivity of proposed experiments at the lifetime frontier, faser 2, codex-b, mathusla as well as a hypothetical version of the mapp detector to a long lived heavy neutrino n originating in the decays of the z(cid:48) . we further complement this study with determining the reach of lhcb and a cms-type detector for the high-luminosity lhc run. we demonstrate that in a background free scenario with g(cid:48) 1 = 10−3 near the current limit, faser 2 is sensitive to the active-sterile neutrino mixing down to vµn ≈ 10−4, while a reach of vµn ≈ 10−5 can be obtained for codex-b and lhcb, in a mass regime of mn ≈ 5 − 20 gev and mz(cid:48) ≈ 20 − 70 gev. finally, mathusla can probe vµn ≈ 10−7 and cover the mixing regime expected in a canonical seesaw scenario of light neutrino mass generation. 9 1 0 2   y a m 8 2       ] h p - p e h [     1 v 9 8 8 1 1 . 5 0 9 1 : v i x r a contents "
1905.11895," a mesh reﬁnement method is developed for solving bang-bang optimal control problems using direct collocation. the method starts by ﬁnding a solution on a coarse mesh. using this initial solution, the method then determines automatically if the hamiltonian is linear with respect to the control, and, if so, estimates the locations of the discontinuities in the control. the switch times are estimated by determining the roots of the switching functions, where the switching functions are determined using estimates of the state and costate obtained from the collocation method. the accuracy of the switch times is then improved on subsequent meshes by dividing the original optimal control problem into multiple domains and including variables that deﬁne the locations of the switch times. while in principle any collocation method can be used, in this research the previously developed legendre-gauss-radau collocation method is employed because it provides an accurate approximation of the costate which in turn improves the approximation of the switching functions. the method of this paper is designed to be used with a previously developed mesh reﬁnement method in order to accurately approximate the solution in segments where the solution is smooth. the method is demonstrated on three examples where it is shown to accurately determine the switching structure of a bang-bang optimal control problem. when compared with previously developed mesh reﬁnement methods, the results demonstrate that the method developed in this paper improves computational eﬃciency when solving bang-bang optimal control problems. nomenclature = vector ﬁeld for right-hand side of dynamics = matrix deﬁning vector ﬁeld for right-hand side of dynamics at collocation points = vector ﬁeld for boundary conditions = vector ﬁeld for path constraints = matrix deﬁning vector ﬁeld for path constraints at collocation points = legendre-gauss-radau diﬀerentiation matrix = v independent contribution of h = hamiltonian of optimal control problem = objective functional = number of mesh intervals used = lagrange polynomial j of mesh interval k = vector ﬁeld for integrand appearing in lagrange cost = matrix deﬁning vector ﬁeld for integrand at collocation points = mayer cost a a b c c d dn +1 = last column of legendre-gauss-radau diﬀerentiation matrix f h j k (cid:96)(k) l j l m ∗ph.d. student, department of mechanical and aerospace engineering. e-mail: yagamawi@uﬂ.edu. †distinguished professor, department of mathematics. e-mail: hager@uﬂ.edu. ‡associate professor, department of mechanical and aerospace engineering, erich farber faculty fellow and university term professor. e-mail: anilvrao@uﬂ.edu. associate fellow, aiaa. corresponding author. 1 = number of mesh iterations = number of state components = number of control components = number of path constraints = number of switching time parameters = number of collocation points used in mesh interval k = total number of collocation points used = total number of collocation points used for problem on ﬁnal mesh = dth time domain = number of constant control domains used to approximate bang-bang control proﬁle = mesh interval k = initial time = ﬁnal time = sth switch time parameter = estimated discontinuity time for ith control component = midpoint time of two adjacent points containing a change in sign of σi = midpoint time of two adjacent points containing largest absolute diﬀerence m ny nu nc ns nk n nf pd q sk t0 tf t[s] s ti d ti σ ti u t u(t) u v w w y(t) y(τ ) y z ∆ λ(t) λ λn +1 = vector of costate estimates at non-collocated end point λ σ τ τ (k) j = computation time = control in time horizon = matrix of control parameterization at collocation points = linearly dependent components of control in hamiltonian = vector of corresponding lgr weights at collocation points = diagonal matrix of corresponding lgr weights at collocation points = state in time horizon = state approximation in τ domain = matrix of state approximation at discretized points = nonlinearly dependent components of control in hamiltonian = defect constraints matrix = costate in time horizon = matrix of costate estimates at collocation points = matrix of nlp multipliers corresponding to defect constraints at collocation points = vector ﬁeld for switching functions of h = domain used for legendre-gauss-radau collocation = support point j of mesh interval k in value of ith control component "
1905.11897," we performed deep spectropolarimetric observations of a prototypical starburst galaxy m82 with the subaru telescope in order to study the kinematics of the dust outﬂow. we obtained optical polarized emission-line spectra up to ∼4 kpc away from the nucleus of the galaxy along three position angles, 138◦, 150◦ and 179◦ within the conical outﬂowing wind (superwind). the hα emission line in the superwind is strongly polarized and the polarization pattern shows dust scattering of central light sources, being consistent with the previous works. the intensity weighted polarization degree of the hα line reaches ∼30% at maximum. there are at least two light sources at the central region of the galaxy; one of which is located at the near- infrared nucleus and the other resides at one of the peaks of the 3 mm radio and molecular gas emission. the outer (> 1 kpc) dust is illuminated by the former, whereas the inner dust is scattering the light from the latter. we investigated as well the dust motion from the velocity ﬁeld of the polarized hα emission line. the dust is accelerated outward on the northwest side of the nucleus. a simple bi-conical dust outﬂow model shows that the outﬂow velocity of the 300 − 450 km s−1 at ∼ 4 kpc from the nucleus, suggesting that some portion of dust reaches > the dust escapes from the gravitational potential of m82 into the intergalactic space. at some regions on the southeast side, in particular along the position angle of 138◦, the dust has radial velocity slower than the systemic velocity of the galaxy, apparently suggesting inﬂowing motion toward the nucleus. these components are spatially consistent with a part of the molecular gas stream, which is kinematically independent of the outﬂow gas, thus the apparent inﬂow motion of the dust reﬂects the streaming motion associated with the molecular gas stream. ∼ key words: galaxies:individual(m82) — galaxies:nearby — galaxies:starburst — interstellar:dust c(cid:13) 2018. astronomical society of japan. 2 publications of the astronomical society of japan, (2018), vol. 00, no. 0 "
1905.11898," a general-purpose c++ software program called cgpops is described for solving multiple-phase opti- mal control problems using adaptive gaussian quadrature collocation. the software employs a legendre- gauss-radau direct orthogonal collocation method to transcribe the continuous-time optimal control prob- lem into a large sparse nonlinear programming problem. a class of hp mesh reﬁnement methods are implemented which determine the number of mesh intervals and the degree of the approximating poly- nomial within each mesh interval to achieve a speciﬁed accuracy tolerance. the software is interfaced with the open source newton nlp solver ipopt. all derivatives required by the nlp solver are computed using either central ﬁnite differencing, bicomplex-step derivative approximation, hyper-dual derivative approximation, or automatic differentiation. the key components of the software are described in detail and the utility of the software is demonstrated on ﬁve optimal control problems of varying complexity. the software described in this article provides a computationally efﬁcient and accurate approach for solving a wide variety of complex constrained optimal control problems. "
1905.11899," this paper presents the unsteady darcy’s equations coupled with two nonlinear reaction-diﬀusion equations, namely this system describes the mass concentration and heat transfer in porous media. the existence and uniqueness of the solution are established for both the variational formulation problem and for its discrete one obtained using spectral discretization. optimal a priori estimates are given using the brezzi-rappaz-raviart theorem. we conclude by some numerical tests which are in agreement with our theoretical results. keywords: darcy’s equation, nonlinear reaction-diﬀusion equations, spectral discretization . a n h t a m [     1 v 9 9 8 1 1 . 5 0 9 1 : v i x r a "
1905.11900," an e(cid:130)ective content recommendation in modern social media platforms should bene(cid:128)t both creators to bring genuine bene(cid:128)ts to them and consumers to help them get really interesting con- tent. in this paper, we propose a model called social explorative a(cid:138)ention network (sean) for content recommendation. sean uses a personalized content recommendation model to encourage personal interests driven recommendation. moreover, sean allows the personalization factors to a(cid:138)end to users’ higher-order friends on the social network to improve the accuracy and diversity of recommendation results. constructing two datasets from a popular decentralized content distribution platform, steemit, we compare sean with state-of-the-art cf and content based recommendation approaches. experimental results demonstrate the e(cid:130)ectiveness of sean in terms of both gini coe(cid:129)cients for recommendation equality and f1 scores for recommendation performance. ccs concepts •information systems → recommender systems; social recommendation; •mathematics of computing → sequential monte carlo methods; keywords content recommendation, social recommendation, monte carlo tree search, social a(cid:138)ention acm reference format: wenyi xiao[1], huan zhao[1], haojie pan[1], yangqiu song[1], vincent w. zheng[2], qiang yang[2][1]. 2019. beyond personalization: social content recommendation for creator equality and consumer satisfaction . in proceedings of kdd ’19, anchorage, alaska, usa, august 04–08, 2019, 11 pages. doi: 10.1145/1122445.1122456 "
1905.11901," it has been shown that the performance of neu- ral machine translation (nmt) drops starkly in low-resource conditions, underperforming phrase-based statistical machine translation (pbsmt) and requiring large amounts of aux- iliary data to achieve competitive results. in this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. we discuss some pitfalls to be aware of when training low-resource nmt systems, and re- cent techniques that have shown to be espe- cially helpful in low-resource settings, result- ing in a set of best practices for low-resource nmt. in our experiments on german–english with different amounts of iwslt14 training data, we show that, without the use of any aux- iliary monolingual or multilingual data, an op- timized nmt system can outperform pbsmt with far less data than previously claimed. we also apply these techniques to a low-resource korean–english dataset, surpassing previously reported results by 4 bleu. 1 "
1905.11902," 2 using q <(cid:0)n (cid:1) queries, and recovers kwikcluster’s bound of 3opt for q =(cid:0)n we investigate learning algorithms that use similarity queries to approximately solve correlation clustering problems. the input consists of n objects; each pair of objects has a hidden binary similarity score that we can learn through a query. the goal is to use as few queries as possible to partition the objects into clusters so to achieve the optimal number opt of disagreements with the scores. our ﬁrst set of contributions is algorithmic: we introduce acc, a simple query-aware variant of an existing algorithm (kwikcluster, with expected error 3opt but a vacuous o(n2) worst-case bound on the number of queries) for which we prove several desirable properties. first, acc has expected error 3opt + o(n3/q) when second, acc accurately recovers every adversarially perturbed latent cluster c. under stronger conditions on c, acc can even be used to recover exactly all clusters with high probability. third, we show an efﬁcient variant, access, with the same expected error as acc but using signiﬁcantly less queries on some graphs. we empirically test our algorithms on real-world and synthetic datasets. our second set of contributions is a nearly complete information-theoretic characterization of the query vs. error trade-off. first, using vc theory, for all q = ω(n) we prove the existence of algorithms with expected error at most opt + n5/2/√q, and at most (cid:0)n3/q(cid:1) if opt = 0. we then show that any randomized algorithm, when using (cid:101)o at most q queries, must output a clustering with expected cost opt + ω(cid:0)n3/q(cid:1), we prove a weaker lower bound of ω(cid:0)n2/√q(cid:1). which matches the upper bound for q = θ(n). for the special case of opt = 0 (cid:1). 2 1 "
1905.11903," we present an analysis of embeddings extracted from different pre-trained mod- els for content-based image retrieval. speciﬁcally, we study embeddings from image classiﬁcation and object detection models. we discover that even with additional human annotations such as bounding boxes and segmentation masks, the discriminative power of the embeddings based on modern object detection models is signiﬁcantly worse than their classiﬁcation counterparts for the retrieval task. at the same time, our analysis also unearths that object detection model can help retrieval task by acting as a hard attention module for extracting object embed- dings that focus on salient region from the convolutional feature map. in order to efﬁciently extract object embeddings, we introduce a simple guided student- teacher training paradigm for learning discriminative embeddings within the object detection framework. we support our ﬁndings with strong experimental results. 1 "
1905.11904,". we introduce bella, a locally superlinearly convergent bregman forward-back- ward splitting method for minimizing the sum of two nonconvex functions, one of which satisfying a relative smoothness condition [9, 46] and the other one possibly nonsmooth. a key tool of our methodology is the bregman forward-backward envelope (bfbe), an ex- act and continuous penalty function with favorable ﬁrst- and second-order properties, and enjoying a nonlinear error bound when the objective function satisﬁes a łojasiewicz-type property. the proposed algorithm is of linesearch type over the bfbe along candidate up- date directions, and converges subsequentially to stationary points, globally under a kl condition, and owing to the given nonlinear error bound can attain superlinear conver- gence rates even when the limit point is a nonisolated minimum, provided the directions are suitably selected. in this paper, we address the composite minimization problem "
1905.11905,"—the price volatility of cryptocurrencies is often cited as a major hindrance to their wide-scale adoption. conse- quently, during the last two years, multiple so called stablecoins have surfaced—cryptocurrencies focused on maintaining stable exchange rates. in this paper, we systematically explore and analyze the stablecoin landscape. based on a survey of 24 speciﬁc stablecoin projects, we go beyond individual coins for extracting general concepts and approaches. we combine our ﬁndings with learnings from classical monetary policy, resulting in a comprehensive taxonomy of cryptocurrency stabilization. we use our taxonomy to highlight the current state of development from different perspectives and show blank spots. for instance, while over 91% of projects promote 1-to-1 stabilization targets to external assets, monetary policy literature suggests that the smoothing of short term volatility is often a more sustainable alternative. our taxonomy bridges computer science and eco- nomics, fostering the transfer of expertise. for example, we ﬁnd that 38% of the reviewed projects use a combination of exchange rate targeting and speciﬁc stabilization techniques that can render them vulnerable to speculative economic attacks—an avoidable design ﬂaw. "
1905.11906," in the framework of the gauge theory based on the poincar´e symmetry group, the gravitational ﬁeld is described in terms of the coframe and the local lorentz connection. considered as gauge ﬁeld potentials, they give rise to the corresponding ﬁeld strength which are naturally identiﬁed with the torsion and the curvature on the riemann–cartan spacetime. we study the class of quadratic poincar´e gauge gravity models with the most general yang–mills type lagrangian which contains all possible parity-even and parity- odd invariants built from the torsion and the curvature. exact vacuum solutions of the gravitational ﬁeld equations are constructed as a certain deformation of de sitter geometry. they are black holes with nontrivial torsion. 1 "
1905.11907," context. t tauri stars are low-mass young stars whose disks provide the setting for planet formation. despite this, their structure is poorly understood. we present new infrared interferometric observations of the su aurigae circumstellar environment that oﬀer 3 times higher resolution and better baseline position angle coverage over previous observations. aims. we aim to investigate the characteristics of the circumstellar material around su aur, constrain the disk geometry, composition and inner dust rim structure. methods. the chara array oﬀers unique opportunities for long baseline observations, with baselines up to 331 m. using the climb 3-telescope combiner in the k-band allows us to measure visibilities as well as closure phase. we undertook image reconstruction for model-independent analysis, and ﬁtted geometric models such as gaussian and ring distributions. additionally, the ﬁtting of radiative transfer models constrain the physical parameters of the disk. for the ﬁrst time, a dusty disk wind is introduced to the radiative transfer code torus to model protoplanetary disks. our implementation is motivated by theoretical models of dusty disk winds, where magnetic ﬁeld lines drive dust above the disk plane close to the sublimation zone. results. image reconstruction reveals an inclined disk with slight asymmetry along its minor-axis, likely due to inclination eﬀects obscuring the inner disk rim through absorption of incident star light on the near-side and thermal re-emission/scattering of the far- side. geometric modelling of a skewed ring ﬁnds the inner rim at 0.17 ± 0.02 au with an inclination of 50.9 ± 1.0◦ and minor axis position angle 60.8 ± 1.2◦. radiative transfer modelling shows a ﬂared disk with an inner radius at 0.18 au which implies a grain size of 0.4 µm assuming astronomical silicates and a scale height of 15.0 au at 100 au. among the tested radiative transfer models, only the dusty disk wind successfully accounts for the k-band excess by introducing dust above the mid-plane. key words. stars: individual: su aurigae – stars: variables: t tauri, herbig ae/be – techniques: interferometric – protoplanetary disks "
1905.11908,". the aim of this note is to exhibit explicit suﬃcient criteria ensuring bigness of globally generated, rank-r vector bundles, r > 2, on smooth, projective varieties of even dimension d 6 4. we also discuss connections of our general criteria to some recent results of other authors, as well as applications to tangent bundles of fano varieties, to suitable lazarsfeld-mukai bundles on four-folds, etcetera. "
1905.11910," in this paper, we introduce channel-wise recurrent convolutional neural networks (recnets), a family of novel, compact neural network architectures for computer vision tasks inspired by recurrent neural networks (rnns). recnets build upon channel-wise recurrent convolutional (crc) layers, a novel type of convolutional layer that splits the input channels into disjoint segments and processes them in a recurrent fashion. in this way, we simulate wide, yet compact models, since the number of parameters is vastly reduced via the parameter sharing of the rnn formulation. experimental results on the cifar-10 and cifar-100 image classiﬁcation tasks demonstrate the superior size-accuracy trade-off of recnets compared to other compact state-of-the-art architectures. 1 "
1905.11911," the level sets of neural networks represent fundamental properties such as decision boundaries of classiﬁers and are used to model non-linear manifold data such as curves and surfaces. thus, methods for controlling the neural level sets could ﬁnd many applications in machine learning. in this paper we present a simple and scalable approach to directly control level sets of a deep neural network. our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples’ positions to the network parameters. the latter is achieved by a sample network that is constructed by adding a single ﬁxed linear layer to the original network. in turn, the sample network can be used to incorporate the level set samples into a loss function of interest. we have tested our method on three different learning tasks: training networks robust to adversarial attacks, improving generalization to unseen data, and curve and surface reconstruction from point clouds. notably, we increase robust accuracy to the level of standard classiﬁcation accuracy in off-the-shelf networks, improving it by 2% in mnist and 27% in cifar10 compared to state-of-the-art methods. for surface reconstruction, we produce high ﬁdelity surfaces directly from raw 3d point clouds. 1 "
1905.11912," coherence is an important aspect of text qual- ity and is crucial for ensuring its readabil- ity. one important limitation of existing co- herence models is that training on one do- main does not easily generalize to unseen cat- egories of text. previous work (li and ju- rafsky, 2017) advocates for generative models for cross-domain generalization, because for discriminative models, the space of incoher- ent sentence orderings to discriminate against during training is prohibitively large. in this work, we propose a local discriminative neu- ral model with a much smaller negative sam- pling space that can efﬁciently learn against incorrect orderings. the proposed coherence model is simple in structure, yet it signiﬁcantly outperforms previous state-of-art methods on a standard benchmark dataset on the wall street journal corpus, as well as in multiple new challenging settings of transfer to unseen cate- gories of discourse on wikipedia articles. 1 "
1905.11913," we consider the behaviour of the fisher information of scaled sums of independent and identically distributed random variables in the central limit theorem regime. we show how this behaviour can be related to the second-largest non-trivial eigen- value associated with the hirschfeld–gebelein–r´enyi maximal correlation. we prove that assuming this eigenvalue satisﬁes a strict inequality, an o(1/n) rate of conver- gence and a strengthened form of monotonicity hold. 1 "
1905.11914," we study some ﬁnite integral symmetric relation algebras whose forbidden cycles are all 2-cycles. these algebras arise from a ﬁnite ﬁeld construction due to comer. we consider conditions that allow other ﬁnite algebras to embed into these comer algebras, and as an application give the ﬁrst known ﬁnite representation of relation al- gebra 3465, one of whose atoms is ﬂexible. we conclude with some speculation about how the ideas presented here might contribute to a proof of the ﬂexible atom conjecture. 1 "
1905.11915,". in nip theories, generically stable keisler measures can be charac- terized in several ways. we analyze these various forms of “generic stability” in arbitrary theories. among other things, we show that the standard deﬁnition of generic stability for types coincides with the notion of a frequency interpreta- tion measure. we also give combinatorial examples of types in nsop theories that are ﬁnitely approximated but not generically stable, as well as φ-types in simple theories that are deﬁnable and ﬁnitely satisﬁable in a small model, but not ﬁnitely approximated. our proofs demonstrate interesting connections to classical results from ramsey theory for ﬁnite graphs and hypergraphs. "
1905.11916," we present a selection criterion for the euclidean metric adapted during warmup in a hamiltonian monte carlo sampler that makes it possible for a sampler to automatically pick the metric based on the model and the availability of warmup draws. additionally, we present a new adaptation inspired by the selection criterion that requires signiﬁcantly fewer warmup draws to be eﬀective. the eﬀectiveness of the selection criterion and adaptation are demonstrated on a number of applied problems. 1 an implementation for the stan probabilistic programming language is provided. "
1905.11921,". this paper studies a time-changed stochastic control problem, where the underly- ing stochastic process is a l´evy noise time-changed by an inverse subordinator. we establish a maximum principle theory for the time-changed stochastic control problem. we also prove the existence and uniqueness of the corresponding time-changed backward stochastic diﬀer- ential equation involved in the stochastic control problem. some examples are provided for illustration. 9 1 0 2   y a m 8 2       ] . r p h t a m [     1 v 1 2 9 1 1 . 5 0 9 1 : v i x r a "
1905.11922,"—fire disasters typically result in lot of loss to life and property. it is therefore imperative that precise, fast, and possibly portable solutions to detect ﬁre be made readily available to the masses at reasonable prices. there have been several research attempts to design effective and appropriately priced ﬁre detection systems with varying degrees of success. however, most of them demonstrate a trade-off between performance and model size (which decides the model’s ability to be installed on portable devices). the work presented in this paper is an attempt to deal with both the performance and model size issues in one design. toward that end, a ‘designed-from-scratch’ neural network, named firenet, is proposed which is worthy on both the counts: (i) it has better performance than existing counterparts, and (ii) it is lightweight enough to be deploy-able on embedded platforms like raspberry pi. performance evaluations on a standard dataset, as well as our own newly introduced custom-compiled ﬁre dataset, are extremely encouraging. index terms—convolutional neural networks, embedded sys- tems, fire detection, internet of things (iot), neural networks, smoke detection. "
1905.11923," we consider states of the d1-d5 cft where only the left-moving sector is excited. as we deform away from the orbifold point, some of these states will remain bps while others can ‘lift’. the lifting can be computed by a path integral containing two twist deformations; however, the relevant 4-point amplitude cannot be computed explicitly in many cases. we analyze an older proposal by gava and narain where the lift can be computed in terms of a ﬁnite number of 3-point functions. a direct hamiltonian decomposition of the path integral involves an inﬁnite number of 3-point functions, as well the ﬁrst order correction to the starting state. we note that these corrections to the state account for the inﬁnite number of 3-point functions arising from higher energy states, and one can indeed express the path-integral result in terms of a ﬁnite number of 3-point functions involving only the leading order states that are degenerate. the ﬁrst order correction to the supercharge ¯g(1) gets replaced by a projection ¯g(p ); this projected operator can also be used to group the states into multiplets whose members have the same lifting. 1guo.1281@osu.edu 2mathur.16@osu.edu 1 "
1905.11924," automatically matching reviewers to papers is a crucial step of the peer review process for venues receiving thousands of submissions. unfortunately, common paper matching algorithms often construct matchings suffering from two critical problems: (1) the group of reviewers assigned to a paper do not collectively possess sufﬁcient expertise, and (2) reviewer workloads are highly skewed. in this paper, we propose a novel local fairness formulation of paper matching that directly addresses both of these issues. since optimizing our formulation is not always tractable, we introduce two new algorithms, fairir and fairflow, for computing fair matchings that approximately optimize the new formulation. fairir solves a relaxation of the local fairness formulation and then employs a rounding technique to construct a valid matching that provably maximizes the objective and only compromises on fairness with respect to reviewer loads and papers by a small constant. in contrast, fairflow is not provably guaranteed to produce fair matchings, however it can be 2x as efﬁcient as fairir and an order of magnitude faster than matching algorithms that directly optimize for fairness. empirically, we demonstrate that both fairir and fairflow improve fairness over standard matching algorithms on real conference data. moreover, in comparison to state-of-the-art matching algorithms that optimize for fairness only, fairir achieves higher objective scores, fairflow achieves competitive fairness, and both are capable of more evenly allocating reviewers. "
1905.11925," in spite of all the interest and importance of complexity, this concept remains elusive. in particular, several attempts at deﬁning and/or quantifying complexity have, at some point, run into intrinsic diﬃculties. this didactic text provides a brief review of some of the approaches that have been used to characterize complexity, and also suggests a possible deﬁnition of complexity based on the cost assigned to mapping the entity of interest, as well as on the cost of the error implied by its respective reconstruction. ‘mai l’ingegno umano trover`a invenzione pi`u bella n´e pi`u facile n´e pi`u breve della natura, perch´e nelle sue invenzioni nulla manca e nulla ´e superﬂuo.’ leonardo da vinci. 1 "
1905.11926," convolution is a central operation in convolutional neural networks (cnns), which applies a kernel or mask to overlapping regions shifted across the image. in this work we show that the underlying kernels are trained with highly correlated data, which leads to co-adaptation of model weights. to address this issue we propose what we call network deconvolution, a procedure that aims to remove pixel- wise and channel-wise correlations before the data is fed into each layer. we show that by removing this correlation we are able to achieve better convergence rates during model training with superior results without the use of batch normalization on the cifar-10, cifar-100, mnist, fashion-mnist datasets, as well as against reference models from ""model zoo"" on the imagenet standard benchmark. 1 "
1905.11930," learning by demonstration is a versatile and rapid mechanism for transferring motor skills from a teacher to a learner. a particular challenge in imitation learning is the so- called correspondence problem, which involves mapping actions between a teacher and a learner having substantially diﬀerent embodiments (say, human to robot). we present a general, model free and non-parametric imitation learning algorithm based on regression between two hilbert spaces. we accomplish this via kirszbraun’s extension theorem — apparently the ﬁrst application of this technique to supervised learning — and analyze its statistical and computational aspects. we begin by formulating the correspondence problem in terms of quadratically constrained quadratic program (qcqp) regression. then we describe a procedure for smoothing the training data, which amounts to regularizing hypothesis complexity via its lipschitz constant. the lipschitz constant is tuned via a structural risk minimization (srm) procedure, based on the covering-number risk bounds we derive. we apply our technique to a static posture imitation task between two robotic manipulators with diﬀerent embodiments, and report promising results. "
1905.11931," adversarial training is a useful approach to promote the learning of transferable representations across the source and target domains, which has been widely applied for domain adaptation (da) tasks based on deep neural networks. until very recently, existing adversarial domain adaptation (ada) methods ignore the useful information from the label space, which is an important factor accountable for the complicated data distributions associated with different semantic classes. especially, the inter-class semantic relationships have been rarely considered and discussed in the current work of transfer learning. in this paper, we propose a novel relationship-aware adversarial domain adaptation (rada) algorithm, which ﬁrst utilizes a single multi-class domain discriminator to enforce the learning of inter-class dependency structure during domain-adversarial training and then aligns this structure with the inter-class dependencies that are characterized from training the label predictor on the source domain. speciﬁcally, we impose a regularization term to penalize the structure discrepancy between the inter-class dependencies respectively estimated from domain discriminator and label predictor. through this alignment, our proposed method makes the ada aware of class relationships. empirical studies show that the incorporation of class relationships signiﬁcantly improves the performance on benchmark datasets. "
1905.11932,"—distributed antenna selection for distributed mas- sive mimo (multiple input multiple output) communication sys- tems reduces computational complexity compared to centralised approaches, and provides high fault tolerance while retaining diversity and spatial multiplexity. we propose a novel distributed algorithm for antenna selection and show its advantage over existing centralised and distributed solutions. the proposed algorithm is shown to perform well with imperfect channel state information, and to execute a small number of simple computational operations per node, converging fast to a steady state. we base it on reversing petri nets, a variant of petri nets inspired by reversible computation, capable of both forward and backward execution while obeying conservation laws. index terms—distributed massive mimo, antenna selection, optimisation, reversible computation, reversing petri nets. "
1905.11933,"—the advancement of various research sectors such as internet of things (iot), machine learning, data mining, big data, and communication technology has shed some light in transforming an urban city integrating the aforementioned techniques to a commonly known term - smart city. with the emergence of smart city, plethora of data sources have been made available for wide variety of applications. the common technique for handling multiple data sources is data fusion, where it improves data output quality or extracts knowledge from the raw data. in order to cater evergrowing highly com- plicated applications, studies in smart city have to utilize data from various sources and evaluate their performance based on multiple aspects. to this end, we introduce a multi-perspectives classiﬁcation of the data fusion to evaluate the smart city ap- plications. moreover, we applied the proposed multi-perspectives classiﬁcation to evaluate selected applications in each domain of the smart city. we conclude the paper by discussing potential future direction and challenges of data fusion integration. index terms—data fusion; sensor fusion;urban computing; smart city; big data; internet of things; multi-perspectives classiﬁcation "
1905.11934,"—in this paper, the downlink coverage probability and average achievable rate of an aerial user in a vertical hetnet (vhetnet) comprising aerial base stations (aerial-bss) and terrestrial-bss are analyzed. the locations of terrestrial-bss are modeled as an inﬁnite 2-d poisson point process (ppp) while the locations of aerial-bss are modeled as a ﬁnite 3-d binomial point process (bpp) deployed at a particular height. we adopt cellular-to-air (c2a) channel model that incorporates los and nlos transmissions between the terrestrial-bss and the typical aerial user while we assume los transmissions for the air-to- air (a2a) channels separating the aerial user and aerial-bss. for tractability reasons, we simplify the expression of the los probability provided by the international telecommunications union using curve ﬁtting. we assume that the aerial user is associated with the bs (either an aerial-bs or terrestrial-bs) that provides the strongest average received power. using tools from stochastic geometry, we derive analytical expressions of the coverage probability and achievable rate in terms of the laplace transform of interference power. to simplify the derived analytical expressions, we assume that the c2a links are in los conditions. although this approximation gives pessimistic results compared to the exact performance, the analytical approxima- tions are easier to evaluate and quantify well the performance at high heights of the aerial user. our ﬁndings reveal that using directive beamforming for the aerial-bss improves the downlink performance substantially since it alleviates the strong interference signals received from the aerial-bss. index terms—aerial-bs, coverage probability, stochastic ge- ometry, poisson point process , binomial point process. "
1905.11935,"—finite rate of innovation (fri) theory considers sampling and reconstruction of classes of non-bandlimited signals, such as streams of diracs. widely used fri reconstruction methods including prony’s method and matrix pencil method involve singular value decomposition (svd). when samples are corrupted with noise, they achieve an optimal performance given by the cram´er-rao bound yet break down at a certain signal-to-noise ratio (snr) due to the so-called subspace swap problem. in this paper, we investigate a deep neural network approach for fri signal reconstruction that directly learns a transformation from signal samples to fri parameters. simulations show signiﬁcant improvement on the breakdown snr over existing fri methods. subspace estimation is not involved, our proposed method has the potential to achieve robust reconstruction of fri signals with a lower breakdown snr. the dnn consists of 3 convolutional (conv) layers followed by 3 fully connected (fc) layers as shown in fig. 2. each of the conv layers has ﬁlters of size 3. rectiﬁed linear unit (relu) is used as the activation function between each two layers. mean-squared error is adopted as training loss function. backpropagation with adam optimizer [9] is used for learning. "
1905.11937," performing exact bayesian inference for complex models is intractable. markov chain monte carlo (mcmc) algorithms can provide reliable approximations of the posterior distribution but are computationally expensive for large datasets. a standard approach to mitigate this complexity consists of using subsampling techniques or distributing the data across a cluster. however, these approaches are typically unreliable in high-dimensional scenarios. we focus here on an alternative class of mcmc schemes exploiting a splitting strategy akin to the one used by the celebrated admm optimization algorithm. these methods, proposed recently in [43, 51], appear to provide empirically state-of-the-art performance. we generalize here these ideas and propose a detailed theoretical study of one of these algorithms known as the split gibbs sampler. under regularity conditions, we establish explicit dimension-free convergence rates for this scheme using ricci curvature and coupling ideas. we demonstrate experimentally the excellent performance of these mcmc schemes on various applications. 1 "
1905.11940," to generalize to novel visual scenes with new viewpoints and new object poses, a visual system needs representations of the shapes of the parts of an object that are invariant to changes in viewpoint or pose. 3d graphics representations disentangle visual factors such as viewpoints and lighting from object structure in a natural way. it is possible to learn to invert the process that converts 3d graphics representations into 2d images, provided the 3d graphics representations are available as labels. when only the unlabeled images are available, how- ever, learning to derender is much harder. we consider a simple model which is just a set of free ﬂoating parts. each part has its own relation to the cam- era and its own triangular mesh which can be deformed to model the shape of the part. at test time, a neural network looks at a single image and extracts the shapes of the parts and their relations to the camera. each part can be viewed as one head of a multi-headed derenderer. during training, the extracted parts are used as input to a differentiable 3d renderer and the reconstruction error is backpropagated to train the neural net. we make the learning task easier by en- couraging the deformations of the part meshes to be invari- ant to changes in viewpoint and invariant to the changes in the relative positions of the parts that occur when the pose of an articulated body changes. cerberus, our multi-headed derenderer, outperforms previous methods for extracting 3d parts from single im- ages without part annotations, and it does quite well at ex- tracting natural parts of human ﬁgures. "
1905.11944," we perform analysis of the three-dimensional kinematics of milky way disk stars in mono-age populations. we focus on stars between galactocentric distances of r = 6 and 14 kpc, selected from the combined lamost dr4 red clump giant stars and gaia dr2 proper motion catalogue. we conﬁrm the 3d asymmetrical motions of recent works, and we provide time tagging of the galactic outer disk asymmetrical motions near the anticenter direction out to galactocentric distances of 14 kpc. radial galactocentric motions reach values up to 10 km s−1, depending on the age of the population, and present a north-south asymmetry in the region corresponding to density and velocity substructures that were sensitive to the perturbations in the early 6 gyr. after that time, the disk stars of this structure are becoming older and kinematically hotter and not sensitive to the possible perturbations, and we ﬁnd it is a low α, metal rich, relatively younger population. with the quantitative analysis, we ﬁnd stars both above and below the plane at r (cid:38) 9 kpc exhibit bending mode motions of which the sensitive duration is around 8 gyr. some possible scenarios for these asymmetries are discussed, including a fast rotating bar, spiral arms, minor mergers, sub-halos, warp dynamics, and streams. although we cannot rule out other factors, for the current results, we speculate that the in-plane asymmetries might be mainly caused by gravitational attraction of overdensities in a spiral arm or monolithic collapse of isolated self-gravitating overdensities from out-of-equilibrium systems. vertical motions might be dominated by bending and breathing modes induced by inner or external perturbers. subject headings: galaxy: kinematics and dynamics − galaxy: disk − galaxy: structure "
1905.11945,". in recent years, histopathology images have been increas- ingly used as a diagnostic tool in the medical ﬁeld. the process of ac- curately diagnosing a biopsy sample requires signiﬁcant expertise in the ﬁeld, and as such can be time-consuming and is prone to uncertainty and error. with the advent of digital pathology, using image recognition sys- tems to highlight problem areas or locate similar images can aid pathol- ogists in making quick and accurate diagnoses. in this paper, we speciﬁ- cally consider the encoded local projections (elp) algorithm, which has previously shown some success as a tool for classiﬁcation and recognition of histopathology images. we build on the success of the elp algorithm as a means for image classiﬁcation and recognition by proposing a mod- iﬁed algorithm which captures the local frequency information of the image. the proposed algorithm estimates local frequencies by quanti- fying the changes in multiple projections in local windows of greyscale images. by doing so we remove the need to store the full projections, thus signiﬁcantly reducing the histogram size, and decreasing computa- tion time for image retrieval and classiﬁcation tasks. furthermore, we investigate the eﬀectiveness of applying our method to histopathology images which have been digitally separated into their hematoxylin and eosin stain components. the proposed algorithm is tested on the publicly available invasive ductal carcinoma (idc) data set. the histograms are used to train an svm to classify the data. the experiments showed that the proposed method outperforms the original elp algorithm in image retrieval tasks. on classiﬁcation tasks, the results are found to be com- parable to state-of-the-art deep learning methods and better than many handcrafted features from the literature. keywords: digital histopathology · encoded local projections (elp) · radon transform · digital stain separation · digital image retrieval and classiﬁcation. ⋆ this research has been supported in part by a natural sciences and engineering research council of canada (nserc) doctoral scholarship (akc). 2 1 a.k. cheeseman et al. "
1905.11946," convolutional neural networks (convnets) are commonly developed at a ﬁxed resource budget, and then scaled up for better accuracy if more resources are available. in this paper, we sys- tematically study model scaling and identify that carefully balancing network depth, width, and res- olution can lead to better performance. based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefﬁcient. we demonstrate the effectiveness of this method on scaling up mobilenets and resnet. to go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called efﬁcientnets, which achieve much better accu- racy and efﬁciency than previous convnets. in particular, our efﬁcientnet-b7 achieves state- of-the-art 84.4% top-1 / 97.1% top-5 accuracy on imagenet, while being 8.4x smaller and 6.1x faster on inference than the best existing convnet. our efﬁcientnets also transfer well and achieve state-of-the-art accuracy on cifar-100 (91.7%), flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. source code is at https: //github.com/tensorflow/tpu/tree/ master/models/official/efficientnet. 9 1 0 2   y a m 8 2       ] g l . s c [     1 v 6 4 9 1 1 . 5 0 9 1 : v i x r a "
1905.11947," in this work we present novel diﬀerentially private identity (goodness-of-ﬁt) testers for natural and widely studied classes of multivariate product distributions: gaussians in rd with known covariance and product distributions over {±1}d. our testers have improved sample complexity compared to those derived from previous techniques, and are the ﬁrst testers whose sample complexity matches the order-optimal minimax sample complexity of o(d1/2/α2) in many parameter regimes. we construct two types of testers, exhibiting tradeoﬀs between sample complexity and computational complexity. finally, we provide a two-way reduction between testing a subclass of multivariate product distributions and testing univariate distributions, and thereby obtain upper and lower bounds for testing this subclass of product distributions. 1 "
1905.11948,"—we enhance dependency-based data cleaning with approximate band conditional order dependencies (abcods) as a novel type of integrity constraint (ic). band ods express the se- mantics over attributes that are monotonically related with small variations without there being an intrinsic violation of semantics. to make band ods relevant to real-world applications, we make them less strict to hold approximately with some exceptions and conditionally on subsets of the data with a mix of ascending and descending directions. formulating ics manually requires domain expertise, is prone to human errors, and time consuming. thus, we study the problem of automatic abcod discovery. the naive solution is prohibitively expensive as it considers all possible segmentations of tuples resulting in exponential data complexity. to reduce the search space, we propose an algorithm that utilize the notion of a longest monotonic band (lmb) to identify longest subsequences of tuples that satisfy a band od. we formulate the abcod discovery problem as a constraint optimization problem, and devise a dynamic programming algorithm that determines the optimal solution in polynomial time (super-cubic complexity). to further optimize the performance over the large datasets, we adapt the algorithm to consider pieces (contiguous sequences of tuples) in a greedy fashion. this improves the performance by orders-of-magnitude without sacriﬁcing the precision. when bidirectionally is removed to consider unidirectional abcods, with all ascending or all descending ordering, we show that our pieces-based algorithm is guaranteed to ﬁnd the optimal solution. we provide an experimental evaluation of our techniques over real-world datasets. "
1905.11949,". in this paper, we study some objects from combinatorial number theory and relate them to the study of the rational dyck paths. let g and h be ﬁnite abelian groups with gcd(|g|, |h|) = 1. for any positive integer m, let m(g, m) be the set of all zero-sum sequences over g of length m. firstly, we provide bijections between m(g, |h|), m(h, |g|), and d|g|,|h|, where d|g|,|h| is the set of all (|g|, |h|)-dyck paths. consequently we have #m(g, |h|) = #m(h, |g|) = cat|g|,|h| := 1 |g| + |h|(cid:16)|g| + |h| (cid:17). |g| secondly, we study the following counting problem. assume that |g| = n. let a1, . . . , ak be given positive integers with k ≤ n. we consider the number of non-equivalent solutions of the following equation in g: a1x1 + a2x2 + · · · + akxk = b, where b ∈ g. if gcd(a1 + · · · + ak, n) = 1, then we show that the number of all non-equivalent solutions coincides with the kreweras number which initially counts the number of some special rational dyck paths. we provide both combinatorial and algebraic proofs of these results. for any positive integers n, m with gcd(n, m) = 1, the rational catalan number "
1905.11950,". in this paper, we are concerned about the qualitative behaviour of planar filippov systems around some typical minimal sets, namely, polycycles. in the smooth context, a polycycle is a simple closed curve composed by a collection of singularities and regular orbits, inducing a ﬁrst return map. here, this concept is extended to filippov systems by allowing typical singularities lying on the switching manifold. our main goal consists in developing a method to investigate the unfolding of polycycles in filippov systems. in addition, we applied this method to describe bifurcation diagrams of filippov systems around certain polycycles. "
1905.11951,". smooth tropical cubic surfaces are parametrized by maximal cones in the uni- modular secondary fan of the triple tetrahedron. there are 344 843 867 such cones, organized into a database of 14 373 645 symmetry classes. the schläﬂi fan gives a further reﬁnement of these cones. it reveals all possible patterns of the 27 or more lines on tropical cubic surfaces, thus serving as a combinatorial base space for the universal fano variety. this article devel- ops the relevant theory and oﬀers a blueprint for the analysis of big data in tropical algebraic geometry. we conclude with a sparse model for cubic surfaces over a ﬁeld with valuation. "
1905.11952," we analyze the kq-based motivic adams spectral sequence over the complex numbers, where kq is the very eﬀective cover of hermitian k-theory deﬁned over c by isaksen-shkembi and over general base ﬁelds by ananyevskiy-r¨ondigs-østvær. we calculate the ring of cooperations of kq modulo v1-torsion, completely calculate the 0- and 1-lines of the kq-resolutions, completely determine the v1-periodic com- plex motivic stable stems, and recover andrews and miller’s computation of the η-periodic complex motivic stable stems. as an application, we propose a motivic telescope conjecture and outline a program for proving the conjecture in two cases using our calculations. we also propose a model for the complex motivic stable orthogonal j-homomorphism and conjecture its location in the kq-resolution. contents "
1905.11953, we report a shot noise limited high speed stimulated raman microscopy platform allowing to acquire molecular vibrational spectra over 200 cm−1 in 12 µs at a scan rate of 40khz. using spectral focusing to- gether with optimized acousto-optics programmable dispersive ﬁlters the designed low noise imaging plat- form performs chemical imaging of dynamical processes such as mannitol crystal hydratation and reaches a signal to noise ratio sufﬁcient to perform label free histological imaging on frozen human colon tissue slides. 
1905.11954," because of the rich dynamical structure of videos and their ubiquity in everyday life, it is a natural idea that video data could serve as a powerful unsupervised learn- ing signal for training visual representations in deep neural networks. however, instantiating this idea, especially at large scale, has remained a signiﬁcant artiﬁcial intelligence challenge. here we present the video instance embedding (vie) framework, which extends powerful recent unsupervised loss functions for learning deep nonlinear embeddings to multi-stream temporal processing architectures on large-scale video datasets. we show that vie-trained networks substantially ad- vance the state of the art in unsupervised learning from video datastreams, both for action recognition in the kinetics dataset, and object recognition in the imagenet dataset. we show that a hybrid model with both static and dynamic processing pathways is optimal for both transfer tasks, and provide analyses indicating how the pathways differ. taken in context, our results suggest that deep neural embeddings are a promising approach to unsupervised visual learning across a wide variety of domains. 1 "
1905.11955,"                        .    in this paper it is shown that in the case of the given probability distribution or  histogram using the jaynes entropy maximum principle an j- measure of uncertainty  (jmu) provides the maximum for a given distribution can be constructed. formulas  for the introduced jmu were obtained explicitly and calculations of this new measure  for a number of distributions are shown as examples. it is shown using as the  example a two-dimensional random variable, the application of the proposed method  to the jmu estimation for the multidimensional case. it was made a comparison of  the information contained in the histogram of a random variable with the information  in the probability distribution obtained as a fitting of this histogram. moreover  studied  the influence of an additional measurement of a certain physical quantity on the  amount of information.      "
1905.11956,". we consider anzellotti-type almost minimizers for the thin obsta- cle (or signorini) problem with zero thin obstacle and establish their c1,β regularity on the either side of the thin manifold, the optimal growth away from the free boundary, the c1,γ regularity of the regular part of the free boundary, as well as a structural theorem for the singular set. the analysis of the free boundary is based on a successful adaptation of energy methods such as a one-parameter family of weiss-type monotonicity formulas, almgren- type frequency formula, and the epiperimetric and logarithmic epiperimetric inequalities for the solutions of the thin obstacle problem. contents "
1905.11957," (cid:16)eη|ξ[gη(x, ξ)] (cid:17) in this paper, we study a class of stochastic optimization problems, referred to as the conditional stochastic optimization (cso), in the form of minx∈x eξfξ . cso ﬁnds a wide spec- trum of applications including portfolio selection, reinforcement learning, robust and invariant learning. we establish the sample complexity of the sample average approximation (saa) for cso, under a vari- ety of structural assumptions, such as lipschitz continuity, smoothness, and error bound conditions. we show that the total sample complexity improves from o(d/4) to o(d/3) when assuming smoothness of the outer function, and further to o(1/2) when the empirical function satisﬁes the quadratic growth condition. we also establish the sample complexity of a modiﬁed saa, when ξ and η are independent. our numerical results from several experiments further support our theoretical ﬁndings. keywords: stochastic optimization, sample average approximation, large deviations theory "
1905.11958,". petri nets are a formalism for modelling and reasoning about the behaviour of distributed systems. recently, a reversible approach to petri nets, reversing petri nets (rpn), has been proposed, allowing transitions to be reversed spontaneously in or out of causal order. in this work we propose an approach for controlling the reversal of actions of an rpn, by associating transitions with conditions whose satisfaction/viola- tion allows the execution of transitions in the forward/reversed direction, respectively. we illustrate the framework with a model of a novel, dis- tributed algorithm for antenna selection in distributed antenna arrays. 1 "
1905.11959," music genre classiﬁcation is the problem of associating genre-related labels to digitized music tracks. it has applications in the organization of commercial and personal music collections. often, music tracks are described as a set of timbre-inspired sound textures. in shallow-learning systems, the total number of sound textures per track is usually too high, and texture downsampling is necessary to make training tractable. although previous work has solved this by linear downsampling, no extensive work has been done to evaluate how texture selection beneﬁts genre classiﬁcation in the context of the bag of frames track descriptions. in this paper, we evaluate the impact of frame selection on automatic music genre classiﬁcation in a bag of frames scenario. we also present a novel texture selector based on k-means aimed to identify diverse sound textures within each track. we evaluated texture selection in diverse datasets, four different feature sets, as well as its relationship to a univariate feature selection strategy. the results show that frame selection leads to signiﬁcant improvement over the single vector baseline on datasets consisting of full-length tracks, regardless of the feature set. results also indicate that the k-means texture selector achieves signiﬁcant improvements over the baseline, using fewer textures per track than the commonly used linear downsampling. the results also suggest that texture selection is complementary to the feature selection strategy evaluated. our qualitative analysis indicates that texture variety within classes beneﬁts model generalization. our analysis shows that selecting speciﬁc audio excerpts can improve classiﬁcation performance, and it can be done automatically. keywords music genre classiﬁcation · sound texture selection · music classiﬁcation · signal processing · music information retrieval 1 "
1905.11960,". the expansion history of the universe between the end of inﬂation and the onset of radiation-domination (rd) is currently unknown. if the equation of state during this period is stiﬀer than that of radiation, w > 1/3, the gravitational wave (gw) background (w+1/3) > 0 at frequencies f (cid:29) frd corre- from inﬂation acquires a blue-tilt d log ρgw sponding to modes re-entering the horizon during the stiﬀ-domination (sd), where frd is the frequency today of the horizon scale at the sd-to-rd transition. we characterized in detail the transfer function of the gw energy density spectrum, considering both ’instant’ and smooth modelings of the sd-to-rd transition. the shape of the spectrum is controlled by w, frd, and hinf (the hubble scale of inﬂation). we determined the parameter space compatible with a detection of this signal by ligo and lisa, including possible changes in the number of relativistic degrees of freedom, and the presence of a tensor tilt. consistency with upper bounds on stochastic gw backgrounds, however, rules out a signiﬁcant fraction of the observable parameter space. we ﬁnd that this renders the signal unobservable by advanced ligo, in all cases. the gw background remains detectable by lisa, though only in a small island of parameter space, corresponding to scenarios with an equation of state in the range 0.46 (cid:46) w (cid:46) 0.56 and a high inﬂationary scale hinf (cid:38) 1013 gev, but low reheating temperature 1 mev (cid:46) trd (cid:46) 150 mev (equivalently, 10−11 hz (cid:46) frd (cid:46) 3.6 · 10−9 hz). implications for early universe scenarios resting upon an sd epoch are brieﬂy discussed. 9 1 0 2   y a m 8 2       ] . o c h p - o r t s a [     1 v 0 6 9 1 1 . 5 0 9 1 : v i x r a contents "
1905.11961,". in this paper we introduce a notion of almost minimizers for cer- tain variational problems governed by the fractional laplacian, with the help of the caﬀarelli-silvestre extension. in particular, we study almost fractional harmonic functions and almost minimizers for the fractional obstacle problem with zero obstacle. we show that for a certain range of parameters, almost minimizers are almost lipschitz or c 1,β -regular. "
1905.11962," we consider the problem of counting the population size in the population model. in this model, we are given a distributed system of n identical agents which interact in pairs with the goal to solve a common task. in each time step, the two interacting agents are selected uniformly at random. in this paper, we consider so-called uniform protocols, where the actions of two agents upon an interaction may not depend on the population size n. we present two population protocols to count the size of the population: protocol approximate, which computes with high probability either blog nc or dlog ne, and protocol countexact, which computes the exact population size in optimal o(n log n) interactions, using ˜o(n) states. both protocols can also be converted to stable protocols that give a correct result with probability 1 by using an additional multiplicative factor of o(log n) states. "
1905.11963," a well-known problem in data science and machine learning is lin- ear regression, which is recently extended to dynamic graphs. existing exact algorithms for updating the solution of dynamic graph regres- sion problem require at least a linear time (in terms of n: the size of the graph). however, this time complexity might be intractable in practice. in the current paper, we utilize subsampled randomized hadamard transform and countsketch to propose the ﬁrst randomized algorithms. suppose that we are given an n × m matrix embedding m of the graph, where m ≪ n. let r be the number of samples required for a guaranteed approximation error, which is a sublinear function of n. our ﬁrst algorithm reduces time complexity of pre-processing to o(n(m + 1) + 2n(m + 1) log2(r + 1) + rm2). then after an edge in- sertion or an edge deletion, it updates the approximate solution in o(rm) time. our second algorithm reduces time complexity of pre- number of nonzero elements of m . then after an edge insertion or an edge deletion or a node insertion or a node deletion, it updates processing to o(cid:0)nnz(m ) + m3ǫ−2 log7(m/ǫ)(cid:1), where nnz(m ) is the ǫ2 log6(m/ǫ)(cid:17). the approximate solution in o(qm) time, with q = o(cid:16) m2 finally, we show that under some assumptions, if ln n < ǫ−1 our ﬁrst 1 algorithm outperforms our second algorithm and if ln n ≥ ǫ−1 our second algorithm outperforms our ﬁrst algorithm. keywords dynamic networks, subsampled randomized hadamard trans- form, countsketch, dynamic graph regression, approximate algorithm, repre- sentation learning, sublinear update time. 1 "
1905.11964,". in this article we prove a reducibility result for the linear schr¨odinger equation on the sphere sn with quasi-periodic in time perturbation. our result includes the case of unbounded perturbation that we assume to be of order strictly less than 1/2 and satisfying some parity condition. as far as we know, this is one of the few reducibility results for an equation in more than one dimension with unbounded perturbations. we notice that our result does not requires the use of the pseudo-differential calculus. contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1. "
1905.11965,". we develop the details of a surgery theory for contact manifolds of arbi- trary dimension via convex structures, extending the 3-dimensional theory developed by giroux. the theory is analogous to that of weinstein manifolds in symplectic geometry, with the key diﬀerence that the vector ﬁeld does not necessarily have pos- itive divergence everywhere. the surgery theory for contact manifolds contains the surgery theory for weinstein manifolds via a sutured model for attaching critical points of low index. using this sutured model, we show that the existence of con- vex structures on closed contact manifolds is guaranteed, a result equivalent to the existence of supporting weinstein open book decompositions. "
1905.11966,". we present a kleene realizability semantics for the intensional level of the min- imalist foundation, for short mtt, extended with inductively generated formal topologies, church’s thesis and axiom of choice. this semantics is an extension of the one used to show consistency of the intensional level of the minimalist foundation with the axiom of choice and formal church’s thesis in previous work. a main novelty here is that such a semantics is formalized in a constructive theory represented by aczel’s constructive set theory czf extended with the regular extension axiom. "
1905.11968," in the chasing convex bodies problem, an online player receives a request sequence of n convex sets k1, . . . , kn contained in a normed space rd. the player starts at x0 ∈ rd, and after observing each kn picks a new point xn ∈ kn. at each step the player pays a movement cost of ||xn − xn−1||. the player aims to maintain a constant competitive ratio against the minimum cost possible in hindsight, i.e. knowing all requests in advance. the existence of a ﬁnite competitive ratio for convex body chasing was ﬁrst conjectured in 1991 by friedman and linial in [fl93]. this conjecture was recently resolved in [blls18] which proved an exponential 2o(d) upper bound on the competitive ratio. in this paper, we drastically improve the exponential upper bound. we give an algorithm achieving competitive ratio d for arbitrary normed spaces, which is exactly tight for ℓ∞. in euclidean space, our algorithm achieves nearly optimal competitive ratio o(√d log n ), com- pared to a lower bound of √d. our approach extends the recent work [bkl+18] which chases nested convex bodies using the classical steiner point of a convex body. we deﬁne the functional steiner point of a convex function and apply it to the work function to obtain our algorithm. 1 "
1905.11969," in this work we analyse the constraints imposed by poincar´e symmetry on the gravitational form factors appearing in the lorentz decomposition of the energy-momentum tensor matrix elements for massive states with arbitrary spin. by adopting a distributional approach, we prove for the ﬁrst time non-perturbatively that the zero momentum transfer limit of the leading two form factors in the decomposition are completely independent of the spin of the states. it turns out that these constraints arise due to the general poincar´e transformation and on-shell properties of the states, as opposed to the speciﬁc characteristics of the individual poincar´e generators themselves. by expressing these leading form factors in terms of generalised parton distributions, we subsequently derive the linear and angular momentum sum rules for states with arbitrary spin. 9 1 0 2   y a m 8 2       ] h t - p e h [     1 v 9 6 9 1 1 . 5 0 9 1 : v i x r a ∗sabrina.cotogno@polytechnique.edu †cedric.lorce@polytechnique.edu ‡peter.lowdon@polytechnique.edu 1 1 "
1905.11970," the formalism of combined ﬁnite modular and generalised cp (gcp) symmetries for theories of ﬂavour is developed. the corresponding consistency conditions for the two symmetry transformations acting on the modulus τ and on the matter ﬁelds are derived. the implications of gcp symmetry in theories of ﬂavour based on modular invariance described by ﬁnite modular groups are illustrated with the example of a modular s4 model of lepton ﬂavour. due to the addition of the gcp symmetry, viable modular models turn out to be more constrained, with the modulus τ being the only source of cp violation. 9 1 0 2   y a m 8 2       ] h p - p e h [     1 v 0 7 9 1 1 . 5 0 9 1 : v i x r a 1e-mail: pavel.novichkov@sissa.it 2e-mail: joao.t.n.penedo@tecnico.ulisboa.pt 3also at institute of nuclear research and nuclear energy, bulgarian academy of sciences, 1784 soﬁa, bulgaria. 4e-mail: arsenii.titov@durham.ac.uk 1 "
1905.11971," deep neural networks are vulnerable to adver- sarial attacks. the literature is rich with algo- rithms that can easily craft successful adversarial examples. in contrast, the performance of de- fense techniques still lags behind. this paper pro- poses me-net, a defense method that leverages matrix estimation (me). in me-net, images are preprocessed using two steps: ﬁrst pixels are ran- domly dropped from the image; then, the image is reconstructed using me. we show that this pro- cess destroys the adversarial structure of the noise, while re-enforcing the global structure in the orig- inal image. since humans typically rely on such global structures in classifying images, the pro- cess makes the network mode compatible with human perception. we conduct comprehensive experiments on prevailing benchmarks such as mnist, cifar-10, svhn, and tiny-imagenet. comparing me-net with state-of-the-art defense mechanisms shows that me-net consistently out- performs prior techniques, improving robustness against both black-box and white-box attacks. "
1905.11972," machine learning theory has mostly focused on generalization to samples from the same distribution as the training data. whereas a better understanding of generalization beyond the training distribution where the observed distribution changes is also fundamentally important to achieve a more powerful form of generalization. in this paper, we attempt to study through the lens of information measures how a particular architecture behaves when the true probability law of the samples is potentially different at training and testing times. our main result is that the testing gap between the empirical cross-entropy and its statistical expectation (measured with respect to the testing probability law) can be bounded with high probability by the mutual information between the input testing samples and the corresponding representations, generated by the encoder obtained at training time. these results of theoretical nature are supported by numerical simulations showing that the mentioned mutual information is representative of the testing gap, capturing qualitatively the dynamic in terms of the hyperparameters of the network. 1 "
1905.11973," in the past decade, research on test-suite-based automatic program repair has grown significantly. each year, new approaches and im- plementations are featured in major software engineering venues. however, most of those approaches are evaluated on a single bench- mark of bugs, which are also rarely reproduced by other researchers. in this paper, we present a large-scale experiment using 11 java test-suite-based repair tools and 5 benchmarks of bugs. our goal is to have a better understanding of the current state of automatic program repair tools on a large diversity of benchmarks. our inves- tigation is guided by the hypothesis that the repairability of repair tools might not be generalized across different benchmarks of bugs. we found that the 11 tools 1) are able to generate patches for 21% of the bugs from the 5 benchmarks, and 2) have better performance on defects4j compared to other benchmarks, by generating patches for 47% of the bugs from defects4j compared to 10-30% of bugs from the other benchmarks. our experiment comprises 23,551 repair attempts in total, which we used to find the causes of non-patch generation. these causes are reported in this paper, which can help repair tool designers to improve their techniques and tools. acm reference format: thomas durieux, fernanda madeiral, matias martinez, and rui abreu. 2019. empirical review of java program repair tools: a large-scale experiment on 2,141 bugs and 23,551 repair attempts. in proceedings of the 27th acm joint european software engineering conference and symposium on the foun- dations of software engineering (esec/fse 2019). acm, new york, ny, usa, 12 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn "
1905.11975," existing controllable text generation systems rely on annotated attributes, which greatly limits their capabilities and applications. in this work, we make the ﬁrst successful attempt to use vaes to achieve controllable text generation without supervision. we do so by decomposing the latent space of the vae into two parts: one incorporates structural constraints to capture dominant global variations implicitly present in the data, e.g., sentiment or topic; the other is unstructured and is used for the reconstruction of the source sentences. with the enforced structural constraint, the underlying global variations will be discovered and disentangled during the training of the vae. the structural constraint also provides a natural recipe for mitigating posterior collapse for the structured part, which cannot be fully resolved by the existing techniques. on the task of text style transfer, our unsupervised approach achieves signiﬁcantly better performance than previous supervised approaches. by showcasing generation with ﬁner-grained control including cards-against-humanity-style1 topic transitions within a sentence, we demonstrate that our model can perform controlled text generation in a more ﬂexible way than existing methods. 1 "
1905.11976," we bring together aspects of covariant hamiltonian ﬁeld theory and of classical integrable ﬁeld theories in 1 + 1 dimensions. speciﬁcally, our main result is to obtain for the ﬁrst time the classical r-matrix structure within a covariant poisson bracket for the lax connection, or lax one form. this exhibits a certain covariant nature of the classical r-matrix with respect to the underlying spacetime variables. the main result is established by means of several prototypical examples of integrable ﬁeld theories, all equipped with a zakharov-shabat type lax pair. full details are presented for: a) the sine-gordon model which provides a relativistic example associated to a classical r-matrix of trigonometric type; b) the nonlinear schr¨odinger equation and the (complex) modiﬁed korteweg-de vries equation which provide two non-relativistic examples associated to the same classical r-matrix of rational type, characteristic of the akns hierarchy. the appearance of the r-matrix in a covariant poisson bracket is a signature of the integrability of the ﬁeld theory in a way that puts the independent variables on equal footing. this is in sharp contrast with the single-time hamiltonian evolution context usually associated to the r-matrix formalism. 1 "
1905.11978," in this work, we develop a novel regularizer to improve the learning of long-range dependency of sequence data. applied on language modelling, our regularizer expresses the inductive bias that sequence variables should have high mutual infor- mation even though the model might not see abundant observations for complex long-range dependency. we show how the “next sentence prediction (classiﬁca- tion)"" heuristic can be derived in a principled way from our mutual information estimation framework, and be further extended to maximize the mutual information of sequence variables. the proposed approach not only is effective at increasing the mutual information of segments under the learned model but more importantly, leads to a higher likelihood on holdout data, and improved generation quality. 1 "
1905.11979," behavioral cloning reduces policy learning to supervised learning by training a discriminative model to predict expert actions given observations. such discrim- inative models are non-causal: the training procedure is unaware of the causal structure of the interaction between the expert and the environment. we point out that ignoring causality is particularly damaging because of the distributional shift in imitation learning. in particular, it leads to a counter-intuitive “causal confusion” phenomenon: access to more information can yield worse performance. we inves- tigate how this problem arises, and propose a solution to combat it through targeted interventions—either environment interaction or expert queries—to determine the correct causal model. we show that causal confusion occurs in several benchmark control domains as well as realistic driving settings, and validate our solution against dagger and other baselines and ablations. 1 "
1905.11980," we obtain sharp estimate on p-spectral gaps, or equivalently optimal con- stant in p-poincar´e inequalities, for metric measure spaces satisfying measure contraction property. we also prove the rigidity for the sharp p-spectral gap. keywords: p-poincar´e inequality, p-spectral gap, p-obata theorem, curvature- dimension condition, measure contraction property, metric measure space. contents "
1905.11981,". we show that any automatic multiplicative sequence either coin- cides with a dirichlet character or is identically zero when restricted to in- tegers not divisible by small primes. this answers a question of bell, bruin and coons. a similar result was obtained independently by klurman and kurlberg.     ] t n h t a m . [     1 v 1 8 9 1 1 . 5 0 9 1 : v i x r a "
1905.11982,": we consider the distributed optimization problem where a group of agents seeks to cooperatively compute the optimizer of the average of local functions over a time-varying directed communication network. to solve this problem, we propose a novel algorithm which adjusts the ratio between the number of communications and computations to achieve fast convergence. in particular, the iterates of our algorithm converge to the optimizer at the same rate as those of centralized gradient descent in terms of the number of computations. we compare our algorithm with other known algorithms on a distributed target localization problem. keywords: distributed optimization, nonconvex, time-varying, ﬁxed-point iteration, gossip, decentralized, gradient method "
1905.11983,": we revisit the calculation of holographic correlators in ads3. we develop new methods to evaluate exchange witten diagrams, resolving some technical diﬃculties that prevent a straightforward application of the methods used in higher dimensions. we perform detailed calculations in the ads3 × s3 × k3 background. we ﬁnd strong evidence that four-point tree-level correlators of kk modes of the tensor multiplets enjoy a hidden 6d conformal symmetry. the correlators can all be packaged into a single generating function, related to the 6d ﬂat space superamplitude. this generalizes an analogous structure found in ads5 × s5 supergravity. 9 1 0 2   y a m 8 2       ] h t - p e h [     1 v 3 8 9 1 1 . 5 0 9 1 : v i x r a contents "
1905.11984," consider the following problem faced by an online voting platform: a user is provided with a list of alternatives, and is asked to rank them in order of preference using only drag-and-drop operations. the platform’s goal is to recommend an initial ranking that minimizes the time spent by the user in arriving at her desired ranking. we develop the (cid:27)rst optimization framework to address this problem, and make theoretical as well as practical contributions. on the practical side, our experiments on amazon mechanical turk provide two interesting insights about user behavior: first, that users’ ranking strategies closely resemble selection or insertion sort, and second, that the time taken for a drag-and-drop operation depends linearly on the number of positions moved. these insights directly motivate our theoretical model of the optimization problem. we show that computing an optimal recommendation is np-hard, and provide exact and approximation algorithms for a variety of special cases of the problem. experimental evaluation on mturk shows that, compared to a random recommendation strategy, the proposed approach reduces the (average) time-to-rank by up to 50%. "
1905.11985,": concerns about gender bias have captured most of the attention in the ai research  literature on the topic of bias in word embeddings models. in this work, a systematic analysis of  popular word embedding models shows that many of those concerns are probably exaggerated.  gender bias in these models is often mild and frequently reversed in polarity to what has been  regularly reported. interestingly, other types of so far unreported moderate biases in word  embedding models have been identified. specifically, biases against intellectual phenomena such  as political orientation and religiosity. this mismatch in the literature could be due to another  type of bias, the bias of an orthodox epistemic community with widely shared community blind  spots that perhaps is mostly bent on exploring only zeitgeist-conforming regions of the research  landscape.  main text:   the term algorithmic bias is often used to describe systematic offsets in algorithmic  output that produce unfair outcomes such as privileging or discriminating an arbitrary group of  people over others. the topic of algorithmic bias has recently elicited widespread attention  among the machine learning research community. popular machine learning artifacts have been  used to illustrate the creeping of societal bias and prejudices into models such as computer vision  (1), recidivism prediction (2) and language modeling (3).   word embedding models are dense vector representations of words learned from a corpus  of natural language (4).  word embeddings have revolutionized natural language processing due  to their ability to model semantic similarity and relatedness among pairs of words as well as  linear regularities between words that roughly capture meaningful language constructs such as  gender or social class (5). the usage of word embeddings in upstream natural language  processing tasks has often improved the accuracy of those systems downstream (6).   word embedding models have been claimed to capture prejudicial bias, stemming from  the corpus on which they were trained, against women and racial minorities (7). accordingly, it  has been shown that popular word embedding models tend to associate word vector  representations of african americans given names with negative terms (8), female given names  with words such as nursing and homemaker and male given names with high status professions  such as computer programmer and doctor (3).  this work has examined the existing literature on the creeping of societal biases into  word embedding models through a systematic search of the sources arxiv, dblp computer  science bibliography, google scholar and semantic scholar for the queries: word embeddings  bias and word vectors bias. a total of 29 papers were identified where the  clearly  indicates that the paper focuses on the topic of bias in word embeddings (table s1). examination  "
1905.11986," electron-positron pair production in dynamically assisted two-color electric ﬁelds is investigated for vari- ous polarizations using the real-time dirac-heisenberg-wigner formalism. momentum spectrum and num- ber density of created pair for diﬀerent polarizations are examined and obtained, in particular the eﬀects of high-harmonics of two ﬁelds and the polarized degree on them are exhibited. it is found that for only single strong ﬁeld, the interference eﬀects vanish with high-degree elliptical polarization, however, for the dynamical assisted two-color ﬁelds, the interference eﬀects of momentum spectrum are more remarkable with multiple peaks in elliptic or/and circular polarization. an important ﬁnding is the strong nonlinear dependence of the number density on the high-harmonics of two-color ﬁelds, which can be enhanced sig- niﬁcantly over 7 − 8 orders when appropriate high-harmonics is present. the polarization eﬀect on number density is gradually weaken as the high-harmonics increases while a weak nonlinearity relation appears again if the high-harmonics exceeds a single-photon threshold. pacs numbers: 12.20.ds, 03.65.pm, 02.60.-x 9 1 0 2   y a m 8 2       ] h p - t n a u q [     1 v 6 8 9 1 1 . 5 0 9 1 : v i x r a ∗ bsxie@bnu.edu.cn 1 i. "
1905.11987,": baseline generation for tracking applications is a difﬁcult task when work- ing with real world radar data. data sparsity usually only allows an indirect way of estimating the original tracks as most objects’ centers are not represented in the data. this article proposes an automated way of acquiring reference trajectories by using a highly accurate hand-held global navigation satellite system (gnss). an embedded in- ertial measurement unit (imu) is used for estimating orientation and motion behavior. this article contains two major contributions. a method for associating radar data to vulnerable road user (vru) tracks is described. it is evaluated how accurate the sys- tem performs under different gnss reception conditions and how carrying a reference system alters radar measurements. second, the system is used to track pedestrians and cyclists over many measurement cycles in order to generate object centered occupancy grid maps. the reference system allows to much more precisely generate real world radar data distributions of vrus than compared to conventional methods. hereby, an important step towards radar-based vru tracking is accomplished. "
1905.11988," this study shows the development and performance assessment of a novel set-up that enables the research of structural materials for fusion reactors, by making possible simultaneous application of temperature (up to 450◦c) and magnetic ﬁeld (close to 0.6 t) during irradiation experiments. these aspects become crit- ical as structural materials in fusion reactors are exposed to intense radiation levels under the presence of strong magnetic ﬁelds. moreover, material micro- structural could be modiﬁed by radiation-induce propagating defects, which are thought to be sensitive to magnetic ﬁelds. the device has three main compo- nents: magnetic closure, sample holder with integrated heater, and radiation shield. it is provided with a thermal shield to prevent other elements of the device to heat up and fail. a mapping of the magnetic ﬂux in the region where sample and heater are located has been modeled by ﬁnite elements simulation software and correlated with magnetic measurements. keywords: pacs numbers:, 28.52.s, 28.52.av, 39.10.+j, 75.50.bb, 07.55.w email address: joseluis.fcunnado@imdea.es (j. l. f. cu˜nado) preprint submitted to fusion engineering and design may 30, 2019 "
1905.11989,. considered here is the derivation of partial diﬀerential equations arising in pulsatile ﬂow in pipes with viscoelastic walls. the equations are asymptotic models describing the propagation of long-crested pulses in pipes with cylindrical symmetry. additional eﬀects due to viscous stresses in bio- ﬂuids are also taken into account. the eﬀects of viscoelasticity of the vessels on the propagation of solitary and periodic waves in a vessel of constant radius are being explored numerically. 
1905.11990,"  since crossing the heliopause on august 25, 2012, voyager 1 observed reductions in galactic  cosmic ray count rates caused by a time-varying depletion of particles with pitch angles near 90°,  while intensities of particles with other pitch angles remain unchanged. between late 2012 and  mid-2017, three large-scale events occurred, lasting from ~100 to ~630 days. omnidirectional  and directional high-energy data from voyager 1’s cosmic ray subsystem are used to report  cosmic ray intensity variations. omnidirectional (≳ 20 mev) proton-dominated measurements  show up to a 3.8% intensity reduction. bi-directional (≳ 70 mev) proton-dominated  measurements taken from various spacecraft orientations provide insight about the depletion  region’s spatial properties. we characterize the anisotropy as a “notch” in an otherwise uniform  pitch-angle distribution of varying depth and width centered about 90° in pitch angle space. the  notch averages 22° wide and 15% deep – signifying a depletion region that is broad and shallow.  there are indications that the anisotropy is formed by a combination of magnetic trapping and  cooling downstream of solar-induced transient disturbances in a region that is also likely  influenced by the highly compressed fields near the heliopause.  "
1905.11991," astrophysical explosions are accompanied by the propagation of a shock wave through an ambient medium. depending on the mass and energy involved in the explosion, the shock velocity v can be non-relativistic (v ≪ c, where c is the speed of light), ultra-relativistic (v ≃ c), or moderately relativistic (v ∼ f ew × 0.1c). while self-similar, energy-conserving solutions to the ﬂuid equations that describe the shock propagation are known in the non-relativistic (the sedov-taylor blastwave) and ultra-relativistic (the blandford-mckee blastwave) regimes, the ﬁnite speed of light violates scale invariance and self-similarity when the ﬂow is only mildly relativistic. by treating relativistic terms as perturbations to the ﬂuid equations, here we derive the o(v 2/c2), energy-conserving corrections to the non-relativistic, sedov-taylor solution for the propagation of a strong shock. we show that relativistic terms modify the post-shock ﬂuid velocity, density, pressure, and the shock speed itself, the latter being constrained by global energy conservation. we derive these corrections for a range of post-shock adiabatic indices γ (which we set as a ﬁxed number for the post-shock gas) and ambient power-law indices n, where the density of the ambient medium ρa into which the shock advances declines with spherical radius r as ρa ∝ r−n. for sedov-taylor blastwaves that terminate in a contact discontinuity with diverging density, we ﬁnd that there is no relativistic correction to the sedov-taylor solution that simultaneously satisﬁes the ﬂuid equations and conserves energy. these solutions have implications for relativistic supernovae, the transition from ultra- to sub-relativistic velocities in gamma-ray bursts, and other high-energy phenomena. keywords: gamma-ray burst: general — hydrodynamics — methods: analytical — relativistic pro- cesses — shock waves — supernovae: general "
1905.11993,": we compute dispersion relations of non-hydrodynamic and hydrodynamic modes in a non-relativistic strongly coupled two-dimensional quantum ﬁeld theory. this is achieved by numerically computing quasinormal modes (qnms) of a particular analyt- ically known black brane solution to 3+1-dimensional hoˇrava gravity. hoˇrava gravity is distinguished from einstein gravity by the presence of a scalar ﬁeld, termed the khronon, deﬁning a preferred time-foliation. surprisingly, for this black brane solution, the khronon ﬂuctuation numerically decouples from all others, having its own set of purely imaginary eigenfrequencies, for which we provide an analytic expression. all other hoˇrava gravity qnms are expressed analytically in terms of qnms of einstein gravity, in units involv- ing the khronon coupling constants and various horizons. our numerical computation reproduces the analytically known momentum diﬀusion mode, and extends the analytic expression for the sound modes to a wide range of khronon coupling values. in the eikonal limit (large momentum limit), the analytically known dispersion of qnm frequencies with the momentum is reproduced by our numerics. we provide a parametrization of all qnm frequencies to fourth order in the momentum. we demonstrate perturbative stability in a wide range of coupling constants and momenta. 9 1 0 2   y a m 8 2       ] h t - p e h [     1 v 3 9 9 1 1 . 5 0 9 1 : v i x r a contents "
1905.11994,": the fundamental couplings of the standard model are known to vary as a function of energy scale through the renormalisation group (rg), and have been measured at the electroweak scale at colliders. however, the variation of the couplings as a function of temperature need not be the same, raising the possibility that couplings in the early universe were not at the values predicted by rg evolution. we study how such temperature- variance of fundamental couplings can aid the production of a baryon asymmetry in the universe through electroweak baryogenesis. we do so in the context of the standard model augmented by higher-dimensional operators up to dimension 6. 9 1 0 2   y a m 8 2       ] h p - p e h [     1 v 4 9 9 1 1 . 5 0 9 1 : v i x r a contents "
1905.11995," identifying active galactic nuclei (agns) through their x-ray emission is eﬃcient, but necessarily biased against x-ray-faint objects. we aim to characterize this bias by comparing x-ray-selected agns to the ones identiﬁed through optical variabil- ity and mid-ir colours. we present a catalogue of agns selected through optical variability using all publicly available z-band hubble space telescope images in the goods-south ﬁeld. for all objects in the catalogue, we compute x-ray upper limits or discuss detections in the deepest available ∼7 ms chandra deep field south images and present the spitzer/irac mid-ir colours. for the variability study, we consider only sources observed over at least ﬁve epochs and over a time baseline of up to ten years. we adopt the elevated median absolute deviation as a variability indicator robust against individual outlier measurements and identify 113 variability-selected agn candidates. among these, 26 have an x-ray counterpart and lie within the con- ventional agn area in the fx /fopt diagram. the candidates with x-ray upper limits are on average optically fainter, have higher redshifts compared to the x-ray detected ones and are consistent with low luminosity agns. out of 41 variable optical sources with ir detections, 13 fulﬁll the ir agn colour selection criteria. our work empha- sizes the importance of optical variability surveys for constructing complete samples of agns including the ones that remain undetected even by the deepest x-ray and ir surveys. key words: galaxies: active – galaxies: photometry – galaxies: nuclei – x-rays: galax- ies – methods: observational "
1905.11996," we report the serendipitous detection of two 3 mm continuum sources found in deep alma band 3 observations to study intermediate redshift galaxies in the cosmos ﬁeld. one is near a foreground galaxy at 1.(cid:48)(cid:48)3, but is a previously unknown dust-obscured star-forming galaxy (dsfg) at probable zco = 3.329, illustrating the risk of misidentifying shorter wavelength counterparts. the optical-to- mm spectral energy distribution (sed) favors a grey λ−0.4 attenuation curve and results in signiﬁcantly larger stellar mass and sfr compared to a calzetti starburst law, suggesting caution when relating progenitors and descendants based on these quantities. the other source is missing from all previous optical/near-infrared/sub-mm/radio catalogs (“alma-only”), and remains undetected even in stacked ultradeep optical (> 29.6 ab) and near-infrared (> 27.9 ab) images. using the alma position as a prior reveals faint sn r ∼ 3 measurements in stacked irac 3.6+4.5, ultradeep scuba2 850µm, and vla 3ghz, indicating the source is real. the sed is robustly reproduced by a massive m∗ = 1010.8m(cid:12) and mgas = 1011m(cid:12), highly obscured av ∼ 4, star forming sf r ∼ 300 m(cid:12)yr−1 galaxy at redshift z = 5.5±1.1. the ultrasmall 8 arcmin2 survey area implies a large yet uncertain contribution to the cosmic star formation rate density csfrd(z=5) ∼ 0.9 × 10−2 m(cid:12) yr−1 mpc−3, comparable to all ultraviolet-selected galaxies combined. these results indicate the existence of a prominent population of dsfgs at z > 4, below the typical detection limit of bright galaxies found in single-dish sub-mm surveys, but with larger space densities ∼ 3× 10−5 mpc−3, higher duty cycles 50− 100%, contributing more to the csfrd, and potentially dominating the high-mass galaxy stellar mass function. "
1905.11998," the progenitors of the high-mass black-hole mergers observed by ligo and virgo are potential lisa sources and promising candidates for multiband gw observations. in this letter, we consider the minimum signal-to-noise ratio these sources must have to be detected by lisa. our revised threshold of ρthr∼ 15 is higher than previous estimates, which signiﬁcantly reduces the expected number of events. we also point out the importance of the detector performance at high-frequencies and the duration of the lisa mission, which both inﬂuence the event rate substantially. key words: gravitational waves – black holes – data analysis "
1905.11999,": the usual methods for formulating and solving the quantum mechanics of a particle moving in a magnetic ﬁeld respect neither locality nor any global symmetries which happen to be present. for example, landau’s solution for a particle moving in a uniform magnetic ﬁeld in the plane involves choosing a gauge in which neither translation nor rotation invariance are manifest. we show that locality can be made manifest by passing to a redundant description in which the particle moves on a u (1)-principal bundle over the original conﬁguration space and that symmetry can be made manifest by passing to a corresponding central extension of the original symmetry group by u (1). with the symmetry manifest, one can attempt to solve the problem by using harmonic analysis and we provide a number of examples where this succeeds. one is a solution of the landau problem in an arbitrary gauge (with either translation invariance or the full euclidean group manifest). another example is the motion of a fermionic rigid body, which can be formulated and solved in a manifestly local and symmetric way via a ﬂat connection on the non-trivial u (1)-central extension of the conﬁguration space so(3) given by u (2). 9 1 0 2   y a m 8 2       ] h t - p e h [     1 v 9 9 9 1 1 . 5 0 9 1 : v i x r a contents "
1905.12000," context. the cosmic microwave background (cmb) yields an inference on the matter sound horizon, within the standard model. independent, direct measurements of the sound horizon are then a probe of possible deviations from the standard model. aims. we aim at measuring the sound horizon rs from low-redshift indicators, completely independent from cmb inference. methods. we use the measured product h(z)rs from baryon acoustic oscillations (bao), plus supernovae ia to constrain h(z)/h0 and time-delay lenses analysed by the h0licow collaboration to anchor cosmological distances (∝ h−1 0 ). additionally, we investigate the inﬂuence of adding a sample of higher-redshift quasars with standardisable uv-xray luminosity distances. we adopt polynomial expansions in h(z) or in comoving distances, so that our inference is completely independent of any underlying cosmological model. our measurements are independent of cepheids and systematics from peculiar motions, to whithin percent-level accuracy. results. the inferred sound horizon rs varies between (133 ± 8) mpc and (138 ± 5) mpc across diﬀerent models. the discrepancy with cmb measurements is robust against model choice. statistical uncertainties are comparable to systematics. conclusions. the combination of time-delay lenses, supernovae and bao yields a cosmology-independent (and cepheid-calibration- independent) distance ladder, and a cmb-independent measurement of rs. these cosmographic measurements are then a competitive test of the standard model, regardless of hypotheses on the underlying cosmology. key words. gravitational lensing: strong – cosmological parameters – distance scale – early universe "
1905.12001," with a spin frequency of 707 hz, psr j0952−0607 is the second fastest spinning pulsar known. it was discovered in radio by lofar in 2017 at an estimated distance of either 0.97 or 1.74 kpc and has a low-mass companion with a 6.42 hr orbital period. we report discovery of the x-ray counterpart of psr j0952−0607 using xmm-newton. the x-ray spectra can be well-ﬁt by a single power law model (γ ≈ 2.5) or by a thermal plus power law model (kteﬀ ≈ 40 ev and γ ≈ 1.4). we do not detect evidence of variability, such as that due to orbital modulation. because of its fast spin rate, psr j0952−0607 is a crucial source for understanding the r-mode instability, which can be an eﬀective mechanism for producing gravitational waves. using the high end of our measured surface temperature, we infer a neutron star core temperature of ∼ 107 k, which places psr j0952−0607 within the window for the r-mode to be unstable unless an eﬀect such as superﬂuid mutual friction damps the ﬂuid oscillation. the measured luminosity limits the dimensionless r-mode amplitude to be less than ∼ 1 × 10−9. keywords: gravitational waves — pulsars: general — pulsars: individual (psr j0952−0607) — stars: neutron — x-rays: stars "
1905.12002," we develop a stochastic geometry framework to characterize the meta distributions of the downlink signal-to- interference-ratio (sir)/signal-to-noise-ratio (snr) and data rate of a typical outdoor user in a coexisting sub-6ghz and millimeter wave (mm-wave) cellular network. macro base-stations (mbss) transmit on sub-6ghz channels (which we term “microwave” channels), whereas small base-stations (sbss) communicate with users on mm-wave channels. the sbss are connected to mbss via a microwave (µwave) wireless backhaul. the µwave channels are interference limited and mm-wave channels are noise limited; therefore, we have the meta-distribution of sir and snr in µwave and mm-wave channels, respectively. to model the line-of-sight (los) nature of mm-wave channels, we use nakagami-m fading model. we ﬁrst characterize the association probabilities of a typical user to a µwave mbs, a los mm-wave sbs and a non-los (nlos) mm-wave sbs. then, we characterize the conditional success probability (csp) (or equivalently reliability) and its bth moment for a typical user (a) when it associates to a µwave mbs for direct transmission and (b) when it associates to a mm-wave sbs for dual-hop transmission (backhaul and access transmission). we then characterize the exact as well as approximate expressions for the meta distributions. performance metrics such as the mean and variance of the local delay (network jitter), mean of the csp (coverage probability), and variance of the csp are studied. closed-form expressions are presented for special scenarios. the extensions of the developed framework to the µwave-only network or networks where sbss have mm-wave backhauls are discussed. numerical results validate the analytical results. insights are extracted related to the reliability, coverage probability, and latency of the considered network. index terms millimeter wave, meta distribution, reliability, wireless backhaul, nakagami fading, 5g, stochastic geometry. the authors are with the department of electrical engineering and computer science, york university, toronto, ontario, m3j 1p3 canada (e-mail:hibrahim,hina,utn@cse.yorku.ca). part of the manuscript will be presented at the 2019 ieee icc: sac internet of things track. "
1905.12003,"—in this paper, the concept of representation learning based on deep neural networks is applied as an alternative to the use of handcrafted features in a method for automatic visual inspection of corroded thermoelectric metallic pipes. a texture convolutional neural network (tcnn) replaces hand- crafted features based on local phase quantization (lpq) and haralick descriptors (hd) with the advantage of learning an appropriate textural representation and the decision boundaries into a single optimization process. experimental results have shown that it is possible to reach the accuracy of 99.20% in the task of identifying different levels of corrosion in the internal surface of thermoelectric pipe walls, while using a compact network that requires much less effort in tuning parameters when compared to the handcrafted approach since the tcnn architecture is compact regarding the number of layers and connections. the observed results open up the possibility of using deep neural networks in real-time applications such as the automatic inspection of thermoelectric metal pipes. index terms—automatic inspection, convolution neural net- works, deep learning, visual inspection, texture; "
1905.12005,"—biopsies are the gold standard for breast cancer diagnosis. this task can be improved by the use of computer aided diagnosis (cad) systems, reducing the time of diagnosis and reducing the inter and intra-observer variability. the advances in computing have brought this type of system closer to reality. however, datasets of histopathological images (hi) from biopsies are quite small and unbalanced what makes difﬁcult to use modern machine learning techniques such as deep learning. in this paper we propose a compact architecture based on texture ﬁlters that has fewer parameters than tradi- tional deep models but is able to capture the difference between malignant and benign tissues with relative accuracy. the experimental results on the breakhis dataset have show that the proposed texture cnn achieves almost 90% of accuracy for classifying benign and malignant tissues. keywords-deep learning, texture, histopathological images, breast cancer. "
1905.12006," we present a framework for autonomously learning a portable representation that describes a collection of low-level continuous environments. we show that these  representations can be learned in a task-independent egocentric space speciﬁc to the agent that, when grounded with problem-speciﬁc information, are provably sufﬁcient for planning. we demonstrate transfer in two different domains, where an agent learns a portable, task-independent symbolic vocabulary, as well as rules expressed in that vocabulary, and then learns to instantiate those rules on a per- task basis. this reduces the number of samples required to learn a representation of a new task. 1 "
1905.12008,". in this working notes paper, we describe ibm research ai (almaden) team’s participation in the imageclef 2019 vqa-med com- petition. the challenge consists of four question-answering tasks based on radiology images. the diversity of imaging modalities, organs and disease types combined with a small imbalanced training set made this a highly complex problem. to overcome these diﬃculties, we implemented a modular pipeline architecture that utilized transfer learning and multi- task learning. our ﬁndings led to the development of a novel model called supporting facts network (sfn). the main idea behind sfn is to cross-utilize information from upstream tasks to improve the accu- racy on harder downstream ones. this approach signiﬁcantly improved the scores achieved in the validation set (18 point improvement in f- 1 score). finally, we submitted four runs to the competition and were ranked seventh. keywords: imageclef 2019 · vqa-med · visual question answering · supporting facts network · multi-task learning · transfer learning 1 "
1905.12009," we consider a new form of model-based reinforcement learning methods that directly learns the optimal control parameters, instead of learning the underlying dynamical system. this includes a form of exploration and exploitation in learning and applying the optimal control parameters over time. this also includes a general framework that manages a collection of such control-model-based reinforcement learning methods running in parallel and that selects the best decision from among these parallel methods with the different methods interactively learning together. we derive theoretical results for the optimal control of linear and nonlinear instances of the new control-model-based reinforcement learning methods. our empirical results demonstrate and quantify the signiﬁcant beneﬁts of our approach. 1 "
1905.12010,. classical parking functions can be deﬁned in terms of drivers with preferred parking spaces searching a linear parking lot for an open parking spot. we may consider this linear parking lot as a collection of n vertices (parking spots) arranged in a directed path. we generalize this notion to allow for more com- plicated “parking lots” and deﬁne parking functions on arbitrary directed graphs. we then consider a relationship proved by lackner and panholzer between parking functions on trees and “mapping digraphs” and we show that a similar relationship holds when edge orientations are reversed. 
1905.12011," we use mars atmosphere and volatile evolution observations of the upstream solar wind, and mars express observations of ionospheric electron densities and mag- netic ﬁelds, to study how the topside ionosphere (> 320 km) of mars is aﬀected by variations in solar wind dynamic pressure. we ﬁnd that high solar wind dynamic pressures result in the topside ionosphere being depleted of plasma at all solar zenith angles, coincident with increased induced magnetic ﬁeld strengths. the depletion of topside plasma in response to high solar wind dynamic pressures is observed in both weak and strong crustal magnetic ﬁeld regions. taken together, our results suggest that high solar wind dynamic pressures lead to ionospheric compression, increased ion escape, and reduced plasma production in the high-altitude nightside ionosphere. "
1905.12012," we systematically study all supersymmetric solutions of six-dimensional (2, 0) supergrav- ity with a null isometry. in particular, every such solution with at least four real supersym- metries is also a supersymmetric solution of a (1, 0) theory preserving the same absolute amount of supersymmetry. this implies that no genuinely new solutions of this type can be found in this framework. the microstate geometries associated to supersymmetric black holes within mathur’s proposal are generically supersymmetric solutions of six-dimensional supergravity. a direct consequence of our result is that supersymmetric microstate geome- tries of single centre supersymmetric black holes should carry only one compact 3-cycle. 1e-mail: guillaume.bossard@polytechnique.edu 2e-mail: severin.luest@polytechnique.edu contents "
1905.12013," investing eﬃciently in future research to improve policy decisions is an important goal. expected value of sample information (evsi) can be used to select the speciﬁc design and sample size of a proposed study by assessing the beneﬁt of a range of diﬀerent studies. estimating evsi with the standard nested monte carlo algorithm has a notoriously high computational burden, especially when using a complex decision model or when optimizing over study sample sizes and designs. therefore, a number of more eﬃcient evsi approximation methods have been developed. however, these approximation methods have not been compared and therefore their relative advantages and disadvantages are not clear. a consortium of evsi researchers, including the developers of several approximation methods, compared four evsi methods using three previously published health economic models. the ex- amples were chosen to represent a range of real-world contexts, including situations with multiple study outcomes, missing data, and data from an observational rather than a randomized study. the computational speed and accuracy of each method were compared, and the relative advantages and implementation challenges of the methods were highlighted. in each example, the approximation methods took minutes or hours to achieve reasonably accurate evsi estimates, whereas the traditional monte carlo method took weeks. speciﬁc methods are particularly suited to problems where we wish to compare multiple proposed sample sizes, when the proposed sample size is large, or when the health economic model is computationally expensive. all the evaluated methods gave estimates similar to those given by traditional monte carlo, suggesting that evsi can now be eﬃciently computed with conﬁdence in realistic examples. "
1905.12014,": we investigate the “pole-skipping” phenomenon in holographic chaos. ac- cording to the pole-skipping, the energy-density green’s function is not unique at a special point in complex momentum plane. this arises because the bulk ﬁeld equation has two regular near-horizon solutions at the special point. we study the regularity of two solutions more carefully using curvature invariants. in the upper-half ω-plane, one solution, which is normally interpreted as the outgoing mode, is in general singular at the future horizon and produces a curvature singularity. however, at the special point, both solutions are indeed regular. moreover, the incoming mode cannot be uniquely deﬁned at the special point due to these solutions. keywords: holography and condensed matter physics (ads/cmt), ads-cft corre- spondence, black holes arxiv eprint: yymm.nnnn 1also at department of particle and nuclear physics, sokendai (the graduate university for ad- vanced studies), 1-1 oho, tsukuba, ibaraki, 305-0801, japan; department of physics engineering, mie university, tsu, 514-8507, japan. contents "
1905.12015,": we investigate a new property of retarded green’s functions using ads/cft. the green’s functions are not unique at special points in complex momentum space. this arises because there is no unique incoming mode at the horizon and is similar to the “pole- skipping” phenomenon in holographic chaos. our examples include the bulk scalar ﬁeld, the bulk maxwell vector and scalar modes, and the shear mode of gravitational perturbations. in these examples, the special points are always located at ω⋆ = −i(2πt ) with appropriate values of complex wave number. keywords: holography and condensed matter physics (ads/cmt), ads-cft corre- spondence, black holes arxiv eprint: yymm.nnnn 9 1 0 2   y a m 8 2       ] h t - p e h [     1 v 5 1 0 2 1 . 5 0 9 1 : v i x r a 1also at department of particle and nuclear physics, sokendai (the graduate university for ad- vanced studies), 1-1 oho, tsukuba, ibaraki, 305-0801, japan; department of physics engineering, mie university, tsu, 514-8507, japan. contents "
1905.12016," coalescing neutron star binary (nsb) systems are primary candidates for r-process enrichment of galaxies. the recent detection of r-process elements in ultra-faint dwarf (ufd) galaxies and the abundances measured in classical dwarfs challenges the nsb merger scenario both in terms of coalescence time scales and merger locations. in this paper, we focus on the dynamics of nsbs in the gravitational potentials of diﬀerent types of host galaxies and on its impact on the subsequent galactic enrichment. we ﬁnd that, even when receiving a low kick (∼ 10 km s−1) from the second supernova explosion, in shallow dwarf galaxy potentials nsbs tend to merge with a large oﬀ-set from the host galaxy. this results in a signiﬁcant geometrical dilution of the amount of produced r−process elements that fall back and pollute the host galaxy gas reservoir. the combination of dilution and small number statistics produces a large scatter in the expected r-process enrichment within a single ufd or classical dwarf galaxy. comparison between our results and observed europium abundances reveals a tension that even a systematic choice of optimistic parameters in our models cannot release. such a discrepancy could point to the need of additional r-process production sites that suﬀer less severe dilution. key words: galaxies: dwarf; stars: neutron; galaxies: abundances; methods: numer- ical "
1905.12018,". if g has 4-periodic cohomology and at most two one-dimensional quaternionic representations, then g has the d2 property if and only if g has a balanced presentation. we use this to solve wall’s d2 problem for several inﬁnite families of non-abelian groups and, in these cases, also show that any poincar´e 3-complex x with π1(x) = g admits a cellular structure with a single 3-cell. the proof involves cancellation theorems for zg modules where g has periodic cohomology. "
1905.12019," we introduce a uniﬁed probabilistic approach for deep continual learning based on variational bayesian inference with open set recognition. our model combines a probabilistic encoder with a generative model and a generative linear classiﬁer that get shared across tasks. the open set recognition bounds the approximate posterior by ﬁtting regions of high density on the basis of correctly classiﬁed data points and balances open-space risk with recognition errors. catastrophic inference for both generative models is signiﬁcantly alleviated through generative replay, where the open set recognition is used to sample from high density areas of the class speciﬁc posterior and reject statistical outliers. our approach naturally allows for forward and backward transfer while maintaining past knowledge without the necessity of storing old data, regularization or inferring task labels. we demonstrate compelling results in the challenging scenario of incrementally expanding the single-head classiﬁer for both class incremental visual and audio classiﬁcation tasks, as well as incremental learning of datasets across modalities. 1 "
1905.12020," when pre-processing observational data via matching, we seek to approximate each unit with maximally similar peers that had an alternative treatment status– essentially replicating a randomized block design. however, as one considers a growing number of continuous features, a curse of dimensionality applies making asymptotically valid inference impossible (abadie and imbens, 2006). the alter- native of ignoring plausibly relevant features is certainly no better, and the result- ing trade-off substantially limits the application of matching methods to “wide” datasets. instead, li and fu (2017) recasts the problem of matching in a metric learning framework that maps features to a low-dimensional space that facilitates “closer matches” while still capturing important aspects of unit-level heterogeneity. however, that method lacks key theoretical guarantees and can produce inconsis- tent estimates in cases of heterogeneous treatment effects. motivated by straight- forward extension of existing results in the matching literature, we present alterna- tive techniques that learn latent matching features through either mlps or through siamese neural networks trained on a carefully selected loss function. we bench- mark the resulting alternative methods in simulations as well as against two ex- perimental data sets–including the canonical nsw worker training program data set–and ﬁnd superior performance of the neural-net-based methods. "
1905.12022," in federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited. we develop a bayesian nonparametric framework for federated learning with neural networks. each data server is assumed to provide local neural network weights, which are modeled through our framework. we then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision, data pooling and with as few as a single communication round. we then demonstrate the efﬁcacy of our approach on federated learning problems simulated from two popular image classiﬁcation datasets.1 "
1905.12023,". a practical number is a positive integer n such that every positive integer less than n can be written as a sum of distinct divisors of n. we prove that most of the binomial coeﬃcients are practical numbers. precisely, letting not practical numbers, we show that f (n) denote the number of binomial coeﬃcients (cid:0)n k(cid:1), with 0 ≤ k ≤ n, that are for all integers n ∈ [3, x], but at most oγ(x1−(δ−γ)/ log log x) exceptions, for all x ≥ 3 and 0 < γ < δ < log 2. furthermore, we prove that the central binomial coeﬃcient(cid:0)2n n(cid:1) is a practical number for all positive integers n ≤ x but at most o(x0.88097) exceptions. we also pose some questions on this topic. f (n) < n1−(log 2−δ)/ log log n "
1905.12025," we prove that multiplicative preprojective algebras, deﬁned by crawley-boevey and shaw, are 2-calabi–yau algebras for quivers containing an unoriented cycle. we also prove that the dg versions of these algebras (arising in fukaya categories of certain we- instein 4-manifolds) are formal with homology the multiplicative preprojective algebra concentrated in degree zero. we prove that the center is trivial in the case that the cy- cle is properly contained, and hence the calabi–yau structure is unique. we conjecture that the same properties hold for all non-dynkin, non-extended dynkin quivers, and explain how to reduce the conjecture to extended-dynkin quivers (note that the cycle is the type a case). as an application, corresponding multiplicative quiver varieties are normal and are locally isomorphic to ordinary quiver varieties. this includes char- acter varieties of riemann surfaces with punctures and monodromy conditions; quivers containing cycles cover all positive genus cases. contents "
1905.12027,". denote by µβ = ” exp(βx)” the gaussian multiplica- tive chaos which is deﬁned using a log-correlated gaussian ﬁeld x on a domain u ⊂ rd. the case β ∈ r has been studied quite intensively, and then µβ is a random measure on u . it is known that µβ can also be deﬁned for complex values β lying in certain subdomain of c, and then the realizations of µβ are random gen- eralized functions on u . in this note we complement the results of [9] (where the case of purely imaginary β was considered) by studying the besov-regularity of µβ and the ﬁniteness of moments for general complex values of β. 9 1 0 2   y a m 8 2       ] . r p h t a m [     1 v 7 2 0 2 1 . 5 0 9 1 : v i x r a "
1905.12028,". image alignment across domains has recently become one of the re- alistic and popular topics in the research community. in this problem, a deep learning-based image alignment method is usually trained on an available large- scale database. during the testing steps, this trained model is deployed on unseen images collected under different camera conditions and modalities. the delivered deep network models are unable to be updated, adapted or ﬁne-tuned in these scenarios. thus, recent deep learning techniques, e.g. domain adaptation, fea- ture transferring, and ﬁne-tuning, are unable to be deployed. this paper presents a novel deep learning based approach5 to tackle the problem of across unseen modalities. the proposed network is then applied to image alignment as an illus- tration. the proposed approach is designed as an end-to-end deep convolutional neural network to optimize the deep models to improve the performance. the pro- posed network has been evaluated in digit recognition when the model is trained on mnist and then tested on unseen mnist-m dataset. finally, the proposed method is benchmarked in image alignment problem when training on rgb im- ages and testing on depth and x-ray images. 1 "
1905.12030," silicon photonics enables scaling of quantum photonic systems by allowing the creation of ex- tensive, low-loss, reconﬁgurable networks linking various functional on-chip elements. inclusion of single quantum emitters onto photonic circuits, acting as on-demand sources of indistinguishable photons or single-photon nonlinearities, may enable large-scale chip-based quantum photonic cir- cuits and networks. towards this, we use low-temperature in situ electron-beam lithography to de- terministically produce hybrid gaas/si3n4 photonic devices containing single inas quantum dots precisely located inside nanophotonic structures, which act as eﬃcient, si3n4 waveguide-coupled on-chip, on-demand single-photon sources. the precise positioning aﬀorded by our scalable fab- rication method furthermore allows observation of post-selected indistinguishable photons. this indicates a promising path towards signiﬁcant scaling of chip-based quantum photonics, enabled by large ﬂuxes of indistinguishable single-photons produced on-demand, directly on-chip. ∗ marcelo.davanco@nist.gov 2 i. "
1905.12031,". in this article we introduce and study oscillating gaussian pro- cesses deﬁned by xt = α+yt1yt>0 + α−yt1yt<0, where α+, α− > 0 are free parameters and y is either stationary or self-similar gaussian process. we study the basic properties of x and we consider estimation of the model parameters. in particular, we show that the moment estimators converge in lp and are, when suitably normalised, asymptotically normal. mathematics subject classiﬁcations (2010): 60g15 (primary), 60f05, 60f25, 62f10, 62f12 keywords: gaussian processes, oscillating processes, stationarity, self-similarity, parameter estimation, central limit theorem "
1905.12032," despite the great achievements of deep neural networks (dnns), the vulnerability of state-of-the-art dnns raises security concerns of dnns in many application domains requiring high reliability. we propose the fault sneaking a(cid:138)ack on dnns, where the adversary aims to misclassify certain input images into any target labels by modifying the dnn parameters. we apply admm (alternating di- rection method of multipliers) for solving the optimization problem of the fault sneaking a(cid:138)ack with two constraints: 1) the classi(cid:128)ca- tion of the other images should be unchanged and 2) the parameter modi(cid:128)cations should be minimized. speci(cid:128)cally, the (cid:128)rst constraint requires us not only to inject designated faults (misclassi(cid:128)cations), but also to hide the faults for stealthy or sneaking considerations by maintaining model accuracy. (cid:140)e second constraint requires us to minimize the parameter modi(cid:128)cations (using (cid:96)0 norm to measure the number of modi(cid:128)cations and (cid:96)2 norm to measure the magnitude of modi(cid:128)cations). comprehensive experimental evaluation demon- strates that the proposed framework can inject multiple sneaking faults without losing the overall test accuracy performance. ccs concepts •security and privacy → domain-speci(cid:128)c security and pri- vacy architectures; network security; •networks → network per- formance analysis; •(cid:135)eory of computation → (cid:138)eory and algo- rithms for application domains; keywords deep neural networks, fault injection, admm acm reference format: pu zhao, siyue wang, cheng gongye, yanzhi wang, yunsi fei, xue lin. 2019. fault sneaking a(cid:138)ack: a stealthy framework for misleading deep neural networks. in proceedings of (cid:138)e 56th annual design automation conference 2019, las vegas, nv, usa, june 2–6, 2019 (dac ’19), 7 pages. doi: 10.1145/3316781.3317825 "
1905.12033," computational drug discovery strategies can be broadly placed in two categories: ligand-based methods which identify novel molecules by similarity with known ligands, and structure-based methods which predict molecules with high-afﬁnity to a given 3d structure (e.g. a protein). however, ligand-based methods do not leverage information about the binding site, and structure-based approaches rely on the knowledge of a ﬁnite set of ligands binding the target. in this work, we introduce tarlig, a novel approach that aims to bridge the gap between ligand and structure-based approaches. we use the 3d structure of the binding site as input to a model which predicts the ligand preferences of the binding site. the resulting predictions could then offer promising seeds and constraints in the chemical space search, based on the binding site structure. tarlig outperforms standard models by introducing a data-alignment and augmentation technique. the recent popularity of volumetric 3dcnn pipelines in structural bioinformatics suggests that this extra step could help a wide range of methods to improve their results with minimal modiﬁcations. 1 "
1905.12034," for recurrent neural networks trained on time se- ries with target and exogenous variables, in ad- dition to accurate prediction, it is also desired to provide interpretable insights into the data. in this paper, we explore the structure of lstm recur- rent neural networks to learn variable-wise hidden states, with the aim to capture different dynamics in multi-variable time series and distinguish the contribution of variables to the prediction. with these variable-wise hidden states, a mixture atten- tion mechanism is proposed to model the genera- tive process of the target. then we develop associ- ated training methods to jointly learn network pa- rameters, variable and temporal importance w.r.t the prediction of the target variable. extensive ex- periments on real datasets demonstrate enhanced prediction performance by capturing the dynam- ics of different variables. meanwhile, we eval- uate the interpretation results both qualitatively and quantitatively. it exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variable data. "
1905.12037,". we study the geography of bilinearized legendrian contact homology for closed, connected legendrian submanifolds with vanishing maslov class in 1-jet spaces. we show that this invariant detects whether the two augmentations used to deﬁne it are dga homotopic or not. we describe a collection of graded vector spaces containing all possible values for bilinearized legendrian contact homology and then show that these vector spaces can be realized up to a speciﬁc stabilization. 9 1 0 2   y a m 8 2       ] . g s h t a m [     1 v 7 3 0 2 1 . 5 0 9 1 : v i x r a "
1905.12039,". we prove the weak lefschetz theorem for simplicial pl-spheres. this result is weaker than the hard lefschetz theorem for more general spheres proved by adiprasito [1], but the proof here involves simple algebra and avoids the more complicated combinatorics. "
1905.12040," context. intermediate-mass stars are often overlooked as they are not supernova progenitors but still host convective cores and complex atmospheres which require computationally expensive treatment. due to this, there is a general lack of such stars modelled by state of the art stellar structure and evolution codes. aims. this paper aims to use high-quality spectroscopy to update the dynamically obtained stellar parameters and produce a new evolutionary assessment of the bright b0.5+b0.5 and b5v+b5v binary systems cw cep and u oph. methods. we use new spectroscopy obtained with the hermes spectrograph to revisit the photometric binary solution of the two systems. the updated mass ratio and eﬀective temperatures are incorporated to obtain new dynamical masses for the primary and secondary. with these, we perform isochrone-cloud based evolutionary modelling to investigate the core properties of these stars. results. we report the ﬁrst abundances for cw cep and u oph as well as report an updated dynamical solution for both systems. we ﬁnd that we cannot uniquely constrain the amount of core boundary mixing in any of the stars we consider. instead, we report their core masses and compare our results to previous studies. conclusions. we ﬁnd that the per-cent level precision on fundamental stellar quantities are accompanied with core mass estimates to between ∼ 5 − 15%. we ﬁnd that diﬀerences in analysis techniques can lead to substantially diﬀerent evolutionary modelling results, which calls for the compilation of a homogeneously analysed sample to draw inference on internal physical processes. key words. stars: keyword 1 – stars: keyword 2 – stars: keyword 3 "
1905.12041," let ω ⊂ rd be a bounded open set with lipschitz boundary γ. it will be shown that the jordan chains of m-sectorial second-order elliptic partial diﬀerential operators with measurable coeﬃcients and (local or non-local) robin boundary conditions in l2(ω) can be character- ized with the help of jordan chains of the dirichlet-to-neumann map and the boundary operator from h 1/2(γ) into h −1/2(γ). this result extends the birman–schwinger principle in the framework of elliptic operators for the characterization of eigenvalues, eigenfunctions and geometric eigenspaces to the complete set of all generalized eigenfunc- tions and algebraic eigenspaces. may 2019 2010 mathematics subject classiﬁcation: 35j57, 35p05, 47a75, 47f05. keywords: jordan chain, eigenvector, generalized eigenvector, robin boundary condition, dirichlet-to-neumann operator home institutions: 1. institut f¨ur angewandte mathematik technische universit¨at graz steyrergasse 30 a-8010 graz austria 2. department of mathematics university of auckland private bag 92019 auckland 1142 new zealand email: behrndt@tugraz.at email: terelst@math.auckland.ac.nz 1 "
1905.12042," the process of identifying changes or transformations in a scene along with the ability of reasoning about their causes and effects, is a key aspect of intelligence. in this work we go beyond recent advances in computational perception, and in- troduce a more challenging task, image-based event-sequencing (ies). in ies, the task is to predict a sequence of actions required to rearrange objects from the conﬁguration in an input source image to the one in the target image. ies also requires systems to possess inductive generalizability. motivated from evidence in cognitive development, we compile the ﬁrst ies dataset, the blocksworld image reasoning dataset (bird) 1 which contains images of wooden blocks in different conﬁgurations, and the sequence of moves to rearrange one conﬁguration to the other. we ﬁrst explore the use of existing deep learning architectures and show that these end-to-end methods under-perform in inferring temporal event-sequences and fail at inductive generalization. we then propose a modular two-step approach: visual perception followed by event-sequencing, and demonstrate improved per- formance by combining learning and reasoning. finally, by showing an extension of our approach on natural images, we seek to pave the way for future research on event sequencing for real world scenes. 1 "
1905.12043," despite remarkable success in image-to-image transla- tion that celebrates the advancements of generative adver- sarial networks (gans), very limited attempts are known for video domain translation. we study the task of video-to- video translation in the context of visual speech generation, where the goal is to transform an input video of any spo- ken word to an output video of a different word. this is a multi-domain translation, where each word forms a do- main of videos uttering this word. adaptation of the state- of-the-art image-to-image translation model (stargan) to this setting falls short with a large vocabulary size. instead we propose to use character encodings of the words and de- sign a novel character-based gans architecture for video- to-video translation called visual speech gan (vispgan). we are the ﬁrst to demonstrate video-to-video translation with a vocabulary of 500 words. "
1905.12044," though reinforcement learning has greatly beneﬁted from the incorporation of neural networks, the inability to verify the correctness of such systems limits their use. current work in explainable deep learning focuses on explaining only a sin- gle decision in terms of input features, making it unsuitable for explaining a sequence of decisions. to address this need, we introduce ed policy graphs, which are markov chains of  states. this representation concisely sum- marizes a policy so that individual decisions can be explained in the context of expected future transitions. additionally, we propose a method to generate these ed policy graphs for deterministic policies given a learned value function and a set of observed transitions, potentially off-policy transi- tions used during training. since no restrictions are placed on how the value function is generated, our method is com- patible with many existing reinforcement learning methods. we prove that the worst-case time complexity of our method is quadratic in the number of features and linear in the num- ber of provided transitions, o(|f|2|tr samples|). by apply- ing our method to a family of domains, we show that our method scales well in practice and produces ed pol- icy graphs which reliably capture relationships within these domains. 1 "
1905.12045," the behaviour of a dirac electron in graphene, under magnetic ﬁelds which are or- thogonal to the layer, is studied. the initial problem is reduced to an equivalent one, where two one-dimensional schr¨odinger hamiltonians h± are intertwined by a ﬁrst order diﬀerential operator. special magnetic ﬁeld are initially chosen, in order that v ± will be shape invariant exactly solvable potentials. when looking for more general ﬁrst or- der operators, intertwining h− with a non necessarily shape invariant hamiltonian, new magnetic ﬁelds associated also to analytic solutions will be generated. the iteration of this procedure is as well discussed. 1 "
1905.12046," this paper extends work by tracy and widom on blocks in the asymmetric simple exclusion process (asep) to the case of step- bernoulli initial condition. we consider the probability that a particle at site x is the beginning of a block of l consecutive particles at time t in asep with step-bernoulli initial condition. a fredholm determi- nant representation for this probability is derived, and the asymptotics are computed for the kpz regime. 1 "
1905.12048," the ”radius valley” is a relative dearth of planets between two potential populations of exoplanets, super-earths and mini-neptunes. this feature appears in examining the distribution of planetary radii, but has only ever been characterized on small samples. the valley could be a result of photoevaporation, which has been predicted in numerous theoretical models, or a result of other processes. here, we investigate the relationship between planetary radius and orbital period through 2-dimensional kernel density estimator and various clustering methods, using all known super-earths (r < 4.0re ). with our larger sample, we conﬁrm the radius valley and characterize it as a power law. using a variety of methods, we ﬁnd a range of slopes that are consistent with each other and distinctly negative. we average over these results and ﬁnd the slope to be m = −0.319+0.088 −0.116. we repeat our analysis on samples from previous studies. for all methods we use, the resulting line has a negative slope, which is consistent with models of photoevaporation and core-powered mass loss but inconsistent with planets forming in a gas-poor disk. key words: planetary systems – methods: statistical "
1905.12051," the nearby (50 pc) k7v dwarf hip 41431 (epic 212096658) is a compact 3- tier hierarchy. three k7v stars with similar masses, from 0.61 to 0.63 solar, make a triple-lined spectroscopic system where the inner binary with a period of 2.9 days is eclipsing, and the outer companion on a 59-day orbit exerts strong dynamical inﬂu- ence revealed by the eclipse time variation in the kepler photometry. moreover, the centre-of-mass of the triple system moves on a 3.9-year orbit, modulating the proper motion. the mass of the 4-th star is 0.35 solar. the kepler and ground-based pho- tometry and radial velocities from four diﬀerent spectrographs are used to adjust the spectro-photodynamical model that accounts for dynamical interaction. the mutual inclination between the two inner orbits is 2.◦16±0.◦11, while the outer orbit is inclined to their common plane by 21◦±16◦. the inner orbit precesses under the inﬂuence of both outer orbits, causing observable variation of the eclipse depth. moreover, the phase of the inner binary is strongly modulated with a 59-day period and its line of apsides precesses. the middle orbit with eccentricity e = 0.28 also precesses, causing the observed variation of its radial velocity curve. masses and other parameters of stars in this unique hierarchy are determined. this system is dynamically stable and likely old. key words: binaries: spectroscopic – binaries: eclipsing – stars: individual: hip 41431 "
1905.12052," we construct a new distribution for the simplex using the kumaraswamy distribu- tion and an ordered stick-breaking process. we explore and develop the theoretical properties of this new distribution and prove that it exhibits symmetry under the same conditions as the well-known dirichlet. like the dirichlet, the new distri- bution is adept at capturing sparsity but, unlike the dirichlet, has an exact and closed form reparameterization–making it well suited for deep variational bayesian modeling. we demonstrate the distribution’s utility in a variety of semi-supervised auto-encoding tasks. in all cases, the resulting models achieve competitive perfor- mance commensurate with their simplicity, use of explicit probability models, and abstinence from adversarial training. 1 "
1905.12053," random quantum circuits are proﬁcient information scramblers and eﬃcient generators of randomness, rapidly approximating moments of the unitary group. we study the convergence of local random quantum circuits to unitary k-designs. employing a statistical mechanical mapping, we give an exact expression of the distance to forming an approximate design as a lattice partition function. in the statistical mechanics model, the approach to randomness has a simple interpretation in terms of domain walls extending through the circuit. we analytically compute the second moment, showing that random circuits acting on n qudits form approxi- mate 2-designs in o(n) depth, as is known. furthermore, we argue that random circuits form approximate unitary k-designs in o(nk) depth and are thus essentially optimal in both n and k. we can show this in the limit of large local dimension, but more generally rely on a conjecture about the dominance of certain domain wall conﬁgurations. 9 1 0 2   y a m 8 2       ] h p - t n a u q [     1 v 3 5 0 2 1 . 5 0 9 1 : v i x r a nickrhj@perimeterinstitute.ca contents "
1905.12055," in our earlier work [8], we approximated solutions of a general class of scalar parabolic semilinear pdes by an interpolatory hybridizable discontinuous galerkin (interpolatory hdg) method. this method reduces the computational cost compared to standard hdg since the hdg matrices are assembled once before the time integration. interpolatory hdg also achieves optimal convergence rates; however, we did not observe superconvergence after an element-by- element postprocessing. in this work, we revisit the interpolatory hdg method for reaction diﬀusion problems, and use the postprocessed approximate solution to evaluate the nonlinear term. we prove this simple change restores the superconvergence and keeps the computational advantages of the interpolatory hdg method. we present numerical results to illustrate the convergence theory and the performance of the method. keywords interpolatory hybridizable discontinuous galerkin method, superconvergence "
1905.12056,"—we present an information-theoretic approach to registration of dwi with explicit optimization over the orientational scale, with an additional focus on normalized mutual information as a robust information-theoretic similarity measure for dwi. the framework is an extension of the lor-dwi density-based hierarchical scale-space model, that varies and optimizes over the integration, spatial, directional, and intensity scales. we extend the model to non-rigid deformations and show that the formulation provides intrinsic regularization through the orientational information. our experiments illustrate that the proposed model deforms odfs correctly and is capable of handling the classic complex challenges in dwi-registrations, such as the registration of ﬁber-crossings along with kissing, fanning and interleaving ﬁbers. our results clearly illustrate a novel promising regularizing effect, that comes from the nonlinear orientation-based cost function. we illustrate the properties of the different image scales, and show that including orientational information in our model make the model better at retrieving deformations compared to standard scalar-based registration. index terms—registration, diffusion-weighted mri, normalized mutual information, explicit reorientation, analytical gradients, density-based formulation, watson density function, free-form deformation, cubic b-spline. ! "
1905.12057,". we study the dynamics induced by an m-linear operator. we answer a question of b`es and conejero showing an example of an m-linear hypercyclic operator acting on a banach space. moreover we prove the existence of m-linear hypercyclic operators on arbitrary inﬁnite dimensional separable fr´echet spaces. we also prove an existence result about symmetric bihypercyclic bilinear operators, answering a question by grosse-erdman and kim. "
1905.12058," though pairs of dynamically associated asteroids in the main belt have been ∗corresponding author email address: nmosko@lowell.edu (nicholas a. moskovitz) preprint submitted to icarus may 30, 2019 identiﬁed and studied for over a decade, very few pair systems have been iden- tiﬁed in the near-earth asteroid population. we present data and analysis that supports the existence of two genetically related pairs in near-earth space. the members of the individual systems, 2015 ee7 – 2015 fp124 and 2017 sn16 – 2018 ry7, are found to be of the same spectral taxonomic class, and both pairs are interpreted to have volatile-poor compositions. in conjunction with dynamical arguments, this suggests that these two systems formed via yorp spin-up and/or dissociation of a binary precursor. back- wards orbital integrations suggest a separation age of < 10 kyr for the pair 2017 sn16 – 2018 ry7, making these objects amongst the youngest multiple asteroid systems known to date. a unique separation age was not realized for 2015 ee7 – 2015 fp124 due to large uncertainties associated with these objects’ orbits. determining the ages of such young pairs is of great value for testing models of space weathering and asteroid spin-state evolution. as the neo catalog continues to grow with current and future discovery surveys, it is expected that more neo pairs will be found, thus providing an ideal labo- ratory for studying time dependent evolutionary processes that are relevant to asteroids throughout the solar system. keywords: asteroids, asteroids, dynamics, near-earth objects "
1905.12059,". in this paper, we study the ﬁrst eigenvalue of a nonlinear ellip- tic system involving p-laplacian as the diﬀerential operator. the principal eigenvalue of the system and the corresponding eigenfunction are investigated both analytically and numerically. an alternative proof to show the simplicity of the ﬁrst eigenvalue is given. in addition, an upper and lower bounds of the ﬁrst eigenvalue are provided. then, a numerical algorithm is developed to approximate the principal eigenvalue. this algorithm generates a decreas- ing sequence of positive numbers and various examples numerically indicate its convergence. further, the algorithm is generalized to a class of gradient quasilinear elliptic systems. keywords: nonlinear elliptic system, p-laplacian, eigenvalue problem, simplicity, numerical approximation 2010 msc: 35p30, 34l15, 34l16, 35j92 "
1905.12060," we report the discovery of soft x-ray pulsations from the nearby millisecond pulsar psr j1231−1411 using nicer. the pulsed emission is characterized by a broad and asymmetric main pulse and a much fainter secondary interpulse, with a total pulsed count rate of 0.055 c s−1 in the 0.35–1.5 kev band. we analyzed fermi lat data to update the pulse timing model covering 10 years of data and used that model to coherently combine nicer data over a year of observations. spectral modeling suggests that the ﬂux is dominated by thermal emission from a hot spot (or spots) on the neutron star surface. the phase relationship between the x-ray pulse and the radio and γ rays provides insight into the geometry of the system. keywords: pulsars: general — pulsars: individual (psr j1231−1411) — stars: neu- tron — x-rays: stars "
1905.12061,"—in recent years, cyber-physical system (cps) se- curity as applied to robotic systems has become a popular research area. mainly because robotics systems have traditionally emphasized the completion of a speciﬁc objective and lack security oriented design. our previous work, honeybot [16], presented the concept and prototype of the ﬁrst software hybrid interaction honeypot speciﬁcally designed for networked robotic systems. the intuition behind honeybot was that it would be a remotely accessible robotic system that could simulate unsafe actions and physically perform safe actions to fool attackers. unassuming attackers would think they were connected to an ordinary robotic system, believing their exploits were being successfully executed. all the while, the honeybot is logging all communications and exploits sent to be used for attacker attribution and threat model creation. in this paper, we present ﬁndings from the result of a user study performed to evaluate the effectiveness of the honeybot framework and architecture as it applies to real robotic systems. the user study consisted of 40 participants, was conducted over the course of several weeks, and drew from a wide range of participants aged between 18- 60 with varying level of technical expertise. from the study we found that research subjects could not tell the difference between the simulated sensor values and the real sensor values coming from the honeybot, meaning the honeybot convincingly spoofed communications. i. "
1905.12062," array-rqmc has been proposed as a way to effectively apply randomized quasi-monte carlo (rqmc) when simulating a markov chain over a large number of steps to estimate an expected cost or reward. the method can be very effective when the state of the chain has low dimension. for pricing an asian option under an ordinary geometric brownian motion model, for example, array-rqmc reduces the variance by huge factors. in this paper, we show how to apply this method and we study its effectiveness in case the underlying process has stochastic volatility. we show that array-rqmc can also work very well for these models, even if it requires rqmc points in larger dimension. we examine in particular the variance-gamma, heston, and ornstein-uhlenbeck stochastic volatility models, and we provide numerical results. "
1905.12063," it has been observed that linearizability, the prevalent consistency condition for implementing con- current objects, does not preserve some probability distributions. a stronger condition, called strong linearizability has been proposed, but its study has been somewhat ad-hoc. this paper investigates strong linearizability by casting it in the context of observational reﬁnement of objects. we present a strengthening of observational reﬁnement, which generalizes strong linearizability, obtaining several important implications. when a concrete concurrent object reﬁnes another, more  object—often sequential—the cor- rectness of a program employing the concrete object can be veriﬁed by considering its behaviors when using the more  object. this means that trace properties of a program using the concrete object can be proved by considering the program with the  object. this, however, does not hold for hyperproperties, including many security properties and probability distributions of events. we deﬁne strong observational reﬁnement, a strengthening of reﬁnement that preserves hyperprop- erties, and prove that it is equivalent to the existence of forward simulations. we show that strong ob- servational reﬁnement generalizes strong linearizability. this implies that strong linearizability is also equivalent to forward simulation, and shows that strongly linearizable implementations can be composed both horizontally (i.e., locality) and vertically (i.e., with instantiation). for situations where strongly linearizable implementations do not exist (or are less efﬁcient), we argue that reasoning about hyperproperties of programs can be simpliﬁed by strong observational reﬁne- ment of non-atomic  objects. 9 1 0 2   y a m 8 2       ] c d . s c [     1 v 3 6 0 2 1 . 5 0 9 1 : v i x r a 1 "
1905.12065," bass most work on sense disambiguation presumes that one knows beforehand — e.g. from a thesaurus — a set of polysemous terms. but published lists invariably give only partial coverage. for example, the english word tan has several obvious senses, but one may overlook the abbreviation for tangent. in this pa- per, we present an algorithm for identifying interesting polyse- mous terms and measuring their degree of polysemy, given an unlabeled corpus. the algorithm involves: (i) collecting all terms within a   -term window of the target term; (ii) computing the inter-term distances of the contextual terms, and reducing the multi-dimensional distance space to two dimensions using stan- dard methods; (iii) converting the two-dimensional representa- tion into radial coordinates and using isotonic/antitonic regression to compute the degree to which the distribution deviates from a single-peak model. the amount of deviation is the proposed pol- ysemy index. 1. "
1905.12066," dwarf stars with debris discs and planets appear to be excellent laboratories to study the core accretion theory of planets formation. these systems are however, insuﬃ- ciently studied. in this paper we present the main metallicity and lithium abundance properties of these stars together with stars with only debris discs and stars with only planets. stars without detected planets nor discs are also considered. the anal- ysed sample is formed by main-sequence fgk ﬁeld single stars. apart from the basic stellar parameters, we include the use of dusty discs masses. the main results show for the ﬁrst time that the dust mass of debris disc stars with planets correlate with metallicity. we conﬁrm that these disc dust masses are related to their central stellar masses. separately, the masses of stars and those of planets also correlate with metallic- ity. we conclude that two conditions are necessary to form giant planets: to have a suﬃcient metallicity and also a suﬃcient protoplanetary mass of gas and dust. the debris discs masses of stars without giant planets do not correlate with metallicity, because they do not fulﬁl these two conditions. concerning lithium, by adopting a stel- lar model for lithium depletion based on a strong interaction between the star and a protoplanetary disc, we found that in agreement with the model predictions, observa- tions indicate that the main lithium depletion occurs during this initial protoplanetary evolution stage. we show that the ultimately lithium depletion is independent of the presence or absence of planets and appears to be only age dependent. key words: (stars:) planetary systems – planets and satellites: formation (stars:) circumtellar matter – stars: solar-type – stars: abundances – 9 1 0 2   y a m 8 2       ] . r s h p - o r t s a [     1 v 6 6 0 2 1 . 5 0 9 1 : v i x r a "
1905.12067," reaction-diffusion equations are one of the most common partial differential equa- tions used to model physical phenomenon. they arise as the combination of two physical processes: a driving force f (u) that depends on the state variable u and a dif- fusive mechanism that spreads this effect over a spatial domain. the canonical form is ut −△u = f (u). application areas include chemical processes, heat ﬂow models and population dynamics. as part of the model building, assumptions are made about the form of f (u) and these inevitably contain various physical constants. the direct or forwards problem for such equations is now very well-developed and understood, especially when the diffusive mechanism is governed by brownian motion resulting in an equation of parabolic type. however, our interest lies in the inverse problem of recovering the reaction term f (u) not just at the level of determining a few parameters in a known functional form, but recovering the complete functional form itself. to achieve this we set up the usual paradigm for the parabolic equation where u is subject to both given initial and bound- ary data, then prescribe overposed data consisting of the solution at a later time t . for example, in the case of a population model this amounts to census data at a ﬁxed time. our approach will be two-fold. first we will transform the inverse problem into an equivalent nonlinear mapping from which we seek a ﬁxed point. we will be able to prove important features of this map such as a self-mapping property and give conditions under which it is contractive. second, we consider the direct map from f through the partial differential operator to the overposed data. we will investigate newton schemes for this case. classical, brownian motion diffusion is not the only version and in recent decades various anomalous processes have been used to generalize this case. amongst the most popular is one that replaces the usual time derivative by a subdiffusion process based on a fractional derivative of order α ≤ 1. we will also include this model in our analysis. the ﬁnal section of the paper will show numerical reconstructions that demonstrate the viability of the suggested approaches. this will also include the de- pendence of the inverse problem on both t and α. ∗department of mathematics, alpen-adria-universit¨at klagenfurt. barbara.kaltenbacher@aau.at. †department of mathematics, texas a&m university, texas 77843. rundell@math.tamu.edu 1 "
1905.12068,"—in this paper, we propose quantum circuits for runtime assertions, which can be used for both software debugging and  error detection. runtime assertion is challenging in quantum computing for two key reasons. first, a quantum bit (qubit) cannot  be copied, which is known as the non-cloning theorem. second, when a qubit is measured, its superposition state collapses into  a classical state, losing the inherent parallel information. in this paper, we overcome these challenges with runtime computation  through ancilla qubits, which are used to indirectly collect the information of the qubits of interest. we design quantum circuits to  assert classical states, entanglement, and superposition states.  1  q "
1905.12069,".  meaning representation (amr) is a recently de- signed semantic representation language intended to capture the mean- ing of a sentence, which may be represented as a single-rooted directed acyclic graph with labeled nodes and edges. the automatic evaluation of this structure plays an important role in the development of better systems, as well as for semantic annotation. despite there is one avail- able metric, smatch, it has some drawbacks. for instance, smatch creates a self-relation on the root of the graph, has weights for diﬀerent error types, and does not take into account the dependence of the elements in the amr structure. with these drawbacks, smatch masks several prob- lems of the amr parsers and distorts the evaluation of the amrs. in view of this, in this paper, we introduce an extended metric to evalu- ate amr parsers, which deals with the drawbacks of the smatch metric. finally, we compare both metrics, using four well-known amr parsers, and we argue that our metric is more reﬁned, robust, fairer, and faster than smatch. keywords:  meaning representation · semantic metric · eval- uation. 1 "
1905.12070," a climate state close to a tipping point will have a degenerate linear response to perturba- tions, which can be associated with extreme values of the equilibrium climate sensitivity (ecs). in this paper we contrast linearized (‘instantaneous’) with fully nonlinear geometric (‘two-point’) notions of ecs, in both presence and absence of tipping points. for a stochastic energy bal- ance model of the global mean surface temperature with two stable regimes, we conﬁrm that tipping events cause the appearance of extremes in both notions of ecs. moreover, multiple regimes with diﬀerent mean sensitivities are visible in the two-point ecs. we conﬁrm some of our ﬁndings in a physics-based multi-box model of the climate system. keywords climate sensitivity, tipping point, energy balance model, stochastic climate model. "
1905.12071,"guarantees for sound ions for generalized planning (extended paper) blai bonet1 , 2 ∗ , raquel fuentetaja2 , yolanda e-mart´ın2 , daniel borrajo2 1universidad sim´on bol´ıvar, venezuela 2universidad carlos iii de madrid, spain bonet@usb.ve, {rfuentet,yescuder}@inf.uc3m.es, dborrajo@ia.uc3m.es  generalized planning is about ﬁnding plans that solve collections of planning instances, often inﬁ- nite collections, rather than single instances. re- cently it has been shown how to reduce the plan- ning problem for generalized planning to the plan- ning problem for a qualitative numerical problem; the latter being a reformulation that simultaneously captures all the instances in the collection. an important thread of research thus consists in ﬁnd- ing such reformulations, or ions, automati- cally. a recent proposal learns the ions in- ductively from a ﬁnite and small sample of transi- tions from instances in the collection. however, as in all inductive processes, the learned ion is not guaranteed to be correct for the whole collec- tion. in this work we address this limitation by per- forming an analysis of the ion with respect to the collection, and show how to obtain formal guarantees for generalization. these guarantees, in the form of ﬁrst-order formulas, may be used to 1) deﬁne subcollections of instances on which the ion is guaranteed to be sound, 2) ob- tain necessary conditions for generalization under certain assumptions, and 3) do automated synthesis of complex invariants for planning problems. our framework is general, it can be extended or com- bined with other approaches, and it has applications that go beyond generalized planning. "
1905.12074," in this paper, we derive an inverse result for bivariate kantorovich type sampling series for f ∈ c2(r2) (the space of all continuous functions with upto second order partial derivatives are continuous and bounded on r2). further, we prove the rate of approximation in the b¨ogel space of continuous functions for the gbs (generalized boolean sum) of these operators. finally, we give some examples for the kernel to which the theory can be applied. keywords inverse result · bivariate kantorovich type sampling series · rate of convergence · gbs operators · mixed modulus of smoothness mathematics subject classiﬁcation (2010) 41a25 · 94a20 · 41a30 · 41a10 "
1905.12077,"— this paper studies the problem of enforcing safety of a stochastic dynamical system over a ﬁnite time horizon. we use stochastic barrier functions as a means to quantify the probability that a system exits a given safe region of the state space in ﬁnite time. a barrier certiﬁcate condition that bounds the inﬁnitesimal generator of the system, and hence bounds the expected value of the barrier function over the time horizon, is recast as a sum-of-squares optimization problem for efﬁcient numerical computation. unlike prior works, the proposed certiﬁcate condition includes a state-dependent bound on the inﬁnitesimal generator, allowing for tighter probability bounds. moreover, for stochastic systems for which the drift dynamics are afﬁne-in-control, we propose a method for syn- thesizing polynomial state feedback controllers that achieve a speciﬁed probability of safety. two case studies are presented that benchmark and illustrate the performance of our method. "
1905.12079," we introduce a new method for category-level pose estimation which produces a distribution over predicted poses by integrating 3d shape estimates from a genera- tive object model with segmentation information. given an input depth-image of an object, our variable-time method uses a mixture density network architecture to produce a multi-modal distribution over 3dof poses; this distribution is then combined with a prior probability encouraging silhouette agreement between the observed input and predicted object pose. our approach signiﬁcantly outperforms the current state-of-the-art in category-level 3dof pose estimation—which out- puts a point estimate and does not explicitly incorporate shape and segmentation information—as measured on the pix3d and shapenet datasets. 1 "
1905.12080," a recent strategy to circumvent the exploding and vanishing gradient problem in rnns, and to allow the stable propagation of signals over long time scales, is to constrain recurrent connectivity matrices to be orthogonal or unitary. this ensures eigenvalues with unit norm and thus stable dynamics and training. however this comes at the cost of reduced expressivity due to the limited variety of orthogonal transformations. we propose a novel connectivity structure based on the schur decomposition and a splitting of the schur form into normal and non-normal parts. this allows to parametrize matrices with unit-norm eigenspectra without orthogonality constraints on eigenbases. the resulting architecture ensures access to a larger space of spectrally constrained matrices, of which orthogonal matrices are a subset. this crucial difference retains the stability advantages and training speed of orthogonal rnns while enhancing expressivity, especially on tasks that require computations over ongoing input sequences. "
1905.12081," while the success of semi-supervised learning (ssl) is still not fully understood, schölkopf et al. (2012) have established a link to the principle of independent causal mechanisms. they conclude that ssl should be impossible when predicting a target variable from its causes, but possible when predicting it from its effects. since both these cases are somewhat restrictive, we extend their work by considering classiﬁcation using cause and effect features at the same time, such as predicting a disease from both risk factors and symptoms. while standard ssl exploits information contained in the marginal distribution of the inputs (to improve our estimate of the conditional distribution of target given inputs), we argue that in our more general setting we can use information in the conditional of effect features given causal features. we explore how this insight generalizes the previous understanding, and how it relates to and can be exploited for ssl. "
1905.12082,"— facial expression recognition is a major problem in the domain of artiﬁcial intelligence. one of the best ways to solve this problem is the use of convolutional neural networks (cnns). however, a large amount of data is required to train properly these networks but most of the datasets available for facial expression recognition are relatively small. a common way to circumvent the lack of data is to use cnns trained on large datasets of different domains and ﬁne-tuning the layers of such networks to the target domain. however, the ﬁne-tuning process does not preserve the memory integrity as cnns have the tendency to forget patterns they have learned. in this paper, we evaluate different strategies of ﬁne-tuning a cnn with the aim of assessing the memory integrity of such strategies in a cross-dataset scenario. a cnn pre-trained on a source dataset is used as the baseline and four adaptation strategies have been evaluated: ﬁne-tuning its fully connected layers; ﬁne-tuning its last convolutional layer and its fully connected layers; retraining the cnn on a target dataset; and the fusion of the source and target datasets and retraining the cnn. experimental results on four datasets have shown that the fusion of the source and the target datasets provides the best trade-off between accuracy and memory integrity. "
1905.12086,"    a new riemann solver is built to address numerical resolution of complex flow models. the research  direction is closely linked to a variant of the baer and nunziato (1986) model developed in saurel et  al. (2017a). this recent model provides a link between the marble (1963) model for two-phase dilute  suspensions and dense mixtures. as in the marble model, saurel et al. system is weakly hyperbolic  with the same 4 characteristic waves, while the system involves 7 partial differential equations. it poses  serious theoretical and practical issues to built simple and accurate flow solver. to overcome related  difficulties  the  riemann  solver  of  linde  (2002)  is  revisited.  the  method  is  first  examined  in  the  simplified  context  of  compressible  euler  equations.  physical  considerations  are  introduced  in  the  solver improving robustness and accuracy of the linde method. with these modifications the flow  solver appears as accurate as the hllc solver of toro et al. (1994). second the two-phase flow model  is considered. a locally conservative formulation is built and validated removing issues related to non- conservative  terms.  however,  two  extra  major  issues  appear  from  numerical  experiments:  the  solution appears not self-similar and multiple contact waves appear in the dispersed phase.  building   hllc-type or any other solver appears consequently challenging. the modified linde (2002) method  is thus examined for the considered flow model. some basic properties of the equations are used, such  as shock relations of the dispersed phase and jump conditions across the contact wave. thanks to  these ingredients the new riemann solver with internal reconstruction (rsir), modification of the  linde  method,  handles  stationary  volume  fraction  discontinuities,  presents  low  dissipation  for  transport waves and handles shocks and expansion waves accurately. it is validated on various test  problems  showing  method’s  accuracy  and  versatility  for  complex  flow  models.  its  capabilities  are  illustrated on a difficult two-phase flow instability problem, unresolved before.              keywords: two-phase, dense-dilute, weakly hyperbolic, riemann solver              1  "
1905.12087," this project is focused on the development of a novel strategy for the precise measurement of beta decay energy spectra. the exact determination of beta spectra has wide implications in the particle and nuclear physics ﬁelds: from sterile neutrino searches [5] to validation of nuclear models of interest for double beta decay searches [39], to reactor neutrino experiments. the experimental strategy is focused on the mitigation of the systematic uncertainty in the determination of the spectral shape related to the energy response of the detector. the plan is to use silicon drift detectors (sdds), exploiting their excellent energy resolution and response uniformity, the thin dead layer and the fast signal that allows high rate operations. a complete modeling of sdds response to the interaction of beta particles via numerical simulations, validated with dedicated measurements, aims at identifying and canceling the detector-related systematics. similar attention will be devoted to the integration of beta radioactive sources that preserve the energy information carried by the emitted electrons, and to a veto system with a full solid angle coverage to intercept any escaping particle potentially carrying a fraction of the original electron energy. keywords nuclear physics · spectroscopic and spectrometric techniques · physics of detectors · elementary particles · beta decay 1 "
1905.12089," in this study we analyze 70 radio continuum sources associated with dust clumps and considered to be candi- dates for the earliest stages of high-mass star formation. the detection of these sources was reported by rosero et al. (2016), who found most of them to show weak (<1 mjy) and compact (<0.(cid:48)(cid:48)6) radio emission. herein, we used the observed parameters of these sources to investigate the origin of the radio continuum emission. we found that at least ∼ 30% of these radio detections are most likely ionized jets associated with high-mass pro- tostars, but for the most compact sources we cannot discard the scenario that they represent pressure-conﬁned hii regions. this result is highly relevant for recent theoretical models based on core accretion that predict the ﬁrst stages of ionization from high-mass stars to be in the form of jets. additionally, we found that properties such as the radio luminosity as a function of the bolometric luminosity of ionized jets from low and high-mass stars are extremely well-correlated. our data improve upon previous studies by providing further evidence of a common origin for jets independently of luminosity. keywords: ionized jets – stars: formation – techniques: high sensitivity – techniques: interferometric "
